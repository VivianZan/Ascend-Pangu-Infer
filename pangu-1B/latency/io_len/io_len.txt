Batch-8 Input len-32 Output len-32
INFO 12-13 11:56:57 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:56:57 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:56:57 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:56:57 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:56:58 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:57:02 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:57:03 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:57:03 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:57:03 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:57:03 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:57:03 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:57:03 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:57:20 [config.py:841] This model supports multiple tasks: {'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-13 11:57:20 [config.py:1472] Using max model len 32768
INFO 12-13 11:57:20 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 11:57:20 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:57:20 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:57:20 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:57:20 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 11:57:21 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 11:57:29 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:57:29 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:57:29 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:57:29 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:57:30 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:57:33 [core.py:526] Waiting for init message from front-end.
INFO 12-13 11:57:34 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:57:34 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:57:34 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:57:34 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:57:34 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:57:34 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:57:34 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:57:34 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 11:57:47 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:57:47 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:57:47 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:57:47 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:57:48 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:57:55 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 11:57:55 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.37it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.36it/s]

INFO 12-13 11:57:56 [default_loader.py:272] Loading weights took 0.66 seconds
INFO 12-13 11:57:57 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 11:58:05 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 11:58:05 [backends.py:519] Dynamo bytecode transform time: 7.11 s
INFO 12-13 11:58:07 [backends.py:193] Compiling a graph for general shape takes 1.90 s
INFO 12-13 11:58:15 [monitor.py:34] torch.compile takes 9.01 s in total
INFO 12-13 11:58:16 [worker_v1.py:181] Available memory: 55438074572, total memory: 65464696832
INFO 12-13 11:58:16 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 11:58:16 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 11:58:58 [model_runner_v1.py:2074] Graph capturing finished in 42 secs, took 0.17 GiB
INFO 12-13 11:58:58 [core.py:172] init engine (profile, create kv cache, warmup model) took 61.17 seconds
WARNING 12-13 11:59:00 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 11:59:00 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:59:00 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:59:00 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:59:00 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:02,  1.91it/s]Warmup iterations:  40%|████      | 2/5 [00:00<00:01,  2.12it/s]Warmup iterations:  60%|██████    | 3/5 [00:01<00:00,  2.20it/s]Warmup iterations:  80%|████████  | 4/5 [00:01<00:00,  2.24it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  2.26it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  2.21it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:03,  2.31it/s]Profiling iterations:  20%|██        | 2/10 [00:00<00:03,  2.31it/s]Profiling iterations:  30%|███       | 3/10 [00:01<00:03,  2.31it/s]Profiling iterations:  40%|████      | 4/10 [00:01<00:02,  2.32it/s]Profiling iterations:  50%|█████     | 5/10 [00:02<00:02,  2.32it/s]Profiling iterations:  60%|██████    | 6/10 [00:02<00:01,  2.33it/s]Profiling iterations:  70%|███████   | 7/10 [00:03<00:01,  2.33it/s]Profiling iterations:  80%|████████  | 8/10 [00:03<00:00,  2.33it/s]Profiling iterations:  90%|█████████ | 9/10 [00:03<00:00,  2.32it/s]Profiling iterations: 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]Profiling iterations: 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]
Avg latency: 0.43045405596494674 seconds
10% percentile latency: 0.4271475756075233 seconds
25% percentile latency: 0.428457543021068 seconds
50% percentile latency: 0.4301542395260185 seconds
75% percentile latency: 0.4326328872703016 seconds
90% percentile latency: 0.43372234059497716 seconds
99% percentile latency: 0.43525511818937956 seconds
Batch-8 Input len-32 Output len-64
INFO 12-13 11:59:23 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:59:23 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:59:23 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:59:23 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:59:24 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:59:28 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:59:28 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:59:28 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:59:28 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:59:28 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:59:28 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:59:28 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:59:45 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 12-13 11:59:45 [config.py:1472] Using max model len 32768
INFO 12-13 11:59:45 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 11:59:45 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:59:45 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:59:45 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:59:45 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 11:59:46 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 11:59:55 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:59:55 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:59:55 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:59:55 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:59:56 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:59:59 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:00:00 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:00:00 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:00:00 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:00:00 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:00:00 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:00:00 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:00:00 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:00:00 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:00:13 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:00:13 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:00:13 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:00:13 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:00:14 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:00:21 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:00:21 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]

INFO 12-13 12:00:23 [default_loader.py:272] Loading weights took 0.68 seconds
INFO 12-13 12:00:25 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:00:33 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:00:33 [backends.py:519] Dynamo bytecode transform time: 7.22 s
INFO 12-13 12:00:36 [backends.py:193] Compiling a graph for general shape takes 1.96 s
INFO 12-13 12:00:44 [monitor.py:34] torch.compile takes 9.17 s in total
INFO 12-13 12:00:45 [worker_v1.py:181] Available memory: 55436649164, total memory: 65464696832
INFO 12-13 12:00:45 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:00:45 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:02:03 [model_runner_v1.py:2074] Graph capturing finished in 78 secs, took 0.17 GiB
INFO 12-13 12:02:03 [core.py:172] init engine (profile, create kv cache, warmup model) took 98.31 seconds
WARNING 12-13 12:02:04 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:02:05 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:02:05 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:02:05 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:02:05 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:04,  1.02s/it]Warmup iterations:  40%|████      | 2/5 [00:01<00:02,  1.03it/s]Warmup iterations:  60%|██████    | 3/5 [00:02<00:01,  1.05it/s]Warmup iterations:  80%|████████  | 4/5 [00:03<00:00,  1.06it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.07it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.06it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:08,  1.08it/s]Profiling iterations:  20%|██        | 2/10 [00:01<00:07,  1.08it/s]Profiling iterations:  30%|███       | 3/10 [00:02<00:06,  1.08it/s]Profiling iterations:  40%|████      | 4/10 [00:03<00:05,  1.08it/s]Profiling iterations:  50%|█████     | 5/10 [00:04<00:04,  1.08it/s]Profiling iterations:  60%|██████    | 6/10 [00:05<00:03,  1.07it/s]Profiling iterations:  70%|███████   | 7/10 [00:06<00:02,  1.08it/s]Profiling iterations:  80%|████████  | 8/10 [00:07<00:01,  1.08it/s]Profiling iterations:  90%|█████████ | 9/10 [00:08<00:00,  1.08it/s]Profiling iterations: 100%|██████████| 10/10 [00:09<00:00,  1.08it/s]Profiling iterations: 100%|██████████| 10/10 [00:09<00:00,  1.08it/s]
Avg latency: 0.9295052861329168 seconds
10% percentile latency: 0.9275106844026595 seconds
25% percentile latency: 0.9291345460806042 seconds
50% percentile latency: 0.9293705965392292 seconds
75% percentile latency: 0.930171204963699 seconds
90% percentile latency: 0.9313869759440422 seconds
99% percentile latency: 0.9320001243986189 seconds
Batch-8 Input len-32 Output len-128
INFO 12-13 12:02:35 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:02:35 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:02:35 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:02:35 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:02:36 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:02:40 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:02:40 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:02:40 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:02:40 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:02:40 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:02:40 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:02:40 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:02:57 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-13 12:02:57 [config.py:1472] Using max model len 32768
INFO 12-13 12:02:57 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:02:57 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:02:57 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:02:57 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:02:57 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:02:58 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:03:07 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:03:07 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:03:07 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:03:07 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:03:08 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:03:11 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:03:12 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:03:12 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:03:12 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:03:12 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:03:12 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:03:12 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:03:12 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:03:12 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:03:25 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:03:25 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:03:25 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:03:25 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:03:26 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:03:34 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:03:35 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]

INFO 12-13 12:03:36 [default_loader.py:272] Loading weights took 0.68 seconds
INFO 12-13 12:03:37 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:03:45 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:03:45 [backends.py:519] Dynamo bytecode transform time: 7.34 s
INFO 12-13 12:03:47 [backends.py:193] Compiling a graph for general shape takes 1.99 s
INFO 12-13 12:03:55 [monitor.py:34] torch.compile takes 9.33 s in total
INFO 12-13 12:03:56 [worker_v1.py:181] Available memory: 55436493516, total memory: 65464696832
INFO 12-13 12:03:56 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:03:56 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:05:13 [model_runner_v1.py:2074] Graph capturing finished in 76 secs, took 0.17 GiB
INFO 12-13 12:05:13 [core.py:172] init engine (profile, create kv cache, warmup model) took 95.84 seconds
WARNING 12-13 12:05:14 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:05:14 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:05:14 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:05:14 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:05:14 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:07,  1.93s/it]Warmup iterations:  40%|████      | 2/5 [00:03<00:05,  1.87s/it]Warmup iterations:  60%|██████    | 3/5 [00:05<00:03,  1.85s/it]Warmup iterations:  80%|████████  | 4/5 [00:07<00:01,  1.84s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.84s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.85s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:16,  1.83s/it]Profiling iterations:  20%|██        | 2/10 [00:03<00:14,  1.83s/it]Profiling iterations:  30%|███       | 3/10 [00:05<00:12,  1.83s/it]Profiling iterations:  40%|████      | 4/10 [00:07<00:10,  1.83s/it]Profiling iterations:  50%|█████     | 5/10 [00:09<00:09,  1.83s/it]Profiling iterations:  60%|██████    | 6/10 [00:10<00:07,  1.83s/it]Profiling iterations:  70%|███████   | 7/10 [00:12<00:05,  1.84s/it]Profiling iterations:  80%|████████  | 8/10 [00:14<00:03,  1.84s/it]Profiling iterations:  90%|█████████ | 9/10 [00:16<00:01,  1.84s/it]Profiling iterations: 100%|██████████| 10/10 [00:18<00:00,  1.84s/it]Profiling iterations: 100%|██████████| 10/10 [00:18<00:00,  1.84s/it]
Avg latency: 1.8352796143386514 seconds
10% percentile latency: 1.8316062392666936 seconds
25% percentile latency: 1.83216726151295 seconds
50% percentile latency: 1.83403344405815 seconds
75% percentile latency: 1.8388628411339596 seconds
90% percentile latency: 1.8402071891352534 seconds
99% percentile latency: 1.842159247798845 seconds
Batch-8 Input len-32 Output len-256
INFO 12-13 12:06:00 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:06:00 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:06:00 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:06:00 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:06:01 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:06:05 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:06:05 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:06:05 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:06:05 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:06:05 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:06:05 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:06:05 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:06:22 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 12-13 12:06:22 [config.py:1472] Using max model len 32768
INFO 12-13 12:06:22 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:06:22 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:06:22 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:06:22 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:06:22 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:06:23 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:06:31 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:06:31 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:06:31 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:06:31 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:06:33 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:06:35 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:06:36 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:06:37 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:06:37 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:06:37 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:06:37 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:06:37 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:06:37 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:06:37 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:06:50 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:06:50 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:06:50 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:06:50 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:06:51 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:06:57 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:06:58 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]

INFO 12-13 12:06:59 [default_loader.py:272] Loading weights took 0.50 seconds
INFO 12-13 12:07:02 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:07:10 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:07:10 [backends.py:519] Dynamo bytecode transform time: 7.23 s
INFO 12-13 12:07:12 [backends.py:193] Compiling a graph for general shape takes 1.96 s
INFO 12-13 12:07:20 [monitor.py:34] torch.compile takes 9.19 s in total
INFO 12-13 12:07:22 [worker_v1.py:181] Available memory: 55436624588, total memory: 65464696832
INFO 12-13 12:07:22 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:07:22 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:08:38 [model_runner_v1.py:2074] Graph capturing finished in 76 secs, took 0.17 GiB
INFO 12-13 12:08:38 [core.py:172] init engine (profile, create kv cache, warmup model) took 96.36 seconds
WARNING 12-13 12:08:39 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:08:39 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:08:39 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:08:39 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:08:39 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:03<00:13,  3.49s/it]Warmup iterations:  40%|████      | 2/5 [00:06<00:10,  3.42s/it]Warmup iterations:  60%|██████    | 3/5 [00:10<00:06,  3.40s/it]Warmup iterations:  80%|████████  | 4/5 [00:13<00:03,  3.39s/it]Warmup iterations: 100%|██████████| 5/5 [00:16<00:00,  3.39s/it]Warmup iterations: 100%|██████████| 5/5 [00:16<00:00,  3.40s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:03<00:30,  3.39s/it]Profiling iterations:  20%|██        | 2/10 [00:06<00:27,  3.38s/it]Profiling iterations:  30%|███       | 3/10 [00:10<00:23,  3.40s/it]Profiling iterations:  40%|████      | 4/10 [00:13<00:20,  3.41s/it]Profiling iterations:  50%|█████     | 5/10 [00:17<00:17,  3.41s/it]Profiling iterations:  60%|██████    | 6/10 [00:20<00:13,  3.41s/it]Profiling iterations:  70%|███████   | 7/10 [00:23<00:10,  3.41s/it]Profiling iterations:  80%|████████  | 8/10 [00:27<00:06,  3.42s/it]Profiling iterations:  90%|█████████ | 9/10 [00:30<00:03,  3.41s/it]Profiling iterations: 100%|██████████| 10/10 [00:34<00:00,  3.42s/it]Profiling iterations: 100%|██████████| 10/10 [00:34<00:00,  3.41s/it]
Avg latency: 3.4115007545333356 seconds
10% percentile latency: 3.39007705594413 seconds
25% percentile latency: 3.406973075005226 seconds
50% percentile latency: 3.4151046900078654 seconds
75% percentile latency: 3.422417249530554 seconds
90% percentile latency: 3.423361771879718 seconds
99% percentile latency: 3.4291082113655285 seconds
Batch-8 Input len-32 Output len-512
INFO 12-13 12:09:47 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:09:47 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:09:47 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:09:47 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:09:48 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:09:52 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:09:52 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:09:52 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:09:52 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:09:52 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:09:52 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:09:52 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:10:09 [config.py:841] This model supports multiple tasks: {'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-13 12:10:09 [config.py:1472] Using max model len 32768
INFO 12-13 12:10:09 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:10:09 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:10:09 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:10:09 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:10:09 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:10:10 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:10:18 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:10:18 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:10:18 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:10:18 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:10:20 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:10:22 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:10:23 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:10:24 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:10:24 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:10:24 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:10:24 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:10:24 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:10:24 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:10:24 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:10:37 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:10:37 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:10:37 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:10:37 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:10:38 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:10:44 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:10:45 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.30it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.30it/s]

INFO 12-13 12:10:47 [default_loader.py:272] Loading weights took 0.66 seconds
INFO 12-13 12:10:48 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:10:57 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:10:57 [backends.py:519] Dynamo bytecode transform time: 7.29 s
INFO 12-13 12:10:59 [backends.py:193] Compiling a graph for general shape takes 2.00 s
INFO 12-13 12:11:07 [monitor.py:34] torch.compile takes 9.30 s in total
INFO 12-13 12:11:08 [worker_v1.py:181] Available memory: 55436944076, total memory: 65464696832
INFO 12-13 12:11:08 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:11:08 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:12:05 [model_runner_v1.py:2074] Graph capturing finished in 57 secs, took 0.17 GiB
INFO 12-13 12:12:05 [core.py:172] init engine (profile, create kv cache, warmup model) took 76.39 seconds
WARNING 12-13 12:12:06 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:12:06 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:12:06 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:12:06 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:12:06 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:06<00:27,  6.81s/it]Warmup iterations:  40%|████      | 2/5 [00:13<00:20,  6.75s/it]Warmup iterations:  60%|██████    | 3/5 [00:20<00:13,  6.72s/it]Warmup iterations:  80%|████████  | 4/5 [00:26<00:06,  6.72s/it]Warmup iterations: 100%|██████████| 5/5 [00:33<00:00,  6.74s/it]Warmup iterations: 100%|██████████| 5/5 [00:33<00:00,  6.74s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:06<01:00,  6.74s/it]Profiling iterations:  20%|██        | 2/10 [00:13<00:53,  6.74s/it]Profiling iterations:  30%|███       | 3/10 [00:20<00:47,  6.74s/it]Profiling iterations:  40%|████      | 4/10 [00:26<00:40,  6.74s/it]Profiling iterations:  50%|█████     | 5/10 [00:33<00:33,  6.74s/it]Profiling iterations:  60%|██████    | 6/10 [00:40<00:26,  6.75s/it]Profiling iterations:  70%|███████   | 7/10 [00:47<00:20,  6.75s/it]Profiling iterations:  80%|████████  | 8/10 [00:53<00:13,  6.74s/it]Profiling iterations:  90%|█████████ | 9/10 [01:00<00:06,  6.74s/it]Profiling iterations: 100%|██████████| 10/10 [01:07<00:00,  6.74s/it]Profiling iterations: 100%|██████████| 10/10 [01:07<00:00,  6.74s/it]
Avg latency: 6.743287703953683 seconds
10% percentile latency: 6.731599290156737 seconds
25% percentile latency: 6.73741263395641 seconds
50% percentile latency: 6.743205248145387 seconds
75% percentile latency: 6.750956862582825 seconds
90% percentile latency: 6.752849186770618 seconds
99% percentile latency: 6.755191592592746 seconds
Batch-8 Input len-32 Output len-1024
INFO 12-13 12:14:05 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:14:05 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:14:05 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:14:05 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:14:07 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:14:11 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:14:11 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:14:11 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:14:11 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:14:11 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:14:11 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:14:11 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:14:28 [config.py:841] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 12-13 12:14:28 [config.py:1472] Using max model len 32768
INFO 12-13 12:14:28 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:14:28 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:14:28 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:14:28 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:14:28 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:14:29 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:14:38 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:14:38 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:14:38 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:14:38 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:14:39 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:14:42 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:14:43 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:14:43 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:14:43 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:14:43 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:14:43 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:14:43 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:14:43 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:14:43 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:14:56 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:14:56 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:14:56 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:14:56 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:14:57 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:15:03 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:15:04 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]

INFO 12-13 12:15:06 [default_loader.py:272] Loading weights took 0.69 seconds
INFO 12-13 12:15:08 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:15:16 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:15:16 [backends.py:519] Dynamo bytecode transform time: 7.10 s
INFO 12-13 12:15:18 [backends.py:193] Compiling a graph for general shape takes 1.91 s
INFO 12-13 12:15:26 [monitor.py:34] torch.compile takes 9.01 s in total
INFO 12-13 12:15:27 [worker_v1.py:181] Available memory: 55436513996, total memory: 65464696832
INFO 12-13 12:15:27 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:15:27 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:16:42 [model_runner_v1.py:2074] Graph capturing finished in 74 secs, took 0.17 GiB
INFO 12-13 12:16:42 [core.py:172] init engine (profile, create kv cache, warmup model) took 94.14 seconds
WARNING 12-13 12:16:43 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:16:43 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:16:43 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:16:43 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:16:43 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:13<00:55, 13.77s/it]Warmup iterations:  40%|████      | 2/5 [00:27<00:41, 13.72s/it]Warmup iterations:  60%|██████    | 3/5 [00:41<00:27, 13.70s/it]Warmup iterations:  80%|████████  | 4/5 [00:54<00:13, 13.69s/it]Warmup iterations: 100%|██████████| 5/5 [01:08<00:00, 13.69s/it]Warmup iterations: 100%|██████████| 5/5 [01:08<00:00, 13.70s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:13<02:03, 13.68s/it]Profiling iterations:  20%|██        | 2/10 [00:27<01:49, 13.68s/it]Profiling iterations:  30%|███       | 3/10 [00:41<01:35, 13.68s/it]Profiling iterations:  40%|████      | 4/10 [00:54<01:22, 13.69s/it]Profiling iterations:  50%|█████     | 5/10 [01:08<01:08, 13.68s/it]Profiling iterations:  60%|██████    | 6/10 [01:22<00:54, 13.68s/it]Profiling iterations:  70%|███████   | 7/10 [01:35<00:41, 13.68s/it]Profiling iterations:  80%|████████  | 8/10 [01:49<00:27, 13.69s/it]Profiling iterations:  90%|█████████ | 9/10 [02:03<00:13, 13.72s/it]Profiling iterations: 100%|██████████| 10/10 [02:17<00:00, 13.73s/it]Profiling iterations: 100%|██████████| 10/10 [02:17<00:00, 13.70s/it]
Avg latency: 13.700664268340915 seconds
10% percentile latency: 13.676963111013174 seconds
25% percentile latency: 13.678150917054154 seconds
50% percentile latency: 13.681860578013584 seconds
75% percentile latency: 13.701682321960106 seconds
90% percentile latency: 13.746954573132097 seconds
99% percentile latency: 13.786918638953939 seconds
Batch-8 Input len-64 Output len-32
INFO 12-13 12:20:26 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:20:26 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:20:26 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:20:26 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:20:28 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:20:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:20:32 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:20:32 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:20:32 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:20:32 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:20:32 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:20:32 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:20:48 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 12-13 12:20:48 [config.py:1472] Using max model len 32768
INFO 12-13 12:20:48 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:20:48 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:20:48 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:20:48 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:20:48 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:20:49 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:20:58 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:20:58 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:20:58 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:20:58 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:20:59 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:21:02 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:21:03 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:21:03 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:21:03 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:21:03 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:21:03 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:21:03 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:21:03 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:21:03 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:21:16 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:21:16 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:21:16 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:21:16 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:21:18 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:21:24 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:21:24 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.33it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]

INFO 12-13 12:21:26 [default_loader.py:272] Loading weights took 0.64 seconds
INFO 12-13 12:21:28 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:21:36 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:21:36 [backends.py:519] Dynamo bytecode transform time: 7.27 s
INFO 12-13 12:21:39 [backends.py:193] Compiling a graph for general shape takes 1.96 s
INFO 12-13 12:21:47 [monitor.py:34] torch.compile takes 9.23 s in total
INFO 12-13 12:21:48 [worker_v1.py:181] Available memory: 55435915980, total memory: 65464696832
INFO 12-13 12:21:48 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:21:48 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:23:00 [model_runner_v1.py:2074] Graph capturing finished in 71 secs, took 0.17 GiB
INFO 12-13 12:23:00 [core.py:172] init engine (profile, create kv cache, warmup model) took 91.85 seconds
WARNING 12-13 12:23:01 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:23:01 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:23:01 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:23:01 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:23:01 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:02,  1.91it/s]Warmup iterations:  40%|████      | 2/5 [00:00<00:01,  2.11it/s]Warmup iterations:  60%|██████    | 3/5 [00:01<00:00,  2.17it/s]Warmup iterations:  80%|████████  | 4/5 [00:01<00:00,  2.20it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  2.22it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  2.18it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:04,  2.25it/s]Profiling iterations:  20%|██        | 2/10 [00:00<00:03,  2.25it/s]Profiling iterations:  30%|███       | 3/10 [00:01<00:03,  2.25it/s]Profiling iterations:  40%|████      | 4/10 [00:01<00:02,  2.26it/s]Profiling iterations:  50%|█████     | 5/10 [00:02<00:02,  2.26it/s]Profiling iterations:  60%|██████    | 6/10 [00:02<00:01,  2.26it/s]Profiling iterations:  70%|███████   | 7/10 [00:03<00:01,  2.26it/s]Profiling iterations:  80%|████████  | 8/10 [00:03<00:00,  2.26it/s]Profiling iterations:  90%|█████████ | 9/10 [00:03<00:00,  2.26it/s]Profiling iterations: 100%|██████████| 10/10 [00:04<00:00,  2.26it/s]Profiling iterations: 100%|██████████| 10/10 [00:04<00:00,  2.26it/s]
Avg latency: 0.44293621606193484 seconds
10% percentile latency: 0.44141074768267574 seconds
25% percentile latency: 0.44276553113013506 seconds
50% percentile latency: 0.4432649235241115 seconds
75% percentile latency: 0.44355376879684627 seconds
90% percentile latency: 0.4438809362240136 seconds
99% percentile latency: 0.44448928838595747 seconds
Batch-8 Input len-64 Output len-64
INFO 12-13 12:23:26 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:23:26 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:23:26 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:23:26 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:23:28 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:23:31 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:23:32 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:23:32 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:23:32 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:23:32 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:23:32 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:23:32 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:23:48 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 12-13 12:23:48 [config.py:1472] Using max model len 32768
INFO 12-13 12:23:48 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:23:48 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:23:48 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:23:48 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:23:48 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:23:49 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:23:58 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:23:58 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:23:58 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:23:58 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:23:59 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:24:02 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:24:03 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:24:03 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:24:03 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:24:03 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:24:03 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:24:03 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:24:03 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:24:03 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:24:16 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:24:16 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:24:16 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:24:16 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:24:18 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:24:24 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:24:24 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]

INFO 12-13 12:24:26 [default_loader.py:272] Loading weights took 0.69 seconds
INFO 12-13 12:24:28 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:24:36 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:24:36 [backends.py:519] Dynamo bytecode transform time: 7.12 s
INFO 12-13 12:24:39 [backends.py:193] Compiling a graph for general shape takes 1.92 s
INFO 12-13 12:24:47 [monitor.py:34] torch.compile takes 9.04 s in total
INFO 12-13 12:24:48 [worker_v1.py:181] Available memory: 55436878540, total memory: 65464696832
INFO 12-13 12:24:48 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:24:48 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:26:05 [model_runner_v1.py:2074] Graph capturing finished in 77 secs, took 0.17 GiB
INFO 12-13 12:26:05 [core.py:172] init engine (profile, create kv cache, warmup model) took 96.66 seconds
WARNING 12-13 12:26:06 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:26:06 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:26:06 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:26:06 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:26:06 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:04,  1.03s/it]Warmup iterations:  40%|████      | 2/5 [00:01<00:02,  1.02it/s]Warmup iterations:  60%|██████    | 3/5 [00:02<00:01,  1.04it/s]Warmup iterations:  80%|████████  | 4/5 [00:03<00:00,  1.05it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.06it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.05it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:08,  1.06it/s]Profiling iterations:  20%|██        | 2/10 [00:01<00:07,  1.07it/s]Profiling iterations:  30%|███       | 3/10 [00:02<00:06,  1.07it/s]Profiling iterations:  40%|████      | 4/10 [00:03<00:05,  1.07it/s]Profiling iterations:  50%|█████     | 5/10 [00:04<00:04,  1.07it/s]Profiling iterations:  60%|██████    | 6/10 [00:05<00:03,  1.07it/s]Profiling iterations:  70%|███████   | 7/10 [00:06<00:02,  1.07it/s]Profiling iterations:  80%|████████  | 8/10 [00:07<00:01,  1.07it/s]Profiling iterations:  90%|█████████ | 9/10 [00:08<00:00,  1.07it/s]Profiling iterations: 100%|██████████| 10/10 [00:09<00:00,  1.07it/s]Profiling iterations: 100%|██████████| 10/10 [00:09<00:00,  1.07it/s]
Avg latency: 0.9374771934002638 seconds
10% percentile latency: 0.936545355990529 seconds
25% percentile latency: 0.9367401499766856 seconds
50% percentile latency: 0.9372074296697974 seconds
75% percentile latency: 0.9375088459346443 seconds
90% percentile latency: 0.9385071426630021 seconds
99% percentile latency: 0.9400942055694759 seconds
Batch-8 Input len-64 Output len-128
INFO 12-13 12:26:37 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:26:37 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:26:37 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:26:37 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:26:38 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:26:42 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:26:42 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:26:42 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:26:42 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:26:42 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:26:42 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:26:42 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:26:59 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 12-13 12:26:59 [config.py:1472] Using max model len 32768
INFO 12-13 12:26:59 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:26:59 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:26:59 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:26:59 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:26:59 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:27:00 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:27:08 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:27:08 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:27:08 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:27:08 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:27:10 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:27:12 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:27:14 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:27:14 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:27:14 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:27:14 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:27:14 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:27:14 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:27:14 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:27:14 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:27:27 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:27:27 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:27:27 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:27:27 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:27:28 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:27:34 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:27:34 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.33it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.33it/s]

INFO 12-13 12:27:36 [default_loader.py:272] Loading weights took 0.63 seconds
INFO 12-13 12:27:38 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:27:46 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:27:46 [backends.py:519] Dynamo bytecode transform time: 7.00 s
INFO 12-13 12:27:48 [backends.py:193] Compiling a graph for general shape takes 1.90 s
INFO 12-13 12:27:56 [monitor.py:34] torch.compile takes 8.89 s in total
INFO 12-13 12:27:57 [worker_v1.py:181] Available memory: 55437939404, total memory: 65464696832
INFO 12-13 12:27:57 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:27:57 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:29:05 [model_runner_v1.py:2074] Graph capturing finished in 68 secs, took 0.17 GiB
INFO 12-13 12:29:05 [core.py:172] init engine (profile, create kv cache, warmup model) took 87.11 seconds
WARNING 12-13 12:29:06 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:29:06 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:29:06 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:29:06 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:29:06 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:07,  1.94s/it]Warmup iterations:  40%|████      | 2/5 [00:03<00:05,  1.90s/it]Warmup iterations:  60%|██████    | 3/5 [00:05<00:03,  1.88s/it]Warmup iterations:  80%|████████  | 4/5 [00:07<00:01,  1.87s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.86s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.88s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:16,  1.86s/it]Profiling iterations:  20%|██        | 2/10 [00:03<00:14,  1.86s/it]Profiling iterations:  30%|███       | 3/10 [00:05<00:13,  1.86s/it]Profiling iterations:  40%|████      | 4/10 [00:07<00:11,  1.86s/it]Profiling iterations:  50%|█████     | 5/10 [00:09<00:09,  1.86s/it]Profiling iterations:  60%|██████    | 6/10 [00:11<00:07,  1.86s/it]Profiling iterations:  70%|███████   | 7/10 [00:13<00:05,  1.86s/it]Profiling iterations:  80%|████████  | 8/10 [00:14<00:03,  1.86s/it]Profiling iterations:  90%|█████████ | 9/10 [00:16<00:01,  1.86s/it]Profiling iterations: 100%|██████████| 10/10 [00:18<00:00,  1.86s/it]Profiling iterations: 100%|██████████| 10/10 [00:18<00:00,  1.86s/it]
Avg latency: 1.8598886894527822 seconds
10% percentile latency: 1.8577776577789336 seconds
25% percentile latency: 1.8585605532862246 seconds
50% percentile latency: 1.859232657821849 seconds
75% percentile latency: 1.8623621757142246 seconds
90% percentile latency: 1.8633001496549695 seconds
99% percentile latency: 1.8633673479175195 seconds
Batch-8 Input len-64 Output len-256
INFO 12-13 12:29:51 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:29:51 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:29:51 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:29:51 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:29:52 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:29:56 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:29:56 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:29:56 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:29:56 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:29:56 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:29:56 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:29:56 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:30:13 [config.py:841] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 12-13 12:30:13 [config.py:1472] Using max model len 32768
INFO 12-13 12:30:13 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:30:13 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:30:13 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:30:13 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:30:13 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:30:14 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:30:22 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:30:22 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:30:22 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:30:22 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:30:24 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:30:26 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:30:27 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:30:28 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:30:28 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:30:28 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:30:28 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:30:28 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:30:28 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:30:28 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:30:41 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:30:41 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:30:41 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:30:41 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:30:42 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:30:48 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:30:49 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]

INFO 12-13 12:30:51 [default_loader.py:272] Loading weights took 0.65 seconds
INFO 12-13 12:30:53 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:31:01 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:31:01 [backends.py:519] Dynamo bytecode transform time: 7.24 s
INFO 12-13 12:31:03 [backends.py:193] Compiling a graph for general shape takes 1.95 s
INFO 12-13 12:31:11 [monitor.py:34] torch.compile takes 9.19 s in total
INFO 12-13 12:31:12 [worker_v1.py:181] Available memory: 55437816524, total memory: 65464696832
INFO 12-13 12:31:12 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:31:12 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:32:25 [model_runner_v1.py:2074] Graph capturing finished in 73 secs, took 0.17 GiB
INFO 12-13 12:32:25 [core.py:172] init engine (profile, create kv cache, warmup model) took 92.89 seconds
WARNING 12-13 12:32:26 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:32:27 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:32:27 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:32:27 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:32:27 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:03<00:14,  3.62s/it]Warmup iterations:  40%|████      | 2/5 [00:07<00:10,  3.56s/it]Warmup iterations:  60%|██████    | 3/5 [00:10<00:07,  3.55s/it]Warmup iterations:  80%|████████  | 4/5 [00:14<00:03,  3.54s/it]Warmup iterations: 100%|██████████| 5/5 [00:17<00:00,  3.54s/it]Warmup iterations: 100%|██████████| 5/5 [00:17<00:00,  3.55s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:03<00:31,  3.55s/it]Profiling iterations:  20%|██        | 2/10 [00:07<00:28,  3.54s/it]Profiling iterations:  30%|███       | 3/10 [00:10<00:24,  3.49s/it]Profiling iterations:  40%|████      | 4/10 [00:13<00:20,  3.45s/it]Profiling iterations:  50%|█████     | 5/10 [00:17<00:17,  3.44s/it]Profiling iterations:  60%|██████    | 6/10 [00:20<00:13,  3.44s/it]Profiling iterations:  70%|███████   | 7/10 [00:24<00:10,  3.44s/it]Profiling iterations:  80%|████████  | 8/10 [00:27<00:06,  3.46s/it]Profiling iterations:  90%|█████████ | 9/10 [00:31<00:03,  3.47s/it]Profiling iterations: 100%|██████████| 10/10 [00:34<00:00,  3.47s/it]Profiling iterations: 100%|██████████| 10/10 [00:34<00:00,  3.47s/it]
Avg latency: 3.466351762926206 seconds
10% percentile latency: 3.415271434513852 seconds
25% percentile latency: 3.4332333707716316 seconds
50% percentile latency: 3.464368194807321 seconds
75% percentile latency: 3.487399539211765 seconds
90% percentile latency: 3.533499143924564 seconds
99% percentile latency: 3.547404623720795 seconds
Batch-8 Input len-64 Output len-512
INFO 12-13 12:33:37 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:33:37 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:33:37 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:33:37 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:33:38 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:33:42 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:33:42 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:33:42 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:33:42 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:33:42 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:33:42 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:33:42 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:33:59 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-13 12:33:59 [config.py:1472] Using max model len 32768
INFO 12-13 12:33:59 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:33:59 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:33:59 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:33:59 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:33:59 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:34:00 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:34:09 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:34:09 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:34:09 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:34:09 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:34:10 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:34:13 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:34:14 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:34:14 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:34:14 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:34:14 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:34:14 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:34:14 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:34:14 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:34:14 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:34:27 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:34:27 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:34:27 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:34:27 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:34:28 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:34:35 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:34:35 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]

INFO 12-13 12:34:37 [default_loader.py:272] Loading weights took 0.66 seconds
INFO 12-13 12:34:39 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:34:47 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:34:47 [backends.py:519] Dynamo bytecode transform time: 7.10 s
INFO 12-13 12:34:49 [backends.py:193] Compiling a graph for general shape takes 1.89 s
INFO 12-13 12:34:57 [monitor.py:34] torch.compile takes 8.99 s in total
INFO 12-13 12:34:59 [worker_v1.py:181] Available memory: 55438566092, total memory: 65464696832
INFO 12-13 12:34:59 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:34:59 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:36:16 [model_runner_v1.py:2074] Graph capturing finished in 77 secs, took 0.17 GiB
INFO 12-13 12:36:16 [core.py:172] init engine (profile, create kv cache, warmup model) took 97.34 seconds
WARNING 12-13 12:36:17 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:36:18 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:36:18 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:36:18 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:36:18 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:07<00:28,  7.01s/it]Warmup iterations:  40%|████      | 2/5 [00:13<00:20,  6.96s/it]Warmup iterations:  60%|██████    | 3/5 [00:20<00:13,  6.95s/it]Warmup iterations:  80%|████████  | 4/5 [00:27<00:06,  6.94s/it]Warmup iterations: 100%|██████████| 5/5 [00:34<00:00,  6.94s/it]Warmup iterations: 100%|██████████| 5/5 [00:34<00:00,  6.95s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:06<01:02,  6.94s/it]Profiling iterations:  20%|██        | 2/10 [00:13<00:55,  6.94s/it]Profiling iterations:  30%|███       | 3/10 [00:20<00:48,  6.94s/it]Profiling iterations:  40%|████      | 4/10 [00:27<00:41,  6.96s/it]Profiling iterations:  50%|█████     | 5/10 [00:34<00:34,  6.96s/it]Profiling iterations:  60%|██████    | 6/10 [00:41<00:27,  6.95s/it]Profiling iterations:  70%|███████   | 7/10 [00:48<00:20,  6.95s/it]Profiling iterations:  80%|████████  | 8/10 [00:55<00:13,  6.96s/it]Profiling iterations:  90%|█████████ | 9/10 [01:02<00:06,  6.96s/it]Profiling iterations: 100%|██████████| 10/10 [01:09<00:00,  6.96s/it]Profiling iterations: 100%|██████████| 10/10 [01:09<00:00,  6.95s/it]
Avg latency: 6.954713145131246 seconds
10% percentile latency: 6.934370338497684 seconds
25% percentile latency: 6.942030369886197 seconds
50% percentile latency: 6.956964953569695 seconds
75% percentile latency: 6.96616919839289 seconds
90% percentile latency: 6.97105827066116 seconds
99% percentile latency: 6.98126594198402 seconds
Batch-8 Input len-64 Output len-1024
INFO 12-13 12:38:20 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:38:20 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:38:20 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:38:20 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:38:21 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:38:25 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:38:25 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:38:25 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:38:25 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:38:25 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:38:25 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:38:25 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:38:42 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-13 12:38:42 [config.py:1472] Using max model len 32768
INFO 12-13 12:38:42 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:38:42 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:38:42 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:38:42 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:38:42 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:38:43 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:38:51 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:38:51 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:38:51 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:38:51 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:38:53 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:38:55 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:38:56 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:38:57 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:38:57 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:38:57 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:38:57 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:38:57 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:38:57 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:38:57 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:39:10 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:39:10 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:39:10 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:39:10 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:39:11 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:39:17 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:39:17 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]

INFO 12-13 12:39:19 [default_loader.py:272] Loading weights took 0.65 seconds
INFO 12-13 12:39:20 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:39:27 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:39:27 [backends.py:519] Dynamo bytecode transform time: 6.82 s
INFO 12-13 12:39:30 [backends.py:193] Compiling a graph for general shape takes 1.81 s
INFO 12-13 12:39:37 [monitor.py:34] torch.compile takes 8.63 s in total
INFO 12-13 12:39:38 [worker_v1.py:181] Available memory: 55436350156, total memory: 65464696832
INFO 12-13 12:39:38 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:39:38 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:40:21 [model_runner_v1.py:2074] Graph capturing finished in 42 secs, took 0.17 GiB
INFO 12-13 12:40:21 [core.py:172] init engine (profile, create kv cache, warmup model) took 60.88 seconds
WARNING 12-13 12:40:22 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:40:22 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:40:22 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:40:22 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:40:22 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:13<00:54, 13.52s/it]Warmup iterations:  40%|████      | 2/5 [00:28<00:43, 14.46s/it]Warmup iterations:  60%|██████    | 3/5 [00:43<00:29, 14.75s/it]Warmup iterations:  80%|████████  | 4/5 [00:58<00:14, 14.89s/it]Warmup iterations: 100%|██████████| 5/5 [01:13<00:00, 14.97s/it]Warmup iterations: 100%|██████████| 5/5 [01:13<00:00, 14.79s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:15<02:16, 15.11s/it]Profiling iterations:  20%|██        | 2/10 [00:30<02:00, 15.12s/it]Profiling iterations:  30%|███       | 3/10 [00:45<01:45, 15.10s/it]Profiling iterations:  40%|████      | 4/10 [00:59<01:29, 14.91s/it]Profiling iterations:  50%|█████     | 5/10 [01:15<01:14, 14.97s/it]Profiling iterations:  60%|██████    | 6/10 [01:30<01:00, 15.02s/it]Profiling iterations:  70%|███████   | 7/10 [01:44<00:44, 14.89s/it]Profiling iterations:  80%|████████  | 8/10 [01:58<00:29, 14.66s/it]Profiling iterations:  90%|█████████ | 9/10 [02:13<00:14, 14.74s/it]Profiling iterations: 100%|██████████| 10/10 [02:28<00:00, 14.85s/it]Profiling iterations: 100%|██████████| 10/10 [02:28<00:00, 14.89s/it]
Avg latency: 14.893336696131154 seconds
10% percentile latency: 14.563962354091927 seconds
25% percentile latency: 14.708601231803186 seconds
50% percentile latency: 15.07978048431687 seconds
75% percentile latency: 15.103449514484964 seconds
90% percentile latency: 15.114638415677472 seconds
99% percentile latency: 15.131187847261318 seconds
Batch-8 Input len-128 Output len-32
INFO 12-13 12:44:23 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:44:23 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:44:23 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:44:23 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:44:24 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:44:28 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:44:28 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:44:28 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:44:28 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:44:28 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:44:28 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:44:28 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:44:45 [config.py:841] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 12-13 12:44:45 [config.py:1472] Using max model len 32768
INFO 12-13 12:44:45 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:44:45 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:44:45 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:44:45 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:44:45 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:44:46 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:44:55 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:44:55 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:44:55 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:44:55 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:44:56 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:44:58 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:45:00 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:45:00 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:45:00 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:45:00 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:45:00 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:45:00 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:45:00 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:45:00 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:45:13 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:45:13 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:45:13 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:45:13 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:45:14 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:45:21 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:45:21 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]

INFO 12-13 12:45:23 [default_loader.py:272] Loading weights took 0.69 seconds
INFO 12-13 12:45:25 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:45:33 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:45:33 [backends.py:519] Dynamo bytecode transform time: 7.23 s
INFO 12-13 12:45:36 [backends.py:193] Compiling a graph for general shape takes 1.97 s
INFO 12-13 12:45:44 [monitor.py:34] torch.compile takes 9.20 s in total
INFO 12-13 12:45:45 [worker_v1.py:181] Available memory: 55436796620, total memory: 65464696832
INFO 12-13 12:45:45 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:45:45 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:47:03 [model_runner_v1.py:2074] Graph capturing finished in 78 secs, took 0.17 GiB
INFO 12-13 12:47:03 [core.py:172] init engine (profile, create kv cache, warmup model) took 98.16 seconds
WARNING 12-13 12:47:04 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:47:05 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:47:05 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:47:05 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:47:05 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:02,  1.74it/s]Warmup iterations:  40%|████      | 2/5 [00:01<00:01,  1.92it/s]Warmup iterations:  60%|██████    | 3/5 [00:01<00:01,  1.99it/s]Warmup iterations:  80%|████████  | 4/5 [00:02<00:00,  2.02it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  2.00it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:04,  2.07it/s]Profiling iterations:  20%|██        | 2/10 [00:00<00:03,  2.07it/s]Profiling iterations:  30%|███       | 3/10 [00:01<00:03,  2.07it/s]Profiling iterations:  40%|████      | 4/10 [00:01<00:02,  2.07it/s]Profiling iterations:  50%|█████     | 5/10 [00:02<00:02,  2.07it/s]Profiling iterations:  60%|██████    | 6/10 [00:02<00:01,  2.07it/s]Profiling iterations:  70%|███████   | 7/10 [00:03<00:01,  2.07it/s]Profiling iterations:  80%|████████  | 8/10 [00:03<00:00,  2.07it/s]Profiling iterations:  90%|█████████ | 9/10 [00:04<00:00,  2.07it/s]Profiling iterations: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]Profiling iterations: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]
Avg latency: 0.48233402776531875 seconds
10% percentile latency: 0.4813864051364362 seconds
25% percentile latency: 0.48167090176139027 seconds
50% percentile latency: 0.482073420425877 seconds
75% percentile latency: 0.4830325642833486 seconds
90% percentile latency: 0.48340786024928095 seconds
99% percentile latency: 0.48395990991033616 seconds
Batch-8 Input len-128 Output len-64
INFO 12-13 12:47:30 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:47:30 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:47:30 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:47:30 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:47:32 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:47:35 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:47:36 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:47:36 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:47:36 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:47:36 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:47:36 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:47:36 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:47:52 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-13 12:47:52 [config.py:1472] Using max model len 32768
INFO 12-13 12:47:52 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:47:52 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:47:52 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:47:52 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:47:52 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:47:53 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:48:02 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:48:02 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:48:02 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:48:02 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:48:03 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:48:06 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:48:07 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:48:07 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:48:07 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:48:07 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:48:07 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:48:07 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:48:07 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:48:07 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:48:20 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:48:20 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:48:20 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:48:20 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:48:22 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:48:28 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:48:28 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.36it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.36it/s]

INFO 12-13 12:48:30 [default_loader.py:272] Loading weights took 0.65 seconds
INFO 12-13 12:48:31 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:48:38 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:48:38 [backends.py:519] Dynamo bytecode transform time: 6.90 s
INFO 12-13 12:48:40 [backends.py:193] Compiling a graph for general shape takes 1.82 s
INFO 12-13 12:48:48 [monitor.py:34] torch.compile takes 8.73 s in total
INFO 12-13 12:48:49 [worker_v1.py:181] Available memory: 55436157644, total memory: 65464696832
INFO 12-13 12:48:49 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:48:49 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:49:32 [model_runner_v1.py:2074] Graph capturing finished in 43 secs, took 0.17 GiB
INFO 12-13 12:49:32 [core.py:172] init engine (profile, create kv cache, warmup model) took 61.46 seconds
WARNING 12-13 12:49:33 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:49:34 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:49:34 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:49:34 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:49:34 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:03,  1.02it/s]Warmup iterations:  40%|████      | 2/5 [00:01<00:02,  1.04it/s]Warmup iterations:  60%|██████    | 3/5 [00:02<00:01,  1.02it/s]Warmup iterations:  80%|████████  | 4/5 [00:03<00:00,  1.01it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.01it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.02it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:09,  1.00s/it]Profiling iterations:  20%|██        | 2/10 [00:01<00:07,  1.01it/s]Profiling iterations:  30%|███       | 3/10 [00:02<00:06,  1.00it/s]Profiling iterations:  40%|████      | 4/10 [00:03<00:05,  1.00it/s]Profiling iterations:  50%|█████     | 5/10 [00:04<00:04,  1.02it/s]Profiling iterations:  60%|██████    | 6/10 [00:05<00:03,  1.03it/s]Profiling iterations:  70%|███████   | 7/10 [00:06<00:02,  1.04it/s]Profiling iterations:  80%|████████  | 8/10 [00:07<00:01,  1.04it/s]Profiling iterations:  90%|█████████ | 9/10 [00:08<00:00,  1.05it/s]Profiling iterations: 100%|██████████| 10/10 [00:09<00:00,  1.05it/s]Profiling iterations: 100%|██████████| 10/10 [00:09<00:00,  1.03it/s]
Avg latency: 0.9683574688620865 seconds
10% percentile latency: 0.9481658442877233 seconds
25% percentile latency: 0.9495953939622268 seconds
50% percentile latency: 0.9531842954456806 seconds
75% percentile latency: 0.990381030831486 seconds
90% percentile latency: 1.0031381857115775 seconds
99% percentile latency: 1.0043192840879782 seconds
Batch-8 Input len-128 Output len-128
INFO 12-13 12:50:06 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:50:06 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:50:06 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:50:06 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:50:07 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:50:11 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:50:11 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:50:11 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:50:11 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:50:11 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:50:11 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:50:11 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:50:28 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-13 12:50:28 [config.py:1472] Using max model len 32768
INFO 12-13 12:50:28 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:50:28 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:50:28 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:50:28 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:50:28 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:50:29 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:50:38 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:50:38 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:50:38 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:50:38 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:50:39 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:50:42 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:50:43 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:50:43 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:50:43 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:50:43 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:50:43 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:50:43 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:50:43 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:50:43 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:50:56 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:50:56 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:50:56 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:50:56 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:50:57 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:51:04 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:51:04 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]

INFO 12-13 12:51:06 [default_loader.py:272] Loading weights took 0.66 seconds
INFO 12-13 12:51:08 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:51:16 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:51:16 [backends.py:519] Dynamo bytecode transform time: 7.08 s
INFO 12-13 12:51:18 [backends.py:193] Compiling a graph for general shape takes 1.89 s
INFO 12-13 12:51:26 [monitor.py:34] torch.compile takes 8.97 s in total
INFO 12-13 12:51:28 [worker_v1.py:181] Available memory: 55436407500, total memory: 65464696832
INFO 12-13 12:51:28 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:51:28 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:52:40 [model_runner_v1.py:2074] Graph capturing finished in 73 secs, took 0.17 GiB
INFO 12-13 12:52:40 [core.py:172] init engine (profile, create kv cache, warmup model) took 92.45 seconds
WARNING 12-13 12:52:41 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:52:42 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:52:42 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:52:42 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:52:42 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:07,  1.87s/it]Warmup iterations:  40%|████      | 2/5 [00:03<00:05,  1.82s/it]Warmup iterations:  60%|██████    | 3/5 [00:05<00:03,  1.80s/it]Warmup iterations:  80%|████████  | 4/5 [00:07<00:01,  1.80s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.79s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.80s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:16,  1.78s/it]Profiling iterations:  20%|██        | 2/10 [00:03<00:14,  1.78s/it]Profiling iterations:  30%|███       | 3/10 [00:05<00:12,  1.78s/it]Profiling iterations:  40%|████      | 4/10 [00:07<00:10,  1.79s/it]Profiling iterations:  50%|█████     | 5/10 [00:08<00:08,  1.79s/it]Profiling iterations:  60%|██████    | 6/10 [00:10<00:07,  1.79s/it]Profiling iterations:  70%|███████   | 7/10 [00:12<00:05,  1.78s/it]Profiling iterations:  80%|████████  | 8/10 [00:14<00:03,  1.78s/it]Profiling iterations:  90%|█████████ | 9/10 [00:16<00:01,  1.80s/it]Profiling iterations: 100%|██████████| 10/10 [00:17<00:00,  1.82s/it]Profiling iterations: 100%|██████████| 10/10 [00:17<00:00,  1.80s/it]
Avg latency: 1.7977554636076092 seconds
10% percentile latency: 1.7763080024160445 seconds
25% percentile latency: 1.7803425301099196 seconds
50% percentile latency: 1.7850631747860461 seconds
75% percentile latency: 1.7912730391835794 seconds
90% percentile latency: 1.861318673333153 seconds
99% percentile latency: 1.8618453855486587 seconds
Batch-8 Input len-128 Output len-256
INFO 12-13 12:53:27 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:53:27 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:53:27 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:53:27 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:53:28 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:53:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:53:32 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:53:32 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:53:32 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:53:32 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:53:32 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:53:32 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:53:49 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 12-13 12:53:49 [config.py:1472] Using max model len 32768
INFO 12-13 12:53:49 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:53:49 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:53:49 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:53:49 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:53:49 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:53:50 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:53:58 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:53:58 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:53:58 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:53:58 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:54:00 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:54:02 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:54:03 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:54:04 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:54:04 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:54:04 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:54:04 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:54:04 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:54:04 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:54:04 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:54:17 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:54:17 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:54:17 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:54:17 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:54:18 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:54:25 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:54:25 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]

INFO 12-13 12:54:27 [default_loader.py:272] Loading weights took 0.51 seconds
INFO 12-13 12:54:29 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:54:37 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:54:37 [backends.py:519] Dynamo bytecode transform time: 7.29 s
INFO 12-13 12:54:40 [backends.py:193] Compiling a graph for general shape takes 1.98 s
INFO 12-13 12:54:48 [monitor.py:34] torch.compile takes 9.27 s in total
INFO 12-13 12:54:49 [worker_v1.py:181] Available memory: 55436010188, total memory: 65464696832
INFO 12-13 12:54:49 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:54:49 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 12:56:10 [model_runner_v1.py:2074] Graph capturing finished in 81 secs, took 0.17 GiB
INFO 12-13 12:56:10 [core.py:172] init engine (profile, create kv cache, warmup model) took 100.88 seconds
WARNING 12-13 12:56:11 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 12:56:11 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:56:11 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:56:11 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:56:11 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:03<00:15,  3.86s/it]Warmup iterations:  40%|████      | 2/5 [00:07<00:11,  3.81s/it]Warmup iterations:  60%|██████    | 3/5 [00:11<00:07,  3.80s/it]Warmup iterations:  80%|████████  | 4/5 [00:15<00:03,  3.80s/it]Warmup iterations: 100%|██████████| 5/5 [00:18<00:00,  3.79s/it]Warmup iterations: 100%|██████████| 5/5 [00:18<00:00,  3.80s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:03<00:33,  3.78s/it]Profiling iterations:  20%|██        | 2/10 [00:07<00:30,  3.78s/it]Profiling iterations:  30%|███       | 3/10 [00:11<00:26,  3.78s/it]Profiling iterations:  40%|████      | 4/10 [00:15<00:22,  3.78s/it]Profiling iterations:  50%|█████     | 5/10 [00:18<00:18,  3.77s/it]Profiling iterations:  60%|██████    | 6/10 [00:22<00:15,  3.78s/it]Profiling iterations:  70%|███████   | 7/10 [00:26<00:11,  3.77s/it]Profiling iterations:  80%|████████  | 8/10 [00:30<00:07,  3.77s/it]Profiling iterations:  90%|█████████ | 9/10 [00:33<00:03,  3.77s/it]Profiling iterations: 100%|██████████| 10/10 [00:37<00:00,  3.76s/it]Profiling iterations: 100%|██████████| 10/10 [00:37<00:00,  3.77s/it]
Avg latency: 3.7705306021030993 seconds
10% percentile latency: 3.7598915295209734 seconds
25% percentile latency: 3.7639387510716915 seconds
50% percentile latency: 3.7718811691738665 seconds
75% percentile latency: 3.7763743319083005 seconds
90% percentile latency: 3.7789591046050193 seconds
99% percentile latency: 3.77916280974634 seconds
Batch-8 Input len-128 Output len-512
INFO 12-13 12:57:24 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:57:24 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:57:24 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:57:24 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:57:25 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:57:29 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:57:29 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:57:29 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:57:29 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:57:29 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:57:29 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:57:29 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:57:46 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-13 12:57:46 [config.py:1472] Using max model len 32768
INFO 12-13 12:57:46 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 12:57:46 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 12:57:46 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 12:57:46 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 12:57:46 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 12:57:47 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 12:57:56 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:57:56 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:57:56 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:57:56 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:57:57 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:58:00 [core.py:526] Waiting for init message from front-end.
INFO 12-13 12:58:01 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 12:58:01 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 12:58:01 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 12:58:01 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 12:58:01 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 12:58:01 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 12:58:01 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 12:58:01 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 12:58:14 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 12:58:14 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 12:58:14 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 12:58:14 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 12:58:15 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 12:58:22 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 12:58:22 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]

INFO 12-13 12:58:24 [default_loader.py:272] Loading weights took 0.53 seconds
INFO 12-13 12:58:26 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 12:58:34 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 12:58:34 [backends.py:519] Dynamo bytecode transform time: 7.22 s
INFO 12-13 12:58:37 [backends.py:193] Compiling a graph for general shape takes 1.98 s
INFO 12-13 12:58:44 [monitor.py:34] torch.compile takes 9.20 s in total
INFO 12-13 12:58:46 [worker_v1.py:181] Available memory: 55436382924, total memory: 65464696832
INFO 12-13 12:58:46 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 12:58:46 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:00:07 [model_runner_v1.py:2074] Graph capturing finished in 81 secs, took 0.17 GiB
INFO 12-13 13:00:07 [core.py:172] init engine (profile, create kv cache, warmup model) took 101.04 seconds
WARNING 12-13 13:00:08 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:00:08 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:00:08 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:00:08 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:00:08 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:07<00:29,  7.49s/it]Warmup iterations:  40%|████      | 2/5 [00:14<00:22,  7.35s/it]Warmup iterations:  60%|██████    | 3/5 [00:22<00:14,  7.31s/it]Warmup iterations:  80%|████████  | 4/5 [00:29<00:07,  7.32s/it]Warmup iterations: 100%|██████████| 5/5 [00:36<00:00,  7.32s/it]Warmup iterations: 100%|██████████| 5/5 [00:36<00:00,  7.33s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:07<01:06,  7.34s/it]Profiling iterations:  20%|██        | 2/10 [00:14<00:58,  7.35s/it]Profiling iterations:  30%|███       | 3/10 [00:22<00:51,  7.35s/it]Profiling iterations:  40%|████      | 4/10 [00:29<00:44,  7.36s/it]Profiling iterations:  50%|█████     | 5/10 [00:36<00:36,  7.36s/it]Profiling iterations:  60%|██████    | 6/10 [00:44<00:29,  7.37s/it]Profiling iterations:  70%|███████   | 7/10 [00:51<00:22,  7.37s/it]Profiling iterations:  80%|████████  | 8/10 [00:58<00:14,  7.38s/it]Profiling iterations:  90%|█████████ | 9/10 [01:06<00:07,  7.40s/it]Profiling iterations: 100%|██████████| 10/10 [01:13<00:00,  7.43s/it]Profiling iterations: 100%|██████████| 10/10 [01:13<00:00,  7.39s/it]
Avg latency: 7.389788376074284 seconds
10% percentile latency: 7.355511578591541 seconds
25% percentile latency: 7.358479168266058 seconds
50% percentile latency: 7.3734916660469025 seconds
75% percentile latency: 7.40207430254668 seconds
90% percentile latency: 7.441895208880306 seconds
99% percentile latency: 7.501707981163636 seconds
Batch-8 Input len-128 Output len-1024
INFO 12-13 13:02:17 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:02:17 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:02:17 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:02:17 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:02:18 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:02:22 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:02:22 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:02:22 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:02:22 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:02:22 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:02:22 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:02:22 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:02:39 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 12-13 13:02:39 [config.py:1472] Using max model len 32768
INFO 12-13 13:02:39 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:02:39 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:02:39 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:02:39 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:02:39 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:02:40 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:02:49 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:02:49 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:02:49 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:02:49 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:02:50 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:02:53 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:02:54 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:02:54 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:02:54 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:02:54 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:02:54 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:02:54 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:02:54 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:02:54 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:03:07 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:03:07 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:03:07 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:03:07 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:03:08 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:03:16 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:03:16 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]

INFO 12-13 13:03:18 [default_loader.py:272] Loading weights took 0.70 seconds
INFO 12-13 13:03:20 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:03:28 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:03:28 [backends.py:519] Dynamo bytecode transform time: 7.02 s
INFO 12-13 13:03:31 [backends.py:193] Compiling a graph for general shape takes 1.89 s
INFO 12-13 13:03:38 [monitor.py:34] torch.compile takes 8.91 s in total
INFO 12-13 13:03:40 [worker_v1.py:181] Available memory: 55436169932, total memory: 65464696832
INFO 12-13 13:03:40 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:03:40 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:04:52 [model_runner_v1.py:2074] Graph capturing finished in 72 secs, took 0.17 GiB
INFO 12-13 13:04:52 [core.py:172] init engine (profile, create kv cache, warmup model) took 91.98 seconds
WARNING 12-13 13:04:53 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:04:54 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:04:54 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:04:54 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:04:54 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:15<01:00, 15.10s/it]Warmup iterations:  40%|████      | 2/5 [00:30<00:45, 15.06s/it]Warmup iterations:  60%|██████    | 3/5 [00:45<00:30, 15.09s/it]Warmup iterations:  80%|████████  | 4/5 [01:00<00:15, 15.18s/it]Warmup iterations: 100%|██████████| 5/5 [01:15<00:00, 15.22s/it]Warmup iterations: 100%|██████████| 5/5 [01:15<00:00, 15.18s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:15<02:17, 15.23s/it]Profiling iterations:  20%|██        | 2/10 [00:30<02:02, 15.26s/it]Profiling iterations:  30%|███       | 3/10 [00:45<01:46, 15.22s/it]Profiling iterations:  40%|████      | 4/10 [01:00<01:31, 15.19s/it]Profiling iterations:  50%|█████     | 5/10 [01:16<01:15, 15.19s/it]Profiling iterations:  60%|██████    | 6/10 [01:31<01:00, 15.22s/it]Profiling iterations:  70%|███████   | 7/10 [01:46<00:45, 15.23s/it]Profiling iterations:  80%|████████  | 8/10 [02:01<00:30, 15.23s/it]Profiling iterations:  90%|█████████ | 9/10 [02:17<00:15, 15.25s/it]Profiling iterations: 100%|██████████| 10/10 [02:32<00:00, 15.27s/it]Profiling iterations: 100%|██████████| 10/10 [02:32<00:00, 15.24s/it]
Avg latency: 15.239303449168801 seconds
10% percentile latency: 15.15954080093652 seconds
25% percentile latency: 15.190307534532622 seconds
50% percentile latency: 15.24965781881474 seconds
75% percentile latency: 15.28095277841203 seconds
90% percentile latency: 15.300408468721434 seconds
99% percentile latency: 15.31200115365442 seconds
Batch-8 Input len-256 Output len-32
INFO 12-13 13:09:00 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:09:00 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:09:00 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:09:00 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:09:01 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:09:05 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:09:05 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:09:05 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:09:05 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:09:05 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:09:05 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:09:05 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:09:22 [config.py:841] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 12-13 13:09:22 [config.py:1472] Using max model len 32768
INFO 12-13 13:09:22 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:09:22 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:09:22 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:09:22 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:09:22 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:09:23 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:09:31 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:09:31 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:09:31 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:09:31 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:09:33 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:09:35 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:09:37 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:09:37 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:09:37 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:09:37 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:09:37 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:09:37 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:09:37 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:09:37 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:09:50 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:09:50 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:09:50 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:09:50 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:09:51 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:09:57 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:09:58 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]

INFO 12-13 13:10:00 [default_loader.py:272] Loading weights took 0.66 seconds
INFO 12-13 13:10:02 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:10:10 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:10:10 [backends.py:519] Dynamo bytecode transform time: 7.26 s
INFO 12-13 13:10:13 [backends.py:193] Compiling a graph for general shape takes 1.98 s
INFO 12-13 13:10:20 [monitor.py:34] torch.compile takes 9.24 s in total
INFO 12-13 13:10:22 [worker_v1.py:181] Available memory: 55436227276, total memory: 65464696832
INFO 12-13 13:10:22 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:10:22 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:11:28 [model_runner_v1.py:2074] Graph capturing finished in 66 secs, took 0.17 GiB
INFO 12-13 13:11:28 [core.py:172] init engine (profile, create kv cache, warmup model) took 86.41 seconds
WARNING 12-13 13:11:29 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:11:30 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:11:30 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:11:30 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:11:30 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:02,  1.79it/s]Warmup iterations:  40%|████      | 2/5 [00:01<00:01,  1.97it/s]Warmup iterations:  60%|██████    | 3/5 [00:01<00:00,  2.04it/s]Warmup iterations:  80%|████████  | 4/5 [00:01<00:00,  2.06it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:04,  2.13it/s]Profiling iterations:  20%|██        | 2/10 [00:00<00:03,  2.12it/s]Profiling iterations:  30%|███       | 3/10 [00:01<00:03,  2.12it/s]Profiling iterations:  40%|████      | 4/10 [00:01<00:02,  2.12it/s]Profiling iterations:  50%|█████     | 5/10 [00:02<00:02,  2.13it/s]Profiling iterations:  60%|██████    | 6/10 [00:02<00:01,  2.13it/s]Profiling iterations:  70%|███████   | 7/10 [00:03<00:01,  2.13it/s]Profiling iterations:  80%|████████  | 8/10 [00:03<00:00,  2.13it/s]Profiling iterations:  90%|█████████ | 9/10 [00:04<00:00,  2.13it/s]Profiling iterations: 100%|██████████| 10/10 [00:04<00:00,  2.12it/s]Profiling iterations: 100%|██████████| 10/10 [00:04<00:00,  2.13it/s]
Avg latency: 0.46975205834023653 seconds
10% percentile latency: 0.4665444726124406 seconds
25% percentile latency: 0.4671824258984998 seconds
50% percentile latency: 0.4697623683605343 seconds
75% percentile latency: 0.4713962823152542 seconds
90% percentile latency: 0.4721131408121437 seconds
99% percentile latency: 0.4752866446459666 seconds
Batch-8 Input len-256 Output len-64
INFO 12-13 13:11:55 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:11:55 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:11:55 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:11:55 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:11:56 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:12:00 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:12:00 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:12:00 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:12:00 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:12:00 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:12:00 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:12:00 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:12:17 [config.py:841] This model supports multiple tasks: {'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-13 13:12:17 [config.py:1472] Using max model len 32768
INFO 12-13 13:12:17 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:12:17 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:12:17 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:12:17 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:12:17 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:12:18 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:12:26 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:12:26 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:12:26 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:12:26 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:12:28 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:12:30 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:12:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:12:32 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:12:32 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:12:32 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:12:32 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:12:32 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:12:32 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:12:32 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:12:45 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:12:45 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:12:45 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:12:45 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:12:46 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:12:52 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:12:53 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.78it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.78it/s]

INFO 12-13 13:12:54 [default_loader.py:272] Loading weights took 0.49 seconds
INFO 12-13 13:12:56 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:13:05 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:13:05 [backends.py:519] Dynamo bytecode transform time: 7.26 s
INFO 12-13 13:13:07 [backends.py:193] Compiling a graph for general shape takes 1.97 s
INFO 12-13 13:13:15 [monitor.py:34] torch.compile takes 9.24 s in total
INFO 12-13 13:13:16 [worker_v1.py:181] Available memory: 55436079820, total memory: 65464696832
INFO 12-13 13:13:16 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:13:16 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:14:31 [model_runner_v1.py:2074] Graph capturing finished in 74 secs, took 0.17 GiB
INFO 12-13 13:14:31 [core.py:172] init engine (profile, create kv cache, warmup model) took 94.24 seconds
WARNING 12-13 13:14:32 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:14:32 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:14:32 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:14:32 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:14:32 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:03,  1.00it/s]Warmup iterations:  40%|████      | 2/5 [00:01<00:02,  1.03it/s]Warmup iterations:  60%|██████    | 3/5 [00:02<00:01,  1.05it/s]Warmup iterations:  80%|████████  | 4/5 [00:03<00:00,  1.06it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.06it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.05it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:08,  1.06it/s]Profiling iterations:  20%|██        | 2/10 [00:01<00:07,  1.06it/s]Profiling iterations:  30%|███       | 3/10 [00:02<00:06,  1.06it/s]Profiling iterations:  40%|████      | 4/10 [00:03<00:05,  1.05it/s]Profiling iterations:  50%|█████     | 5/10 [00:04<00:04,  1.04it/s]Profiling iterations:  60%|██████    | 6/10 [00:05<00:03,  1.04it/s]Profiling iterations:  70%|███████   | 7/10 [00:06<00:02,  1.03it/s]Profiling iterations:  80%|████████  | 8/10 [00:07<00:01,  1.03it/s]Profiling iterations:  90%|█████████ | 9/10 [00:08<00:00,  1.03it/s]Profiling iterations: 100%|██████████| 10/10 [00:09<00:00,  1.03it/s]Profiling iterations: 100%|██████████| 10/10 [00:09<00:00,  1.04it/s]
Avg latency: 0.9634946355130524 seconds
10% percentile latency: 0.9441369437146931 seconds
25% percentile latency: 0.9535813546972349 seconds
50% percentile latency: 0.9694830970838666 seconds
75% percentile latency: 0.9723381794756278 seconds
90% percentile latency: 0.9736610881984233 seconds
99% percentile latency: 0.9738859044201672 seconds
Batch-8 Input len-256 Output len-128
INFO 12-13 13:15:04 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:15:04 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:15:04 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:15:04 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:15:06 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:15:09 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:15:10 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:15:10 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:15:10 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:15:10 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:15:10 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:15:10 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:15:26 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 12-13 13:15:26 [config.py:1472] Using max model len 32768
INFO 12-13 13:15:26 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:15:26 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:15:26 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:15:26 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:15:26 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:15:27 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:15:36 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:15:36 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:15:36 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:15:36 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:15:37 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:15:40 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:15:41 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:15:41 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:15:41 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:15:41 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:15:41 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:15:41 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:15:41 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:15:41 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:15:54 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:15:54 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:15:54 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:15:54 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:15:56 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:16:05 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:16:05 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]

INFO 12-13 13:16:07 [default_loader.py:272] Loading weights took 0.50 seconds
INFO 12-13 13:16:09 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:16:17 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:16:17 [backends.py:519] Dynamo bytecode transform time: 7.24 s
INFO 12-13 13:16:20 [backends.py:193] Compiling a graph for general shape takes 1.98 s
INFO 12-13 13:16:28 [monitor.py:34] torch.compile takes 9.22 s in total
INFO 12-13 13:16:30 [worker_v1.py:181] Available memory: 55436403404, total memory: 65464696832
INFO 12-13 13:16:30 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:16:30 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:17:48 [model_runner_v1.py:2074] Graph capturing finished in 78 secs, took 0.17 GiB
INFO 12-13 13:17:48 [core.py:172] init engine (profile, create kv cache, warmup model) took 98.79 seconds
WARNING 12-13 13:17:49 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:17:49 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:17:49 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:17:49 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:17:49 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:07,  1.92s/it]Warmup iterations:  40%|████      | 2/5 [00:03<00:05,  1.87s/it]Warmup iterations:  60%|██████    | 3/5 [00:05<00:03,  1.85s/it]Warmup iterations:  80%|████████  | 4/5 [00:07<00:01,  1.84s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.83s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.84s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:16,  1.82s/it]Profiling iterations:  20%|██        | 2/10 [00:03<00:14,  1.83s/it]Profiling iterations:  30%|███       | 3/10 [00:05<00:12,  1.83s/it]Profiling iterations:  40%|████      | 4/10 [00:07<00:11,  1.84s/it]Profiling iterations:  50%|█████     | 5/10 [00:09<00:09,  1.85s/it]Profiling iterations:  60%|██████    | 6/10 [00:11<00:07,  1.86s/it]Profiling iterations:  70%|███████   | 7/10 [00:12<00:05,  1.85s/it]Profiling iterations:  80%|████████  | 8/10 [00:14<00:03,  1.86s/it]Profiling iterations:  90%|█████████ | 9/10 [00:16<00:01,  1.87s/it]Profiling iterations: 100%|██████████| 10/10 [00:18<00:00,  1.88s/it]Profiling iterations: 100%|██████████| 10/10 [00:18<00:00,  1.86s/it]
Avg latency: 1.8583564141765236 seconds
10% percentile latency: 1.8276106898207218 seconds
25% percentile latency: 1.8341171459760517 seconds
50% percentile latency: 1.8589716714341193 seconds
75% percentile latency: 1.8776147294556722 seconds
90% percentile latency: 1.8915345183108 seconds
99% percentile latency: 1.8919387017702685 seconds
Batch-8 Input len-256 Output len-256
INFO 12-13 13:18:35 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:18:35 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:18:35 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:18:35 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:18:36 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:18:40 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:18:40 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:18:40 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:18:40 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:18:40 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:18:40 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:18:40 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:18:57 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 12-13 13:18:57 [config.py:1472] Using max model len 32768
INFO 12-13 13:18:57 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:18:57 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:18:57 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:18:57 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:18:57 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:18:58 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:19:06 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:19:06 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:19:06 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:19:06 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:19:08 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:19:10 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:19:12 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:19:12 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:19:12 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:19:12 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:19:12 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:19:12 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:19:12 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:19:12 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:19:25 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:19:25 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:19:25 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:19:25 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:19:26 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:19:33 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:19:33 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.78it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.77it/s]

INFO 12-13 13:19:35 [default_loader.py:272] Loading weights took 0.50 seconds
INFO 12-13 13:19:37 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:19:45 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:19:45 [backends.py:519] Dynamo bytecode transform time: 7.25 s
INFO 12-13 13:19:48 [backends.py:193] Compiling a graph for general shape takes 1.98 s
INFO 12-13 13:19:56 [monitor.py:34] torch.compile takes 9.24 s in total
INFO 12-13 13:19:57 [worker_v1.py:181] Available memory: 55436550860, total memory: 65464696832
INFO 12-13 13:19:57 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:19:57 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:20:54 [model_runner_v1.py:2074] Graph capturing finished in 57 secs, took 0.17 GiB
INFO 12-13 13:20:54 [core.py:172] init engine (profile, create kv cache, warmup model) took 77.48 seconds
WARNING 12-13 13:20:55 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:20:56 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:20:56 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:20:56 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:20:56 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:03<00:15,  3.77s/it]Warmup iterations:  40%|████      | 2/5 [00:07<00:11,  3.74s/it]Warmup iterations:  60%|██████    | 3/5 [00:11<00:07,  3.73s/it]Warmup iterations:  80%|████████  | 4/5 [00:14<00:03,  3.72s/it]Warmup iterations: 100%|██████████| 5/5 [00:18<00:00,  3.72s/it]Warmup iterations: 100%|██████████| 5/5 [00:18<00:00,  3.72s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:03<00:33,  3.73s/it]Profiling iterations:  20%|██        | 2/10 [00:07<00:29,  3.73s/it]Profiling iterations:  30%|███       | 3/10 [00:11<00:26,  3.72s/it]Profiling iterations:  40%|████      | 4/10 [00:14<00:22,  3.72s/it]Profiling iterations:  50%|█████     | 5/10 [00:18<00:18,  3.72s/it]Profiling iterations:  60%|██████    | 6/10 [00:22<00:14,  3.72s/it]Profiling iterations:  70%|███████   | 7/10 [00:26<00:11,  3.73s/it]Profiling iterations:  80%|████████  | 8/10 [00:29<00:07,  3.76s/it]Profiling iterations:  90%|█████████ | 9/10 [00:33<00:03,  3.79s/it]Profiling iterations: 100%|██████████| 10/10 [00:37<00:00,  3.80s/it]Profiling iterations: 100%|██████████| 10/10 [00:37<00:00,  3.76s/it]
Avg latency: 3.7583397788461297 seconds
10% percentile latency: 3.7158341792877763 seconds
25% percentile latency: 3.72223922831472 seconds
50% percentile latency: 3.733167989877984 seconds
75% percentile latency: 3.7937421378446743 seconds
90% percentile latency: 3.8394880907144397 seconds
99% percentile latency: 3.8582411668030545 seconds
Batch-8 Input len-256 Output len-512
INFO 12-13 13:22:10 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:22:10 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:22:10 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:22:10 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:22:11 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:22:15 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:22:15 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:22:15 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:22:15 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:22:15 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:22:15 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:22:15 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:22:32 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-13 13:22:32 [config.py:1472] Using max model len 32768
INFO 12-13 13:22:32 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:22:32 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:22:32 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:22:32 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:22:32 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:22:33 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:22:42 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:22:42 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:22:42 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:22:42 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:22:43 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:22:45 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:22:47 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:22:47 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:22:47 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:22:47 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:22:47 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:22:47 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:22:47 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:22:47 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:23:00 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:23:00 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:23:00 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:23:00 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:23:01 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:23:08 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:23:08 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.31it/s]

INFO 12-13 13:23:10 [default_loader.py:272] Loading weights took 0.64 seconds
INFO 12-13 13:23:12 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:23:20 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:23:20 [backends.py:519] Dynamo bytecode transform time: 7.21 s
INFO 12-13 13:23:22 [backends.py:193] Compiling a graph for general shape takes 1.97 s
INFO 12-13 13:23:30 [monitor.py:34] torch.compile takes 9.17 s in total
INFO 12-13 13:23:31 [worker_v1.py:181] Available memory: 55436382924, total memory: 65464696832
INFO 12-13 13:23:31 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:23:31 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:24:44 [model_runner_v1.py:2074] Graph capturing finished in 73 secs, took 0.17 GiB
INFO 12-13 13:24:44 [core.py:172] init engine (profile, create kv cache, warmup model) took 92.61 seconds
WARNING 12-13 13:24:45 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:24:46 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:24:46 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:24:46 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:24:46 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:07<00:29,  7.33s/it]Warmup iterations:  40%|████      | 2/5 [00:14<00:21,  7.29s/it]Warmup iterations:  60%|██████    | 3/5 [00:21<00:14,  7.28s/it]Warmup iterations:  80%|████████  | 4/5 [00:29<00:07,  7.27s/it]Warmup iterations: 100%|██████████| 5/5 [00:36<00:00,  7.28s/it]Warmup iterations: 100%|██████████| 5/5 [00:36<00:00,  7.28s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:07<01:05,  7.28s/it]Profiling iterations:  20%|██        | 2/10 [00:14<00:58,  7.28s/it]Profiling iterations:  30%|███       | 3/10 [00:21<00:51,  7.29s/it]Profiling iterations:  40%|████      | 4/10 [00:29<00:43,  7.28s/it]Profiling iterations:  50%|█████     | 5/10 [00:36<00:36,  7.29s/it]Profiling iterations:  60%|██████    | 6/10 [00:43<00:29,  7.29s/it]Profiling iterations:  70%|███████   | 7/10 [00:51<00:21,  7.30s/it]Profiling iterations:  80%|████████  | 8/10 [00:58<00:14,  7.29s/it]Profiling iterations:  90%|█████████ | 9/10 [01:05<00:07,  7.28s/it]Profiling iterations: 100%|██████████| 10/10 [01:12<00:00,  7.29s/it]Profiling iterations: 100%|██████████| 10/10 [01:12<00:00,  7.29s/it]
Avg latency: 7.287264607008547 seconds
10% percentile latency: 7.268717977916822 seconds
25% percentile latency: 7.2773572757141665 seconds
50% percentile latency: 7.287927614990622 seconds
75% percentile latency: 7.29871064319741 seconds
90% percentile latency: 7.3064654872287065 seconds
99% percentile latency: 7.307000372302719 seconds
Batch-8 Input len-256 Output len-1024
INFO 12-13 13:26:53 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:26:53 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:26:53 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:26:53 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:26:54 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:26:58 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:26:58 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:26:58 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:26:58 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:26:58 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:26:58 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:26:58 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:27:15 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 12-13 13:27:15 [config.py:1472] Using max model len 32768
INFO 12-13 13:27:15 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:27:15 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:27:15 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:27:15 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:27:15 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:27:16 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:27:25 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:27:25 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:27:25 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:27:25 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:27:26 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:27:29 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:27:30 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:27:30 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:27:30 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:27:30 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:27:30 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:27:30 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:27:30 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:27:30 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:27:43 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:27:43 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:27:43 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:27:43 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:27:44 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:27:51 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:27:51 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.36it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.36it/s]

INFO 12-13 13:27:53 [default_loader.py:272] Loading weights took 0.65 seconds
INFO 12-13 13:27:55 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:28:03 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:28:03 [backends.py:519] Dynamo bytecode transform time: 7.05 s
INFO 12-13 13:28:05 [backends.py:193] Compiling a graph for general shape takes 1.89 s
INFO 12-13 13:28:12 [monitor.py:34] torch.compile takes 8.94 s in total
INFO 12-13 13:28:14 [worker_v1.py:181] Available memory: 55437021900, total memory: 65464696832
INFO 12-13 13:28:14 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:28:14 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:29:06 [model_runner_v1.py:2074] Graph capturing finished in 52 secs, took 0.17 GiB
INFO 12-13 13:29:06 [core.py:172] init engine (profile, create kv cache, warmup model) took 71.77 seconds
WARNING 12-13 13:29:07 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:29:08 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:29:08 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:29:08 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:29:08 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:13<00:55, 13.84s/it]Warmup iterations:  40%|████      | 2/5 [00:27<00:41, 13.78s/it]Warmup iterations:  60%|██████    | 3/5 [00:41<00:27, 13.76s/it]Warmup iterations:  80%|████████  | 4/5 [00:54<00:13, 13.66s/it]Warmup iterations: 100%|██████████| 5/5 [01:08<00:00, 13.68s/it]Warmup iterations: 100%|██████████| 5/5 [01:08<00:00, 13.71s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:13<02:04, 13.81s/it]Profiling iterations:  20%|██        | 2/10 [00:27<01:52, 14.00s/it]Profiling iterations:  30%|███       | 3/10 [00:43<01:41, 14.53s/it]Profiling iterations:  40%|████      | 4/10 [00:58<01:28, 14.76s/it]Profiling iterations:  50%|█████     | 5/10 [01:13<01:14, 14.81s/it]Profiling iterations:  60%|██████    | 6/10 [01:28<00:59, 14.85s/it]Profiling iterations:  70%|███████   | 7/10 [01:43<00:44, 14.92s/it]Profiling iterations:  80%|████████  | 8/10 [01:58<00:29, 14.98s/it]Profiling iterations:  90%|█████████ | 9/10 [02:13<00:15, 15.02s/it]Profiling iterations: 100%|██████████| 10/10 [02:28<00:00, 15.03s/it]Profiling iterations: 100%|██████████| 10/10 [02:28<00:00, 14.84s/it]
Avg latency: 14.839817366702482 seconds
10% percentile latency: 14.102160075027495 seconds
25% percentile latency: 14.913596801576205 seconds
50% percentile latency: 15.065363093046471 seconds
75% percentile latency: 15.105349867488258 seconds
90% percentile latency: 15.113355279993266 seconds
99% percentile latency: 15.16475691428408 seconds
Batch-8 Input len-512 Output len-32
INFO 12-13 13:33:03 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:33:03 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:33:03 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:33:03 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:33:04 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:33:08 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:33:08 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:33:08 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:33:08 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:33:08 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:33:08 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:33:08 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:33:25 [config.py:841] This model supports multiple tasks: {'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-13 13:33:25 [config.py:1472] Using max model len 32768
INFO 12-13 13:33:25 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:33:25 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:33:25 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:33:25 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:33:25 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:33:26 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:33:34 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:33:34 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:33:34 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:33:34 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:33:36 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:33:38 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:33:39 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:33:40 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:33:40 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:33:40 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:33:40 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:33:40 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:33:40 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:33:40 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:33:53 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:33:53 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:33:53 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:33:53 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:33:54 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:34:00 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:34:01 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]

INFO 12-13 13:34:02 [default_loader.py:272] Loading weights took 0.65 seconds
INFO 12-13 13:34:03 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:34:11 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:34:11 [backends.py:519] Dynamo bytecode transform time: 6.93 s
INFO 12-13 13:34:13 [backends.py:193] Compiling a graph for general shape takes 1.83 s
INFO 12-13 13:34:21 [monitor.py:34] torch.compile takes 8.77 s in total
INFO 12-13 13:34:22 [worker_v1.py:181] Available memory: 55437099724, total memory: 65464696832
INFO 12-13 13:34:22 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:34:22 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:35:05 [model_runner_v1.py:2074] Graph capturing finished in 43 secs, took 0.17 GiB
INFO 12-13 13:35:05 [core.py:172] init engine (profile, create kv cache, warmup model) took 61.49 seconds
WARNING 12-13 13:35:06 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:35:06 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:35:06 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:35:06 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:35:06 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:02,  1.76it/s]Warmup iterations:  40%|████      | 2/5 [00:01<00:01,  1.95it/s]Warmup iterations:  60%|██████    | 3/5 [00:01<00:00,  2.01it/s]Warmup iterations:  80%|████████  | 4/5 [00:02<00:00,  2.04it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  2.02it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:04,  2.11it/s]Profiling iterations:  20%|██        | 2/10 [00:00<00:03,  2.11it/s]Profiling iterations:  30%|███       | 3/10 [00:01<00:03,  2.12it/s]Profiling iterations:  40%|████      | 4/10 [00:01<00:02,  2.12it/s]Profiling iterations:  50%|█████     | 5/10 [00:02<00:02,  2.12it/s]Profiling iterations:  60%|██████    | 6/10 [00:02<00:01,  2.12it/s]Profiling iterations:  70%|███████   | 7/10 [00:03<00:01,  2.12it/s]Profiling iterations:  80%|████████  | 8/10 [00:03<00:00,  2.12it/s]Profiling iterations:  90%|█████████ | 9/10 [00:04<00:00,  2.12it/s]Profiling iterations: 100%|██████████| 10/10 [00:04<00:00,  2.12it/s]Profiling iterations: 100%|██████████| 10/10 [00:04<00:00,  2.12it/s]
Avg latency: 0.47197891063988207 seconds
10% percentile latency: 0.46950954883359375 seconds
25% percentile latency: 0.470983867184259 seconds
50% percentile latency: 0.4719435884617269 seconds
75% percentile latency: 0.47280218184459955 seconds
90% percentile latency: 0.47421547430567446 seconds
99% percentile latency: 0.4748028643755242 seconds
Batch-8 Input len-512 Output len-64
INFO 12-13 13:35:31 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:35:31 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:35:31 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:35:31 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:35:32 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:35:36 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:35:37 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:35:37 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:35:37 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:35:37 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:35:37 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:35:37 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:35:53 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-13 13:35:53 [config.py:1472] Using max model len 32768
INFO 12-13 13:35:53 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:35:53 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:35:53 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:35:53 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:35:53 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:35:54 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:36:03 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:36:03 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:36:03 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:36:03 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:36:04 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:36:07 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:36:08 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:36:08 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:36:08 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:36:08 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:36:08 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:36:08 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:36:08 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:36:08 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:36:21 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:36:21 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:36:21 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:36:21 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:36:23 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:36:29 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:36:29 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]

INFO 12-13 13:36:31 [default_loader.py:272] Loading weights took 0.64 seconds
INFO 12-13 13:36:33 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:36:41 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:36:41 [backends.py:519] Dynamo bytecode transform time: 7.20 s
INFO 12-13 13:36:44 [backends.py:193] Compiling a graph for general shape takes 1.96 s
INFO 12-13 13:36:51 [monitor.py:34] torch.compile takes 9.16 s in total
INFO 12-13 13:36:53 [worker_v1.py:181] Available memory: 55436960460, total memory: 65464696832
INFO 12-13 13:36:53 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:36:53 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:38:06 [model_runner_v1.py:2074] Graph capturing finished in 73 secs, took 0.17 GiB
INFO 12-13 13:38:06 [core.py:172] init engine (profile, create kv cache, warmup model) took 93.08 seconds
WARNING 12-13 13:38:07 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:38:07 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:38:07 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:38:07 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:38:07 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:04,  1.03s/it]Warmup iterations:  40%|████      | 2/5 [00:01<00:02,  1.03it/s]Warmup iterations:  60%|██████    | 3/5 [00:02<00:01,  1.04it/s]Warmup iterations:  80%|████████  | 4/5 [00:03<00:00,  1.05it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.04it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.04it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:08,  1.03it/s]Profiling iterations:  20%|██        | 2/10 [00:01<00:07,  1.05it/s]Profiling iterations:  30%|███       | 3/10 [00:02<00:06,  1.05it/s]Profiling iterations:  40%|████      | 4/10 [00:03<00:05,  1.06it/s]Profiling iterations:  50%|█████     | 5/10 [00:04<00:04,  1.06it/s]Profiling iterations:  60%|██████    | 6/10 [00:05<00:03,  1.06it/s]Profiling iterations:  70%|███████   | 7/10 [00:06<00:02,  1.06it/s]Profiling iterations:  80%|████████  | 8/10 [00:07<00:01,  1.06it/s]Profiling iterations:  90%|█████████ | 9/10 [00:08<00:00,  1.05it/s]Profiling iterations: 100%|██████████| 10/10 [00:09<00:00,  1.04it/s]Profiling iterations: 100%|██████████| 10/10 [00:09<00:00,  1.05it/s]
Avg latency: 0.9525962717365474 seconds
10% percentile latency: 0.9412645979318768 seconds
25% percentile latency: 0.9431483832886443 seconds
50% percentile latency: 0.9435309593100101 seconds
75% percentile latency: 0.9644816261716187 seconds
90% percentile latency: 0.977304640877992 seconds
99% percentile latency: 0.9774130115937442 seconds
Batch-8 Input len-512 Output len-128
INFO 12-13 13:38:40 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:38:40 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:38:40 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:38:40 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:38:41 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:38:45 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:38:45 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:38:45 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:38:45 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:38:45 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:38:45 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:38:45 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:39:02 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 12-13 13:39:02 [config.py:1472] Using max model len 32768
INFO 12-13 13:39:02 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:39:02 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:39:02 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:39:02 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:39:02 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:39:03 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:39:11 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:39:11 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:39:11 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:39:11 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:39:13 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:39:15 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:39:16 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:39:17 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:39:17 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:39:17 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:39:17 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:39:17 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:39:17 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:39:17 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:39:30 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:39:30 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:39:30 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:39:30 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:39:31 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:39:37 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:39:38 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]

INFO 12-13 13:39:39 [default_loader.py:272] Loading weights took 0.68 seconds
INFO 12-13 13:39:40 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:39:48 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:39:48 [backends.py:519] Dynamo bytecode transform time: 7.32 s
INFO 12-13 13:39:50 [backends.py:193] Compiling a graph for general shape takes 1.95 s
INFO 12-13 13:39:58 [monitor.py:34] torch.compile takes 9.27 s in total
INFO 12-13 13:39:59 [worker_v1.py:181] Available memory: 55437087436, total memory: 65464696832
INFO 12-13 13:39:59 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:39:59 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:40:38 [model_runner_v1.py:2074] Graph capturing finished in 39 secs, took 0.17 GiB
INFO 12-13 13:40:38 [core.py:172] init engine (profile, create kv cache, warmup model) took 58.24 seconds
WARNING 12-13 13:40:39 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:40:40 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:40:40 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:40:40 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:40:40 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:07,  1.91s/it]Warmup iterations:  40%|████      | 2/5 [00:03<00:05,  1.86s/it]Warmup iterations:  60%|██████    | 3/5 [00:05<00:03,  1.82s/it]Warmup iterations:  80%|████████  | 4/5 [00:07<00:01,  1.80s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.80s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:16,  1.79s/it]Profiling iterations:  20%|██        | 2/10 [00:03<00:14,  1.79s/it]Profiling iterations:  30%|███       | 3/10 [00:05<00:12,  1.79s/it]Profiling iterations:  40%|████      | 4/10 [00:07<00:10,  1.79s/it]Profiling iterations:  50%|█████     | 5/10 [00:08<00:08,  1.79s/it]Profiling iterations:  60%|██████    | 6/10 [00:10<00:07,  1.79s/it]Profiling iterations:  70%|███████   | 7/10 [00:12<00:05,  1.79s/it]Profiling iterations:  80%|████████  | 8/10 [00:14<00:03,  1.79s/it]Profiling iterations:  90%|█████████ | 9/10 [00:16<00:01,  1.79s/it]Profiling iterations: 100%|██████████| 10/10 [00:17<00:00,  1.79s/it]Profiling iterations: 100%|██████████| 10/10 [00:17<00:00,  1.79s/it]
Avg latency: 1.7912328644655644 seconds
10% percentile latency: 1.7855784404091537 seconds
25% percentile latency: 1.7869527046568692 seconds
50% percentile latency: 1.7908030999824405 seconds
75% percentile latency: 1.7948877535527572 seconds
90% percentile latency: 1.796001670928672 seconds
99% percentile latency: 1.7997394376946614 seconds
Batch-8 Input len-512 Output len-256
INFO 12-13 13:41:23 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:41:23 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:41:23 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:41:23 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:41:24 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:41:28 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:41:28 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:41:28 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:41:28 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:41:28 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:41:28 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:41:28 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:41:45 [config.py:841] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 12-13 13:41:45 [config.py:1472] Using max model len 32768
INFO 12-13 13:41:45 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:41:45 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:41:45 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:41:45 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:41:45 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:41:46 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:41:55 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:41:55 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:41:55 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:41:55 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:41:56 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:41:58 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:42:00 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:42:00 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:42:00 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:42:00 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:42:00 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:42:00 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:42:00 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:42:00 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:42:13 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:42:13 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:42:13 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:42:13 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:42:14 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:42:20 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:42:21 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]

INFO 12-13 13:42:22 [default_loader.py:272] Loading weights took 0.68 seconds
INFO 12-13 13:42:23 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:42:31 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:42:31 [backends.py:519] Dynamo bytecode transform time: 7.29 s
INFO 12-13 13:42:33 [backends.py:193] Compiling a graph for general shape takes 1.97 s
INFO 12-13 13:42:41 [monitor.py:34] torch.compile takes 9.26 s in total
INFO 12-13 13:42:42 [worker_v1.py:181] Available memory: 55436178124, total memory: 65464696832
INFO 12-13 13:42:42 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:42:42 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:43:42 [model_runner_v1.py:2074] Graph capturing finished in 61 secs, took 0.17 GiB
INFO 12-13 13:43:42 [core.py:172] init engine (profile, create kv cache, warmup model) took 79.38 seconds
WARNING 12-13 13:43:43 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:43:44 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:43:44 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:43:44 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:43:44 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:03<00:15,  3.83s/it]Warmup iterations:  40%|████      | 2/5 [00:07<00:11,  3.74s/it]Warmup iterations:  60%|██████    | 3/5 [00:11<00:07,  3.71s/it]Warmup iterations:  80%|████████  | 4/5 [00:14<00:03,  3.70s/it]Warmup iterations: 100%|██████████| 5/5 [00:18<00:00,  3.70s/it]Warmup iterations: 100%|██████████| 5/5 [00:18<00:00,  3.71s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:03<00:33,  3.68s/it]Profiling iterations:  20%|██        | 2/10 [00:07<00:29,  3.69s/it]Profiling iterations:  30%|███       | 3/10 [00:11<00:25,  3.69s/it]Profiling iterations:  40%|████      | 4/10 [00:14<00:22,  3.68s/it]Profiling iterations:  50%|█████     | 5/10 [00:18<00:18,  3.68s/it]Profiling iterations:  60%|██████    | 6/10 [00:22<00:14,  3.67s/it]Profiling iterations:  70%|███████   | 7/10 [00:25<00:10,  3.66s/it]Profiling iterations:  80%|████████  | 8/10 [00:29<00:07,  3.65s/it]Profiling iterations:  90%|█████████ | 9/10 [00:32<00:03,  3.64s/it]Profiling iterations: 100%|██████████| 10/10 [00:36<00:00,  3.64s/it]Profiling iterations: 100%|██████████| 10/10 [00:36<00:00,  3.66s/it]
Avg latency: 3.6602297424804418 seconds
10% percentile latency: 3.6307700763922184 seconds
25% percentile latency: 3.637161274673417 seconds
50% percentile latency: 3.6594871380366385 seconds
75% percentile latency: 3.68138404621277 seconds
90% percentile latency: 3.688232127064839 seconds
99% percentile latency: 3.699107810757123 seconds
Batch-8 Input len-512 Output len-512
INFO 12-13 13:44:57 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:44:57 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:44:57 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:44:57 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:44:58 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:45:02 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:45:02 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:45:02 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:45:02 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:45:02 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:45:02 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:45:02 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:45:19 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-13 13:45:19 [config.py:1472] Using max model len 32768
INFO 12-13 13:45:19 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:45:19 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:45:19 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:45:19 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:45:19 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:45:20 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:45:29 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:45:29 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:45:29 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:45:29 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:45:30 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:45:33 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:45:34 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:45:34 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:45:34 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:45:34 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:45:34 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:45:34 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:45:34 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:45:34 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:45:47 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:45:47 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:45:47 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:45:47 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:45:49 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:45:55 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:45:55 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]

INFO 12-13 13:45:57 [default_loader.py:272] Loading weights took 0.66 seconds
INFO 12-13 13:45:58 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:46:05 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:46:05 [backends.py:519] Dynamo bytecode transform time: 7.32 s
INFO 12-13 13:46:08 [backends.py:193] Compiling a graph for general shape takes 1.97 s
INFO 12-13 13:46:15 [monitor.py:34] torch.compile takes 9.30 s in total
INFO 12-13 13:46:16 [worker_v1.py:181] Available memory: 55437042380, total memory: 65464696832
INFO 12-13 13:46:16 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:46:16 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:46:55 [model_runner_v1.py:2074] Graph capturing finished in 39 secs, took 0.17 GiB
INFO 12-13 13:46:55 [core.py:172] init engine (profile, create kv cache, warmup model) took 57.86 seconds
WARNING 12-13 13:46:56 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:46:57 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:46:57 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:46:57 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:46:57 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:07<00:28,  7.08s/it]Warmup iterations:  40%|████      | 2/5 [00:14<00:21,  7.02s/it]Warmup iterations:  60%|██████    | 3/5 [00:21<00:13,  6.99s/it]Warmup iterations:  80%|████████  | 4/5 [00:27<00:06,  6.96s/it]Warmup iterations: 100%|██████████| 5/5 [00:34<00:00,  6.93s/it]Warmup iterations: 100%|██████████| 5/5 [00:34<00:00,  6.96s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:06<01:02,  6.94s/it]Profiling iterations:  20%|██        | 2/10 [00:13<00:55,  6.90s/it]Profiling iterations:  30%|███       | 3/10 [00:20<00:48,  6.88s/it]Profiling iterations:  40%|████      | 4/10 [00:27<00:41,  6.88s/it]Profiling iterations:  50%|█████     | 5/10 [00:34<00:34,  6.88s/it]Profiling iterations:  60%|██████    | 6/10 [00:41<00:27,  6.89s/it]Profiling iterations:  70%|███████   | 7/10 [00:48<00:20,  6.90s/it]Profiling iterations:  80%|████████  | 8/10 [00:55<00:13,  6.90s/it]Profiling iterations:  90%|█████████ | 9/10 [01:02<00:06,  6.99s/it]Profiling iterations: 100%|██████████| 10/10 [01:09<00:00,  7.00s/it]Profiling iterations: 100%|██████████| 10/10 [01:09<00:00,  6.94s/it]
Avg latency: 6.9364363856147975 seconds
10% percentile latency: 6.864486461924389 seconds
25% percentile latency: 6.874258384457789 seconds
50% percentile latency: 6.911039191531017 seconds
75% percentile latency: 6.933868337073363 seconds
90% percentile latency: 7.0286643745843325 seconds
99% percentile latency: 7.17829925886821 seconds
Batch-8 Input len-512 Output len-1024
INFO 12-13 13:48:59 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:48:59 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:48:59 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:48:59 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:49:00 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:49:04 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:49:04 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:49:04 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:49:04 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:49:04 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:49:04 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:49:04 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:49:21 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 12-13 13:49:21 [config.py:1472] Using max model len 32768
INFO 12-13 13:49:21 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:49:21 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:49:21 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:49:21 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:49:21 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:49:22 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:49:31 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:49:31 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:49:31 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:49:31 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:49:32 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:49:35 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:49:36 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:49:36 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:49:36 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:49:36 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:49:36 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:49:36 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:49:36 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:49:36 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:49:49 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:49:49 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:49:49 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:49:49 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:49:50 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:49:57 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:49:57 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]

INFO 12-13 13:49:59 [default_loader.py:272] Loading weights took 0.67 seconds
INFO 12-13 13:50:01 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:50:09 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:50:09 [backends.py:519] Dynamo bytecode transform time: 7.17 s
INFO 12-13 13:50:12 [backends.py:193] Compiling a graph for general shape takes 1.95 s
INFO 12-13 13:50:20 [monitor.py:34] torch.compile takes 9.12 s in total
INFO 12-13 13:50:22 [worker_v1.py:181] Available memory: 55437165260, total memory: 65464696832
INFO 12-13 13:50:22 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:50:22 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:51:37 [model_runner_v1.py:2074] Graph capturing finished in 75 secs, took 0.17 GiB
INFO 12-13 13:51:37 [core.py:172] init engine (profile, create kv cache, warmup model) took 95.79 seconds
WARNING 12-13 13:51:38 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:51:38 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:51:38 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:51:38 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:51:38 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:14<00:56, 14.07s/it]Warmup iterations:  40%|████      | 2/5 [00:27<00:41, 13.86s/it]Warmup iterations:  60%|██████    | 3/5 [00:41<00:27, 13.80s/it]Warmup iterations:  80%|████████  | 4/5 [00:55<00:13, 13.77s/it]Warmup iterations: 100%|██████████| 5/5 [01:08<00:00, 13.75s/it]Warmup iterations: 100%|██████████| 5/5 [01:08<00:00, 13.79s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:13<02:05, 13.99s/it]Profiling iterations:  20%|██        | 2/10 [00:28<01:53, 14.17s/it]Profiling iterations:  30%|███       | 3/10 [00:42<01:39, 14.19s/it]Profiling iterations:  40%|████      | 4/10 [00:56<01:24, 14.13s/it]Profiling iterations:  50%|█████     | 5/10 [01:10<01:10, 14.17s/it]Profiling iterations:  60%|██████    | 6/10 [01:25<00:56, 14.23s/it]Profiling iterations:  70%|███████   | 7/10 [01:39<00:42, 14.27s/it]Profiling iterations:  80%|████████  | 8/10 [01:53<00:28, 14.34s/it]Profiling iterations:  90%|█████████ | 9/10 [02:08<00:14, 14.39s/it]Profiling iterations: 100%|██████████| 10/10 [02:22<00:00, 14.40s/it]Profiling iterations: 100%|██████████| 10/10 [02:22<00:00, 14.29s/it]
Avg latency: 14.289059895882383 seconds
10% percentile latency: 14.034336438216268 seconds
25% percentile latency: 14.220960405422375 seconds
50% percentile latency: 14.317167829722166 seconds
75% percentile latency: 14.401354904286563 seconds
90% percentile latency: 14.474975692061708 seconds
99% percentile latency: 14.515539778391831 seconds
Batch-8 Input len-1024 Output len-32
INFO 12-13 13:55:28 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:55:28 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:55:28 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:55:28 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:55:29 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:55:33 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:55:33 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:55:33 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:55:33 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:55:33 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:55:33 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:55:33 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:55:50 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 12-13 13:55:50 [config.py:1472] Using max model len 32768
INFO 12-13 13:55:50 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:55:50 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:55:50 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:55:50 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:55:50 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:55:51 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:56:00 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:56:00 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:56:00 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:56:00 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:56:01 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:56:04 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:56:05 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:56:05 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:56:05 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:56:05 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:56:05 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:56:05 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:56:05 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:56:05 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:56:18 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:56:18 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:56:18 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:56:18 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:56:19 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:56:26 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:56:26 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.36it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.36it/s]

INFO 12-13 13:56:27 [default_loader.py:272] Loading weights took 0.65 seconds
INFO 12-13 13:56:29 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:56:36 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:56:36 [backends.py:519] Dynamo bytecode transform time: 6.90 s
INFO 12-13 13:56:38 [backends.py:193] Compiling a graph for general shape takes 1.82 s
INFO 12-13 13:56:46 [monitor.py:34] torch.compile takes 8.72 s in total
INFO 12-13 13:56:47 [worker_v1.py:181] Available memory: 55436108492, total memory: 65464696832
INFO 12-13 13:56:47 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:56:47 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 13:57:30 [model_runner_v1.py:2074] Graph capturing finished in 43 secs, took 0.17 GiB
INFO 12-13 13:57:30 [core.py:172] init engine (profile, create kv cache, warmup model) took 61.92 seconds
WARNING 12-13 13:57:32 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 13:57:32 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:57:32 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:57:32 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:57:32 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:02,  1.62it/s]Warmup iterations:  40%|████      | 2/5 [00:01<00:01,  1.76it/s]Warmup iterations:  60%|██████    | 3/5 [00:01<00:01,  1.81it/s]Warmup iterations:  80%|████████  | 4/5 [00:02<00:00,  1.84it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  1.85it/s]Warmup iterations: 100%|██████████| 5/5 [00:02<00:00,  1.82it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:04,  1.88it/s]Profiling iterations:  20%|██        | 2/10 [00:01<00:04,  1.88it/s]Profiling iterations:  30%|███       | 3/10 [00:01<00:03,  1.87it/s]Profiling iterations:  40%|████      | 4/10 [00:02<00:03,  1.86it/s]Profiling iterations:  50%|█████     | 5/10 [00:02<00:02,  1.84it/s]Profiling iterations:  60%|██████    | 6/10 [00:03<00:02,  1.80it/s]Profiling iterations:  70%|███████   | 7/10 [00:03<00:01,  1.77it/s]Profiling iterations:  80%|████████  | 8/10 [00:04<00:01,  1.75it/s]Profiling iterations:  90%|█████████ | 9/10 [00:05<00:00,  1.74it/s]Profiling iterations: 100%|██████████| 10/10 [00:05<00:00,  1.73it/s]Profiling iterations: 100%|██████████| 10/10 [00:05<00:00,  1.78it/s]
Avg latency: 0.5608706122729927 seconds
10% percentile latency: 0.5324031742755324 seconds
25% percentile latency: 0.5378582422854379 seconds
50% percentile latency: 0.5678603507112712 seconds
75% percentile latency: 0.5828702204162255 seconds
90% percentile latency: 0.5833504320587963 seconds
99% percentile latency: 0.584091884358786 seconds
Batch-8 Input len-1024 Output len-64
INFO 12-13 13:57:58 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:57:58 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:57:58 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:57:58 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:57:59 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:58:03 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:58:04 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:58:04 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:58:04 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:58:04 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:58:04 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:58:04 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:58:20 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 12-13 13:58:20 [config.py:1472] Using max model len 32768
INFO 12-13 13:58:20 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 13:58:20 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 13:58:20 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 13:58:20 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 13:58:20 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 13:58:21 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 13:58:30 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:58:30 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:58:30 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:58:30 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:58:31 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:58:34 [core.py:526] Waiting for init message from front-end.
INFO 12-13 13:58:35 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 13:58:35 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 13:58:35 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 13:58:35 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 13:58:35 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 13:58:35 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 13:58:35 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 13:58:35 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 13:58:48 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 13:58:48 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 13:58:48 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 13:58:48 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 13:58:49 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 13:58:56 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 13:58:56 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]

INFO 12-13 13:58:58 [default_loader.py:272] Loading weights took 0.66 seconds
INFO 12-13 13:59:00 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 13:59:08 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 13:59:08 [backends.py:519] Dynamo bytecode transform time: 7.26 s
INFO 12-13 13:59:11 [backends.py:193] Compiling a graph for general shape takes 1.96 s
INFO 12-13 13:59:19 [monitor.py:34] torch.compile takes 9.22 s in total
INFO 12-13 13:59:20 [worker_v1.py:181] Available memory: 55436796620, total memory: 65464696832
INFO 12-13 13:59:20 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 13:59:20 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 14:00:38 [model_runner_v1.py:2074] Graph capturing finished in 78 secs, took 0.17 GiB
INFO 12-13 14:00:38 [core.py:172] init engine (profile, create kv cache, warmup model) took 98.17 seconds
WARNING 12-13 14:00:39 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 14:00:40 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 14:00:40 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 14:00:40 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 14:00:40 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:04,  1.13s/it]Warmup iterations:  40%|████      | 2/5 [00:02<00:03,  1.08s/it]Warmup iterations:  60%|██████    | 3/5 [00:03<00:02,  1.06s/it]Warmup iterations:  80%|████████  | 4/5 [00:04<00:01,  1.05s/it]Warmup iterations: 100%|██████████| 5/5 [00:05<00:00,  1.05s/it]Warmup iterations: 100%|██████████| 5/5 [00:05<00:00,  1.06s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:09,  1.04s/it]Profiling iterations:  20%|██        | 2/10 [00:02<00:08,  1.04s/it]Profiling iterations:  30%|███       | 3/10 [00:03<00:07,  1.04s/it]Profiling iterations:  40%|████      | 4/10 [00:04<00:06,  1.04s/it]Profiling iterations:  50%|█████     | 5/10 [00:05<00:05,  1.04s/it]Profiling iterations:  60%|██████    | 6/10 [00:06<00:04,  1.04s/it]Profiling iterations:  70%|███████   | 7/10 [00:07<00:03,  1.04s/it]Profiling iterations:  80%|████████  | 8/10 [00:08<00:02,  1.04s/it]Profiling iterations:  90%|█████████ | 9/10 [00:09<00:01,  1.04s/it]Profiling iterations: 100%|██████████| 10/10 [00:10<00:00,  1.04s/it]Profiling iterations: 100%|██████████| 10/10 [00:10<00:00,  1.04s/it]
Avg latency: 1.0402475625276566 seconds
10% percentile latency: 1.0338556339032947 seconds
25% percentile latency: 1.0383122226921842 seconds
50% percentile latency: 1.040777719579637 seconds
75% percentile latency: 1.04158356518019 seconds
90% percentile latency: 1.0449161909520626 seconds
99% percentile latency: 1.0485993242636322 seconds
Batch-8 Input len-1024 Output len-128
INFO 12-13 14:01:13 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:01:13 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:01:13 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:01:13 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:01:15 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:01:19 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 14:01:19 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 14:01:19 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 14:01:19 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 14:01:19 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 14:01:19 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 14:01:19 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 14:01:35 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 12-13 14:01:35 [config.py:1472] Using max model len 32768
INFO 12-13 14:01:35 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 14:01:35 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 14:01:35 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 14:01:35 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 14:01:35 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 14:01:36 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 14:01:45 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:01:45 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:01:45 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:01:45 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:01:46 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:01:49 [core.py:526] Waiting for init message from front-end.
INFO 12-13 14:01:50 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 14:01:50 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 14:01:50 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 14:01:50 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 14:01:50 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 14:01:50 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 14:01:50 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 14:01:50 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 14:02:03 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:02:03 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:02:03 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:02:03 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:02:05 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:02:11 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 14:02:11 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]

INFO 12-13 14:02:13 [default_loader.py:272] Loading weights took 0.70 seconds
INFO 12-13 14:02:15 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 14:02:24 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 14:02:24 [backends.py:519] Dynamo bytecode transform time: 7.10 s
INFO 12-13 14:02:26 [backends.py:193] Compiling a graph for general shape takes 1.91 s
INFO 12-13 14:02:34 [monitor.py:34] torch.compile takes 9.01 s in total
INFO 12-13 14:02:35 [worker_v1.py:181] Available memory: 55437710028, total memory: 65464696832
INFO 12-13 14:02:35 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 14:02:35 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 14:03:53 [model_runner_v1.py:2074] Graph capturing finished in 77 secs, took 0.17 GiB
INFO 12-13 14:03:53 [core.py:172] init engine (profile, create kv cache, warmup model) took 97.34 seconds
WARNING 12-13 14:03:54 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 14:03:54 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 14:03:54 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 14:03:54 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 14:03:54 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:07,  1.94s/it]Warmup iterations:  40%|████      | 2/5 [00:03<00:05,  1.88s/it]Warmup iterations:  60%|██████    | 3/5 [00:05<00:03,  1.87s/it]Warmup iterations:  80%|████████  | 4/5 [00:07<00:01,  1.86s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.86s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.87s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:16,  1.86s/it]Profiling iterations:  20%|██        | 2/10 [00:03<00:14,  1.85s/it]Profiling iterations:  30%|███       | 3/10 [00:05<00:12,  1.85s/it]Profiling iterations:  40%|████      | 4/10 [00:07<00:11,  1.85s/it]Profiling iterations:  50%|█████     | 5/10 [00:09<00:09,  1.85s/it]Profiling iterations:  60%|██████    | 6/10 [00:11<00:07,  1.85s/it]Profiling iterations:  70%|███████   | 7/10 [00:12<00:05,  1.85s/it]Profiling iterations:  80%|████████  | 8/10 [00:14<00:03,  1.84s/it]Profiling iterations:  90%|█████████ | 9/10 [00:16<00:01,  1.83s/it]Profiling iterations: 100%|██████████| 10/10 [00:18<00:00,  1.83s/it]Profiling iterations: 100%|██████████| 10/10 [00:18<00:00,  1.84s/it]
Avg latency: 1.8403394605498762 seconds
10% percentile latency: 1.8153339320793749 seconds
25% percentile latency: 1.8234974627848715 seconds
50% percentile latency: 1.8469929920975119 seconds
75% percentile latency: 1.852100151241757 seconds
90% percentile latency: 1.856131450133398 seconds
99% percentile latency: 1.8605669786548242 seconds
Batch-8 Input len-1024 Output len-256
INFO 12-13 14:04:40 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:04:40 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:04:40 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:04:40 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:04:41 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:04:45 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 14:04:45 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 14:04:45 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 14:04:45 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 14:04:45 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 14:04:45 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 14:04:45 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 14:05:02 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 12-13 14:05:02 [config.py:1472] Using max model len 32768
INFO 12-13 14:05:02 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 14:05:02 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 14:05:02 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 14:05:02 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 14:05:02 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 14:05:03 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 14:05:11 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:05:11 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:05:11 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:05:11 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:05:13 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:05:15 [core.py:526] Waiting for init message from front-end.
INFO 12-13 14:05:17 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 14:05:17 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 14:05:17 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 14:05:17 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 14:05:17 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 14:05:17 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 14:05:17 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 14:05:17 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 14:05:30 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:05:30 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:05:30 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:05:30 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:05:31 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:05:37 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 14:05:38 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]

INFO 12-13 14:05:39 [default_loader.py:272] Loading weights took 0.68 seconds
INFO 12-13 14:05:40 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 14:05:48 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 14:05:48 [backends.py:519] Dynamo bytecode transform time: 7.27 s
INFO 12-13 14:05:50 [backends.py:193] Compiling a graph for general shape takes 1.94 s
INFO 12-13 14:05:58 [monitor.py:34] torch.compile takes 9.21 s in total
INFO 12-13 14:05:59 [worker_v1.py:181] Available memory: 55436665548, total memory: 65464696832
INFO 12-13 14:05:59 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 14:05:59 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 14:06:39 [model_runner_v1.py:2074] Graph capturing finished in 40 secs, took 0.17 GiB
INFO 12-13 14:06:39 [core.py:172] init engine (profile, create kv cache, warmup model) took 59.43 seconds
WARNING 12-13 14:06:40 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 14:06:41 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 14:06:41 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 14:06:41 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 14:06:41 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:03<00:14,  3.61s/it]Warmup iterations:  40%|████      | 2/5 [00:07<00:10,  3.56s/it]Warmup iterations:  60%|██████    | 3/5 [00:10<00:07,  3.55s/it]Warmup iterations:  80%|████████  | 4/5 [00:14<00:03,  3.55s/it]Warmup iterations: 100%|██████████| 5/5 [00:17<00:00,  3.54s/it]Warmup iterations: 100%|██████████| 5/5 [00:17<00:00,  3.55s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:03<00:31,  3.53s/it]Profiling iterations:  20%|██        | 2/10 [00:07<00:28,  3.51s/it]Profiling iterations:  30%|███       | 3/10 [00:10<00:24,  3.52s/it]Profiling iterations:  40%|████      | 4/10 [00:14<00:21,  3.52s/it]Profiling iterations:  50%|█████     | 5/10 [00:17<00:17,  3.51s/it]Profiling iterations:  60%|██████    | 6/10 [00:21<00:14,  3.63s/it]Profiling iterations:  70%|███████   | 7/10 [00:25<00:11,  3.83s/it]Profiling iterations:  80%|████████  | 8/10 [00:29<00:07,  3.94s/it]Profiling iterations:  90%|█████████ | 9/10 [00:33<00:03,  3.99s/it]Profiling iterations: 100%|██████████| 10/10 [00:38<00:00,  4.02s/it]Profiling iterations: 100%|██████████| 10/10 [00:38<00:00,  3.80s/it]
Avg latency: 3.802156866993755 seconds
10% percentile latency: 3.501211383799091 seconds
25% percentile latency: 3.516554788337089 seconds
50% percentile latency: 3.6982451966032386 seconds
75% percentile latency: 4.096803686232306 seconds
90% percentile latency: 4.173814003402367 seconds
99% percentile latency: 4.230058169136755 seconds
Batch-8 Input len-1024 Output len-512
INFO 12-13 14:07:55 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:07:55 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:07:55 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:07:55 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:07:56 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:08:00 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 14:08:00 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 14:08:00 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 14:08:00 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 14:08:00 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 14:08:00 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 14:08:00 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 14:08:17 [config.py:841] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 12-13 14:08:17 [config.py:1472] Using max model len 32768
INFO 12-13 14:08:17 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 14:08:17 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 14:08:17 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 14:08:17 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 14:08:17 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 14:08:18 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 14:08:26 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:08:26 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:08:26 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:08:26 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:08:28 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:08:30 [core.py:526] Waiting for init message from front-end.
INFO 12-13 14:08:31 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 14:08:32 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 14:08:32 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 14:08:32 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 14:08:32 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 14:08:32 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 14:08:32 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 14:08:32 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 14:08:45 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:08:45 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:08:45 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:08:45 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:08:46 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:08:52 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 14:08:53 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.31it/s]

INFO 12-13 14:08:55 [default_loader.py:272] Loading weights took 0.67 seconds
INFO 12-13 14:08:56 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 14:09:05 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 14:09:05 [backends.py:519] Dynamo bytecode transform time: 7.25 s
INFO 12-13 14:09:07 [backends.py:193] Compiling a graph for general shape takes 1.97 s
INFO 12-13 14:09:15 [monitor.py:34] torch.compile takes 9.22 s in total
INFO 12-13 14:09:17 [worker_v1.py:181] Available memory: 55436550860, total memory: 65464696832
INFO 12-13 14:09:17 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 14:09:17 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 14:10:30 [model_runner_v1.py:2074] Graph capturing finished in 73 secs, took 0.17 GiB
INFO 12-13 14:10:30 [core.py:172] init engine (profile, create kv cache, warmup model) took 93.15 seconds
WARNING 12-13 14:10:31 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 14:10:31 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 14:10:31 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 14:10:31 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 14:10:31 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:07<00:30,  7.63s/it]Warmup iterations:  40%|████      | 2/5 [00:15<00:22,  7.49s/it]Warmup iterations:  60%|██████    | 3/5 [00:22<00:14,  7.46s/it]Warmup iterations:  80%|████████  | 4/5 [00:29<00:07,  7.47s/it]Warmup iterations: 100%|██████████| 5/5 [00:37<00:00,  7.45s/it]Warmup iterations: 100%|██████████| 5/5 [00:37<00:00,  7.47s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:07<01:08,  7.56s/it]Profiling iterations:  20%|██        | 2/10 [00:15<01:00,  7.54s/it]Profiling iterations:  30%|███       | 3/10 [00:22<00:52,  7.55s/it]Profiling iterations:  40%|████      | 4/10 [00:30<00:45,  7.54s/it]Profiling iterations:  50%|█████     | 5/10 [00:37<00:36,  7.39s/it]Profiling iterations:  60%|██████    | 6/10 [00:44<00:28,  7.23s/it]Profiling iterations:  70%|███████   | 7/10 [00:51<00:21,  7.14s/it]Profiling iterations:  80%|████████  | 8/10 [00:58<00:14,  7.07s/it]Profiling iterations:  90%|█████████ | 9/10 [01:05<00:07,  7.02s/it]Profiling iterations: 100%|██████████| 10/10 [01:11<00:00,  6.99s/it]Profiling iterations: 100%|██████████| 10/10 [01:11<00:00,  7.19s/it]
Avg latency: 7.192990745510906 seconds
10% percentile latency: 6.9148687296547 seconds
25% percentile latency: 6.924795493017882 seconds
50% percentile latency: 7.034733792999759 seconds
75% percentile latency: 7.5309086967026815 seconds
90% percentile latency: 7.558771047089249 seconds
99% percentile latency: 7.564218564312905 seconds
Batch-8 Input len-1024 Output len-1024
INFO 12-13 14:12:38 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:12:38 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:12:38 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:12:38 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:12:40 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:12:44 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 14:12:44 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 14:12:44 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 14:12:44 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 14:12:44 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 14:12:44 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 14:12:44 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 14:13:00 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 12-13 14:13:00 [config.py:1472] Using max model len 32768
INFO 12-13 14:13:00 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 14:13:00 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 14:13:00 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 14:13:00 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 14:13:00 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 14:13:01 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 14:13:10 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:13:10 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:13:10 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:13:10 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:13:11 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:13:14 [core.py:526] Waiting for init message from front-end.
INFO 12-13 14:13:15 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 14:13:16 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 14:13:16 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 14:13:16 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 14:13:16 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 14:13:16 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 14:13:16 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 14:13:16 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 14:13:29 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 14:13:29 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 14:13:29 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 14:13:29 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 14:13:30 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 14:13:36 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 14:13:36 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]

INFO 12-13 14:13:38 [default_loader.py:272] Loading weights took 0.69 seconds
INFO 12-13 14:13:41 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 14:13:49 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 14:13:49 [backends.py:519] Dynamo bytecode transform time: 7.25 s
INFO 12-13 14:13:51 [backends.py:193] Compiling a graph for general shape takes 1.97 s
INFO 12-13 14:13:59 [monitor.py:34] torch.compile takes 9.22 s in total
INFO 12-13 14:14:01 [worker_v1.py:181] Available memory: 55436243660, total memory: 65464696832
INFO 12-13 14:14:01 [kv_cache_utils.py:716] GPU KV cache size: 694,016 tokens
INFO 12-13 14:14:01 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 14:15:12 [model_runner_v1.py:2074] Graph capturing finished in 71 secs, took 0.17 GiB
INFO 12-13 14:15:12 [core.py:172] init engine (profile, create kv cache, warmup model) took 91.64 seconds
WARNING 12-13 14:15:13 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 14:15:14 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 14:15:14 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 14:15:14 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 14:15:14 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:13<00:55, 13.90s/it]Warmup iterations:  40%|████      | 2/5 [00:28<00:42, 14.03s/it]Warmup iterations:  60%|██████    | 3/5 [00:41<00:27, 13.81s/it]Warmup iterations:  80%|████████  | 4/5 [00:55<00:13, 13.81s/it]Warmup iterations: 100%|██████████| 5/5 [01:09<00:00, 13.83s/it]Warmup iterations: 100%|██████████| 5/5 [01:09<00:00, 13.85s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:14<02:09, 14.43s/it]Profiling iterations:  20%|██        | 2/10 [00:29<01:59, 14.97s/it]Profiling iterations:  30%|███       | 3/10 [00:45<01:45, 15.13s/it]Profiling iterations:  40%|████      | 4/10 [01:00<01:31, 15.22s/it]Profiling iterations:  50%|█████     | 5/10 [01:14<01:14, 14.96s/it]Profiling iterations:  60%|██████    | 6/10 [01:29<00:59, 14.76s/it]Profiling iterations:  70%|███████   | 7/10 [01:43<00:43, 14.61s/it]Profiling iterations:  80%|████████  | 8/10 [01:57<00:29, 14.53s/it]Profiling iterations:  90%|█████████ | 9/10 [02:12<00:14, 14.41s/it]Profiling iterations: 100%|██████████| 10/10 [02:26<00:00, 14.33s/it]Profiling iterations: 100%|██████████| 10/10 [02:26<00:00, 14.63s/it]
Avg latency: 14.626738449651748 seconds
10% percentile latency: 14.155956558510661 seconds
25% percentile latency: 14.319633895531297 seconds
50% percentile latency: 14.393705589696765 seconds
75% percentile latency: 15.117397734895349 seconds
90% percentile latency: 15.343684509210288 seconds
99% percentile latency: 15.365504395384342 seconds
