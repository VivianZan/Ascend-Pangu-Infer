Batch-1 Input len-2048 Output len-1
INFO 12-13 11:22:52 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:22:52 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:22:52 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:22:52 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:22:54 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:22:58 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:22:58 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:22:58 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:22:58 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:22:58 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:22:58 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:22:58 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:23:15 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 12-13 11:23:15 [config.py:1472] Using max model len 32768
INFO 12-13 11:23:15 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 11:23:15 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:23:15 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:23:15 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:23:15 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 11:23:16 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 11:23:24 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:23:24 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:23:24 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:23:24 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:23:26 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:23:28 [core.py:526] Waiting for init message from front-end.
INFO 12-13 11:23:29 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:23:30 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:23:30 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:23:30 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:23:30 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:23:30 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:23:30 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:23:30 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 11:23:43 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:23:43 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:23:43 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:23:43 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:23:44 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:23:50 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 11:23:50 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
INFO 12-13 11:23:52 [default_loader.py:272] Loading weights took 0.67 seconds
INFO 12-13 11:23:54 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 11:24:02 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 11:24:02 [backends.py:519] Dynamo bytecode transform time: 7.21 s
INFO 12-13 11:24:05 [backends.py:193] Compiling a graph for general shape takes 1.95 s
INFO 12-13 11:24:13 [monitor.py:34] torch.compile takes 9.16 s in total
INFO 12-13 11:24:14 [worker_v1.py:181] Available memory: 55445971660, total memory: 65464696832
INFO 12-13 11:24:14 [kv_cache_utils.py:716] GPU KV cache size: 694,144 tokens
INFO 12-13 11:24:14 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 11:25:05 [model_runner_v1.py:2074] Graph capturing finished in 51 secs, took 0.17 GiB
INFO 12-13 11:25:05 [core.py:172] init engine (profile, create kv cache, warmup model) took 71.33 seconds
WARNING 12-13 11:25:07 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 11:25:07 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:25:07 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:25:07 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:25:07 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Avg latency: 0.03407290228642523 seconds
10% percentile latency: 0.0338728507515043 seconds
25% percentile latency: 0.03397417638916522 seconds
50% percentile latency: 0.03399778949096799 seconds
75% percentile latency: 0.03416013263631612 seconds
90% percentile latency: 0.034372515697032216 seconds
99% percentile latency: 0.03442651133053005 seconds
Batch-1 Input len-4096 Output len-1
INFO 12-13 11:25:26 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:25:26 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:25:26 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:25:26 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:25:27 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:25:31 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:25:31 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:25:31 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:25:31 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:25:31 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:25:31 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:25:31 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:25:48 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 12-13 11:25:48 [config.py:1472] Using max model len 32768
INFO 12-13 11:25:48 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 11:25:48 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:25:48 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:25:48 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:25:48 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 11:25:49 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 11:25:57 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:25:57 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:25:57 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:25:57 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:25:59 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:26:01 [core.py:526] Waiting for init message from front-end.
INFO 12-13 11:26:02 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:26:03 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:26:03 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:26:03 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:26:03 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:26:03 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:26:03 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:26:03 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 11:26:16 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:26:16 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:26:16 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:26:16 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:26:17 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:26:23 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 11:26:24 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
INFO 12-13 11:26:25 [default_loader.py:272] Loading weights took 0.64 seconds
INFO 12-13 11:26:26 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 11:26:33 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 11:26:33 [backends.py:519] Dynamo bytecode transform time: 6.86 s
INFO 12-13 11:26:36 [backends.py:193] Compiling a graph for general shape takes 1.82 s
Batch-1 Input len-8192 Output len-1
INFO 12-13 11:26:56 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:26:56 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:26:56 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:26:56 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:26:57 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:27:01 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:27:01 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:27:01 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:27:01 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:27:01 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:27:01 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:27:01 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:27:17 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 12-13 11:27:17 [config.py:1472] Using max model len 32768
INFO 12-13 11:27:17 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 11:27:17 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:27:17 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:27:17 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:27:17 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 11:27:19 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
Batch-1 Input len-2048 Output len-1
INFO 12-13 11:45:34 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:45:34 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:45:34 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:45:34 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:45:36 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:45:39 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:45:40 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:45:40 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:45:40 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:45:40 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:45:40 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:45:40 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:45:56 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-13 11:45:56 [config.py:1472] Using max model len 32768
INFO 12-13 11:45:56 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 11:45:56 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:45:56 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:45:56 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:45:56 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 11:45:57 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 11:46:06 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:46:06 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:46:06 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:46:06 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:46:07 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:46:10 [core.py:526] Waiting for init message from front-end.
INFO 12-13 11:46:11 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:46:11 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:46:11 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:46:11 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:46:11 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:46:11 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:46:11 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:46:11 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 11:46:25 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:46:25 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:46:25 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:46:25 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:46:26 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:46:32 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 11:46:32 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
INFO 12-13 11:46:34 [default_loader.py:272] Loading weights took 0.68 seconds
INFO 12-13 11:46:36 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 11:46:44 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 11:46:44 [backends.py:519] Dynamo bytecode transform time: 7.22 s
INFO 12-13 11:46:47 [backends.py:193] Compiling a graph for general shape takes 1.95 s
INFO 12-13 11:46:55 [monitor.py:34] torch.compile takes 9.18 s in total
INFO 12-13 11:46:57 [worker_v1.py:181] Available memory: 55445684940, total memory: 65464696832
INFO 12-13 11:46:57 [kv_cache_utils.py:716] GPU KV cache size: 694,144 tokens
INFO 12-13 11:46:57 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 11:48:10 [model_runner_v1.py:2074] Graph capturing finished in 73 secs, took 0.17 GiB
INFO 12-13 11:48:10 [core.py:172] init engine (profile, create kv cache, warmup model) took 93.79 seconds
WARNING 12-13 11:48:11 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 11:48:12 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:48:12 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:48:12 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:48:12 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Avg latency: 0.03627114775590599 seconds
10% percentile latency: 0.036147718271240595 seconds
25% percentile latency: 0.03616415162105113 seconds
50% percentile latency: 0.03625009930692613 seconds
75% percentile latency: 0.03632607730105519 seconds
90% percentile latency: 0.036446709837764504 seconds
99% percentile latency: 0.03648363782092929 seconds
Batch-1 Input len-4096 Output len-1
INFO 12-13 11:48:31 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:48:31 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:48:31 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:48:31 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:48:32 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:48:36 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:48:36 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:48:36 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:48:36 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:48:36 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:48:36 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:48:36 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:48:52 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-13 11:48:52 [config.py:1472] Using max model len 32768
INFO 12-13 11:48:52 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 11:48:52 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:48:52 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:48:52 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:48:52 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 11:48:54 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 11:49:02 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:49:02 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:49:02 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:49:02 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:49:03 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:49:06 [core.py:526] Waiting for init message from front-end.
INFO 12-13 11:49:07 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:49:08 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:49:08 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:49:08 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:49:08 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:49:08 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:49:08 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:49:08 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 11:49:21 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:49:21 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:49:21 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:49:21 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:49:22 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:49:28 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 11:49:28 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
INFO 12-13 11:49:30 [default_loader.py:272] Loading weights took 0.53 seconds
INFO 12-13 11:49:32 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 11:49:40 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 11:49:40 [backends.py:519] Dynamo bytecode transform time: 7.29 s
INFO 12-13 11:49:43 [backends.py:193] Compiling a graph for general shape takes 2.00 s
INFO 12-13 11:49:51 [monitor.py:34] torch.compile takes 9.29 s in total
INFO 12-13 11:49:52 [worker_v1.py:181] Available memory: 55446311628, total memory: 65464696832
INFO 12-13 11:49:52 [kv_cache_utils.py:716] GPU KV cache size: 694,144 tokens
INFO 12-13 11:49:52 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 11:51:06 [model_runner_v1.py:2074] Graph capturing finished in 73 secs, took 0.17 GiB
INFO 12-13 11:51:06 [core.py:172] init engine (profile, create kv cache, warmup model) took 93.76 seconds
WARNING 12-13 11:51:07 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 11:51:07 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:51:07 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:51:07 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:51:07 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Avg latency: 0.06649584746919572 seconds
10% percentile latency: 0.0662622309755534 seconds
25% percentile latency: 0.06627961818594486 seconds
50% percentile latency: 0.0663577790837735 seconds
75% percentile latency: 0.06671735574491322 seconds
90% percentile latency: 0.06694606812670827 seconds
99% percentile latency: 0.06695807243697345 seconds
Batch-1 Input len-8192 Output len-1
INFO 12-13 11:51:25 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:51:25 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:51:25 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:51:25 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:51:26 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:51:30 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:51:30 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:51:30 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:51:30 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:51:30 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:51:30 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:51:30 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:51:47 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 12-13 11:51:47 [config.py:1472] Using max model len 32768
INFO 12-13 11:51:47 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 11:51:47 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:51:47 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:51:47 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:51:47 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 11:51:48 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 11:51:57 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:51:57 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:51:57 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:51:57 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:51:58 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:52:01 [core.py:526] Waiting for init message from front-end.
INFO 12-13 11:52:02 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:52:02 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:52:02 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:52:02 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:52:02 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:52:02 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:52:02 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:52:02 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 11:52:15 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:52:15 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:52:15 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:52:15 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:52:16 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:52:23 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 11:52:23 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
INFO 12-13 11:52:25 [default_loader.py:272] Loading weights took 0.68 seconds
INFO 12-13 11:52:27 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 11:52:35 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 11:52:35 [backends.py:519] Dynamo bytecode transform time: 7.09 s
INFO 12-13 11:52:37 [backends.py:193] Compiling a graph for general shape takes 1.83 s
INFO 12-13 11:52:44 [monitor.py:34] torch.compile takes 8.91 s in total
INFO 12-13 11:52:46 [worker_v1.py:181] Available memory: 55445893836, total memory: 65464696832
INFO 12-13 11:52:46 [kv_cache_utils.py:716] GPU KV cache size: 694,144 tokens
INFO 12-13 11:52:46 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 11:53:26 [model_runner_v1.py:2074] Graph capturing finished in 40 secs, took 0.17 GiB
INFO 12-13 11:53:26 [core.py:172] init engine (profile, create kv cache, warmup model) took 59.29 seconds
WARNING 12-13 11:53:27 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 11:53:27 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:53:27 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:53:27 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:53:27 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Avg latency: 0.17337699150666594 seconds
10% percentile latency: 0.17287786309607328 seconds
25% percentile latency: 0.1729882959043607 seconds
50% percentile latency: 0.1733045899309218 seconds
75% percentile latency: 0.17331870272755623 seconds
90% percentile latency: 0.17368437498807907 seconds
99% percentile latency: 0.17519541914574802 seconds
Batch-1 Input len-16384 Output len-1
INFO 12-13 11:53:49 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:53:49 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:53:49 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:53:49 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:53:50 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:53:54 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:53:54 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:53:54 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:53:54 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:53:54 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:53:54 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:53:54 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:54:10 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 12-13 11:54:10 [config.py:1472] Using max model len 32768
INFO 12-13 11:54:10 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-13 11:54:10 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:54:10 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:54:10 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:54:10 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
WARNING 12-13 11:54:12 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-13 11:54:20 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:54:20 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:54:20 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:54:20 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:54:21 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:54:24 [core.py:526] Waiting for init message from front-end.
INFO 12-13 11:54:25 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-13 11:54:26 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-13 11:54:26 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-13 11:54:26 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-13 11:54:26 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-13 11:54:26 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-13 11:54:26 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-13 11:54:26 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-1B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-1B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-13 11:54:39 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-13 11:54:39 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-13 11:54:39 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-13 11:54:39 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-13 11:54:40 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-13 11:54:46 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-13 11:54:46 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-1B-V1.1...
INFO 12-13 11:54:48 [default_loader.py:272] Loading weights took 0.70 seconds
INFO 12-13 11:54:51 [model_runner_v1.py:1778] Loading model weights took 2.1617 GB
INFO 12-13 11:54:59 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/866e4cb0f9/rank_0_0/backbone for vLLM's torch.compile
INFO 12-13 11:54:59 [backends.py:519] Dynamo bytecode transform time: 7.18 s
INFO 12-13 11:55:01 [backends.py:193] Compiling a graph for general shape takes 1.95 s
INFO 12-13 11:55:10 [monitor.py:34] torch.compile takes 9.13 s in total
INFO 12-13 11:55:11 [worker_v1.py:181] Available memory: 55445656268, total memory: 65464696832
INFO 12-13 11:55:11 [kv_cache_utils.py:716] GPU KV cache size: 694,144 tokens
INFO 12-13 11:55:11 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 21.18x
INFO 12-13 11:56:30 [model_runner_v1.py:2074] Graph capturing finished in 78 secs, took 0.17 GiB
INFO 12-13 11:56:30 [core.py:172] init engine (profile, create kv cache, warmup model) took 98.97 seconds
WARNING 12-13 11:56:31 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-13 11:56:31 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-13 11:56:31 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-13 11:56:31 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 71
INFO 12-13 11:56:31 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 26) with 67 sizes
Warming up...
Avg latency: 0.4861820418387651 seconds
10% percentile latency: 0.48443354056216775 seconds
25% percentile latency: 0.48504547332413495 seconds
50% percentile latency: 0.4857153140474111 seconds
75% percentile latency: 0.4871965523343533 seconds
90% percentile latency: 0.48833894496783614 seconds
99% percentile latency: 0.488763629598543 seconds
