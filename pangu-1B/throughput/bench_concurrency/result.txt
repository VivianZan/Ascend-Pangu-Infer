INFO 12-18 17:40:30 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 17:40:30 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 17:40:30 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 17:40:30 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 17:40:31 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 17:40:35 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 17:40:35 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 17:40:35 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 17:40:35 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 17:40:35 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 17:40:35 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 17:40:35 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0xfffceb1bb560>, seed=0, num_prompts=100, dataset_name='random', dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', max_concurrency=None, model='pangu_embedded_1b', tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1/', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=True, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)
WARNING 12-18 17:40:38 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 17:40:38 [datasets.py:355] Sampling input_len from [1023, 1023] and output_len from [128, 128]
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: None
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:02<03:28,  2.11s/it]  3%|▎         | 3/100 [00:03<01:52,  1.15s/it]  4%|▍         | 4/100 [00:05<02:05,  1.31s/it]  5%|▌         | 5/100 [00:06<02:12,  1.39s/it]  6%|▌         | 6/100 [00:08<02:15,  1.44s/it]  7%|▋         | 7/100 [00:10<02:17,  1.48s/it]  9%|▉         | 9/100 [00:11<01:45,  1.16s/it] 10%|█         | 10/100 [00:13<01:53,  1.26s/it] 11%|█         | 11/100 [00:14<01:59,  1.34s/it] 12%|█▏        | 12/100 [00:16<02:03,  1.40s/it] 14%|█▍        | 14/100 [00:17<01:37,  1.13s/it] 15%|█▌        | 15/100 [00:19<01:44,  1.23s/it] 16%|█▌        | 16/100 [00:21<01:50,  1.32s/it] 17%|█▋        | 17/100 [00:22<01:55,  1.39s/it] 20%|██        | 20/100 [00:24<01:15,  1.06it/s] 21%|██        | 21/100 [00:25<01:24,  1.08s/it] 22%|██▏       | 22/100 [00:27<01:33,  1.19s/it] 23%|██▎       | 23/100 [00:29<01:39,  1.29s/it] 24%|██▍       | 24/100 [00:30<01:43,  1.37s/it] 26%|██▌       | 26/100 [00:32<01:24,  1.14s/it] 28%|██▊       | 28/100 [00:33<01:13,  1.02s/it] 29%|██▉       | 29/100 [00:35<01:20,  1.13s/it] 30%|███       | 30/100 [00:37<01:26,  1.24s/it] 32%|███▏      | 32/100 [00:38<01:12,  1.06s/it] 34%|███▍      | 34/100 [00:40<01:04,  1.03it/s] 37%|███▋      | 37/100 [00:41<00:50,  1.26it/s] 39%|███▉      | 39/100 [00:42<00:35,  1.74it/s] 40%|████      | 40/100 [00:43<00:45,  1.33it/s] 43%|████▎     | 43/100 [00:45<00:38,  1.50it/s] 45%|████▌     | 45/100 [00:46<00:39,  1.40it/s] 47%|████▋     | 47/100 [00:48<00:39,  1.34it/s] 48%|████▊     | 48/100 [00:50<00:46,  1.12it/s] 49%|████▉     | 49/100 [00:51<00:52,  1.03s/it] 51%|█████     | 51/100 [00:53<00:46,  1.05it/s] 52%|█████▏    | 52/100 [00:55<00:51,  1.08s/it] 53%|█████▎    | 53/100 [00:56<00:55,  1.19s/it] 54%|█████▍    | 54/100 [00:58<00:59,  1.28s/it] 56%|█████▌    | 56/100 [00:59<00:47,  1.09s/it] 57%|█████▋    | 57/100 [01:01<00:51,  1.20s/it] 58%|█████▊    | 58/100 [01:02<00:54,  1.30s/it] 59%|█████▉    | 59/100 [01:04<00:56,  1.37s/it] 60%|██████    | 60/100 [01:06<00:57,  1.43s/it] 62%|██████▏   | 62/100 [01:07<00:44,  1.17s/it] 63%|██████▎   | 63/100 [01:09<00:46,  1.27s/it] 65%|██████▌   | 65/100 [01:09<00:27,  1.29it/s] 68%|██████▊   | 68/100 [01:11<00:21,  1.48it/s] 70%|███████   | 70/100 [01:12<00:21,  1.39it/s] 72%|███████▏  | 72/100 [01:13<00:15,  1.83it/s] 73%|███████▎  | 73/100 [01:14<00:19,  1.38it/s] 74%|███████▍  | 74/100 [01:16<00:23,  1.12it/s] 76%|███████▌  | 76/100 [01:17<00:20,  1.15it/s] 79%|███████▉  | 79/100 [01:19<00:15,  1.36it/s] 82%|████████▏ | 82/100 [01:21<00:11,  1.50it/s] 83%|████████▎ | 83/100 [01:22<00:13,  1.23it/s] 85%|████████▌ | 85/100 [01:22<00:08,  1.73it/s] 86%|████████▌ | 86/100 [01:24<00:10,  1.31it/s] 87%|████████▋ | 87/100 [01:26<00:12,  1.07it/s] 89%|████████▉ | 89/100 [01:27<00:09,  1.11it/s] 90%|█████████ | 90/100 [01:29<00:10,  1.04s/it] 91%|█████████ | 91/100 [01:30<00:10,  1.17s/it] 92%|█████████▏| 92/100 [01:32<00:10,  1.27s/it] 93%|█████████▎| 93/100 [01:34<00:09,  1.35s/it] 94%|█████████▍| 94/100 [01:35<00:08,  1.42s/it] 96%|█████████▌| 96/100 [01:35<00:03,  1.21it/s] 98%|█████████▊| 98/100 [01:37<00:01,  1.22it/s] 99%|█████████▉| 99/100 [01:39<00:00,  1.01it/s]100%|██████████| 100/100 [01:40<00:00,  1.13s/it]100%|██████████| 100/100 [01:40<00:00,  1.01s/it]
============ Serving Benchmark Result ============
Successful requests:                     100       
Benchmark duration (s):                  100.66    
Total input tokens:                      102300    
Total generated tokens:                  8035      
Request throughput (req/s):              0.99      
Output token throughput (tok/s):         79.82     
Total Token throughput (tok/s):          1096.08   
---------------Time to First Token----------------
Mean TTFT (ms):                          52256.66  
Median TTFT (ms):                        51836.93  
P99 TTFT (ms):                           97459.20  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          12.19     
Median TPOT (ms):                        12.25     
P99 TPOT (ms):                           12.69     
---------------Inter-token Latency----------------
Mean ITL (ms):                           12.19     
Median ITL (ms):                         12.12     
P99 ITL (ms):                            13.03     
==================================================
INFO 12-18 17:42:32 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 17:42:32 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 17:42:32 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 17:42:32 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 17:42:33 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 17:42:37 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 17:42:37 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 17:42:37 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 17:42:37 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 17:42:37 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 17:42:37 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 17:42:37 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0xfffcdc6c7560>, seed=0, num_prompts=100, dataset_name='random', dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=8001, endpoint='/v1/completions', max_concurrency=None, model='pangu_embedded_1b', tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1/', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=True, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)
WARNING 12-18 17:42:40 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 17:42:41 [datasets.py:355] Sampling input_len from [1023, 1023] and output_len from [128, 128]
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: None
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<01:50,  1.12s/it]  2%|▏         | 2/100 [00:01<00:55,  1.76it/s]  4%|▍         | 4/100 [00:02<00:52,  1.83it/s]  5%|▌         | 5/100 [00:02<00:39,  2.43it/s]  9%|▉         | 9/100 [00:02<00:15,  6.07it/s] 11%|█         | 11/100 [00:02<00:11,  7.52it/s] 13%|█▎        | 13/100 [00:02<00:09,  8.75it/s] 15%|█▌        | 15/100 [00:04<00:26,  3.16it/s] 17%|█▋        | 17/100 [00:04<00:20,  4.06it/s] 21%|██        | 21/100 [00:04<00:11,  6.85it/s] 24%|██▍       | 24/100 [00:04<00:09,  8.31it/s] 28%|██▊       | 28/100 [00:06<00:16,  4.41it/s] 30%|███       | 30/100 [00:06<00:13,  5.14it/s] 34%|███▍      | 34/100 [00:06<00:08,  7.59it/s] 37%|███▋      | 37/100 [00:06<00:06,  9.63it/s] 41%|████      | 41/100 [00:07<00:04, 11.89it/s] 44%|████▍     | 44/100 [00:08<00:11,  4.79it/s] 46%|████▌     | 46/100 [00:08<00:09,  5.42it/s] 49%|████▉     | 49/100 [00:08<00:07,  7.07it/s] 51%|█████     | 51/100 [00:09<00:06,  8.06it/s] 54%|█████▍    | 54/100 [00:10<00:11,  3.87it/s] 56%|█████▌    | 56/100 [00:10<00:09,  4.61it/s] 59%|█████▉    | 59/100 [00:11<00:06,  6.22it/s] 64%|██████▍   | 64/100 [00:11<00:03,  9.51it/s] 67%|██████▋   | 67/100 [00:11<00:02, 11.13it/s] 69%|██████▉   | 69/100 [00:11<00:02, 10.86it/s] 71%|███████   | 71/100 [00:12<00:06,  4.52it/s] 73%|███████▎  | 73/100 [00:13<00:05,  5.17it/s] 77%|███████▋  | 77/100 [00:13<00:02,  7.93it/s] 80%|████████  | 80/100 [00:13<00:02,  9.87it/s] 82%|████████▏ | 82/100 [00:13<00:01, 10.56it/s] 84%|████████▍ | 84/100 [00:13<00:01,  9.82it/s] 86%|████████▌ | 86/100 [00:15<00:03,  4.07it/s] 88%|████████▊ | 88/100 [00:15<00:02,  4.66it/s] 90%|█████████ | 90/100 [00:15<00:01,  5.85it/s] 92%|█████████▏| 92/100 [00:15<00:01,  7.01it/s] 95%|█████████▌| 95/100 [00:15<00:00,  8.59it/s] 97%|█████████▋| 97/100 [00:17<00:00,  3.68it/s]100%|██████████| 100/100 [00:17<00:00,  5.08it/s]100%|██████████| 100/100 [00:17<00:00,  5.75it/s]
============ Serving Benchmark Result ============
Successful requests:                     100       
Benchmark duration (s):                  17.40     
Total input tokens:                      102300    
Total generated tokens:                  7909      
Request throughput (req/s):              5.75      
Output token throughput (tok/s):         454.57    
Total Token throughput (tok/s):          6334.24   
---------------Time to First Token----------------
Mean TTFT (ms):                          7988.67   
Median TTFT (ms):                        7112.91   
P99 TTFT (ms):                           15435.85  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.62     
Median TPOT (ms):                        15.89     
P99 TPOT (ms):                           36.02     
---------------Inter-token Latency----------------
Mean ITL (ms):                           15.96     
Median ITL (ms):                         14.38     
P99 ITL (ms):                            36.01     
==================================================
INFO 12-18 17:43:11 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 17:43:11 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 17:43:11 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 17:43:11 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 17:43:12 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 17:43:16 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 17:43:16 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 17:43:16 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 17:43:16 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 17:43:16 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 17:43:16 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 17:43:16 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0xfffce37c7560>, seed=0, num_prompts=100, dataset_name='random', dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=8002, endpoint='/v1/completions', max_concurrency=None, model='pangu_embedded_1b', tokenizer='/opt/pangu/openPangu-Embedded-1B-V1.1/', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=True, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)
WARNING 12-18 17:43:19 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 17:43:20 [datasets.py:355] Sampling input_len from [1023, 1023] and output_len from [128, 128]
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: None
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<02:09,  1.31s/it]  5%|▌         | 5/100 [00:01<00:21,  4.49it/s]  7%|▋         | 7/100 [00:02<00:38,  2.43it/s]  9%|▉         | 9/100 [00:03<00:26,  3.37it/s] 12%|█▏        | 12/100 [00:03<00:16,  5.40it/s] 16%|█▌        | 16/100 [00:03<00:09,  8.73it/s] 19%|█▉        | 19/100 [00:03<00:07, 10.82it/s] 22%|██▏       | 22/100 [00:03<00:05, 13.40it/s] 25%|██▌       | 25/100 [00:03<00:04, 15.31it/s] 30%|███       | 30/100 [00:03<00:03, 21.31it/s] 35%|███▌      | 35/100 [00:03<00:02, 25.77it/s] 39%|███▉      | 39/100 [00:05<00:08,  6.87it/s] 42%|████▏     | 42/100 [00:05<00:07,  8.16it/s] 45%|████▌     | 45/100 [00:05<00:05,  9.73it/s] 48%|████▊     | 48/100 [00:05<00:04, 10.47it/s] 53%|█████▎    | 53/100 [00:06<00:03, 14.73it/s] 59%|█████▉    | 59/100 [00:06<00:01, 20.83it/s] 63%|██████▎   | 63/100 [00:07<00:05,  7.36it/s] 66%|██████▌   | 66/100 [00:07<00:03,  8.58it/s] 70%|███████   | 70/100 [00:07<00:02, 11.16it/s] 74%|███████▍  | 74/100 [00:08<00:01, 13.30it/s] 77%|███████▋  | 77/100 [00:08<00:01, 14.69it/s] 80%|████████  | 80/100 [00:08<00:01, 15.09it/s] 83%|████████▎ | 83/100 [00:08<00:01, 16.60it/s] 86%|████████▌ | 86/100 [00:08<00:00, 17.55it/s] 89%|████████▉ | 89/100 [00:10<00:01,  5.82it/s] 94%|█████████▍| 94/100 [00:10<00:00,  8.84it/s] 97%|█████████▋| 97/100 [00:10<00:00, 10.24it/s]100%|██████████| 100/100 [00:10<00:00, 12.26it/s]100%|██████████| 100/100 [00:10<00:00,  9.52it/s]
============ Serving Benchmark Result ============
Successful requests:                     100       
Benchmark duration (s):                  10.50     
Total input tokens:                      102300    
Total generated tokens:                  7909      
Request throughput (req/s):              9.52      
Output token throughput (tok/s):         753.06    
Total Token throughput (tok/s):          10493.55  
---------------Time to First Token----------------
Mean TTFT (ms):                          4696.29   
Median TTFT (ms):                        3886.11   
P99 TTFT (ms):                           8552.17   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.49     
Median TPOT (ms):                        17.84     
P99 TPOT (ms):                           36.74     
---------------Inter-token Latency----------------
Mean ITL (ms):                           17.59     
Median ITL (ms):                         14.86     
P99 ITL (ms):                            37.38     
==================================================
