No optim
Batch-1 Input len-32 Output len-128
INFO 12-18 13:37:55 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:37:55 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:37:55 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:37:55 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:37:56 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:38:00 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 13:38:00 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 13:38:00 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 13:38:00 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 13:38:00 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 13:38:00 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 13:38:00 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 13:38:16 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 12-18 13:38:16 [config.py:1472] Using max model len 32768
INFO 12-18 13:38:16 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-18 13:38:16 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 13:38:16 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 13:38:16 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 13:38:16 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-18 13:38:17 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 13:38:26 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:38:26 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:38:26 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:38:26 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:38:28 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:38:30 [core.py:526] Waiting for init message from front-end.
INFO 12-18 13:38:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 13:38:32 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 13:38:32 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 13:38:32 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 13:38:32 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 13:38:32 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 13:38:32 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 13:38:32 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-18 13:38:45 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:38:45 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:38:45 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:38:45 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:38:46 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:38:52 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-18 13:38:53 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.61it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.21it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.12it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.35it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]

INFO 12-18 13:38:57 [default_loader.py:272] Loading weights took 3.13 seconds
INFO 12-18 13:38:59 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-18 13:39:10 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-18 13:39:10 [backends.py:519] Dynamo bytecode transform time: 9.30 s
INFO 12-18 13:39:13 [backends.py:193] Compiling a graph for general shape takes 2.58 s
INFO 12-18 13:39:21 [monitor.py:34] torch.compile takes 11.88 s in total
INFO 12-18 13:39:23 [worker_v1.py:181] Available memory: 41209295564, total memory: 65464696832
INFO 12-18 13:39:23 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-18 13:39:23 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-18 13:39:26 [model_runner_v1.py:2074] Graph capturing finished in 3 secs, took 0.10 GiB
INFO 12-18 13:39:26 [core.py:172] init engine (profile, create kv cache, warmup model) took 27.05 seconds
WARNING 12-18 13:39:27 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-18 13:39:28 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 13:39:28 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 13:39:28 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 13:39:28 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...

Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]
Warmup iterations:  20%|██        | 1/5 [00:05<00:20,  5.01s/it]
Warmup iterations:  40%|████      | 2/5 [00:09<00:14,  4.97s/it]
Warmup iterations:  60%|██████    | 3/5 [00:14<00:09,  4.97s/it]
Warmup iterations:  80%|████████  | 4/5 [00:20<00:05,  5.02s/it]
Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.05s/it]
Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.02s/it]

Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]
Profiling iterations:  10%|█         | 1/10 [00:05<00:45,  5.11s/it]
Profiling iterations:  20%|██        | 2/10 [00:10<00:40,  5.11s/it]
Profiling iterations:  30%|███       | 3/10 [00:15<00:35,  5.10s/it]
Profiling iterations:  40%|████      | 4/10 [00:20<00:30,  5.10s/it]
Profiling iterations:  50%|█████     | 5/10 [00:25<00:25,  5.10s/it]
Profiling iterations:  60%|██████    | 6/10 [00:30<00:20,  5.09s/it]
Profiling iterations:  70%|███████   | 7/10 [00:35<00:15,  5.09s/it]
Profiling iterations:  80%|████████  | 8/10 [00:40<00:10,  5.09s/it]
Profiling iterations:  90%|█████████ | 9/10 [00:45<00:05,  5.09s/it]
Profiling iterations: 100%|██████████| 10/10 [00:50<00:00,  5.09s/it]
Profiling iterations: 100%|██████████| 10/10 [00:50<00:00,  5.09s/it]
Avg latency: 5.0937850848771635 seconds
10% percentile latency: 5.083229633793235 seconds
25% percentile latency: 5.08781281625852 seconds
50% percentile latency: 5.092266065534204 seconds
75% percentile latency: 5.101832756539807 seconds
90% percentile latency: 5.104464289639145 seconds
99% percentile latency: 5.1076720490958545 seconds
No optim
Batch-1 Input len-128 Output len-128
INFO 12-18 13:41:00 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:41:00 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:41:00 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:41:00 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:41:01 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:41:05 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 13:41:05 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 13:41:05 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 13:41:05 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 13:41:05 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 13:41:05 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 13:41:05 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 13:41:22 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 12-18 13:41:22 [config.py:1472] Using max model len 32768
INFO 12-18 13:41:22 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-18 13:41:22 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 13:41:22 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 13:41:22 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 13:41:22 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-18 13:41:23 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 13:41:31 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:41:31 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:41:31 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:41:31 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:41:33 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:41:35 [core.py:526] Waiting for init message from front-end.
INFO 12-18 13:41:37 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 13:41:37 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 13:41:37 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 13:41:37 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 13:41:37 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 13:41:37 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 13:41:37 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 13:41:37 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-18 13:41:50 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:41:50 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:41:50 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:41:50 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:41:51 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:41:57 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-18 13:41:57 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.53it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.08it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.02s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.20it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.16it/s]

INFO 12-18 13:42:02 [default_loader.py:272] Loading weights took 3.60 seconds
INFO 12-18 13:42:04 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-18 13:42:15 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-18 13:42:15 [backends.py:519] Dynamo bytecode transform time: 9.21 s
INFO 12-18 13:42:18 [backends.py:193] Compiling a graph for general shape takes 2.54 s
INFO 12-18 13:42:27 [monitor.py:34] torch.compile takes 11.75 s in total
INFO 12-18 13:42:29 [worker_v1.py:181] Available memory: 41209791180, total memory: 65464696832
INFO 12-18 13:42:29 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-18 13:42:29 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-18 13:42:33 [model_runner_v1.py:2074] Graph capturing finished in 4 secs, took 0.10 GiB
INFO 12-18 13:42:33 [core.py:172] init engine (profile, create kv cache, warmup model) took 28.30 seconds
WARNING 12-18 13:42:34 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-18 13:42:34 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 13:42:34 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 13:42:34 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 13:42:34 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...

Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]
Warmup iterations:  20%|██        | 1/5 [00:05<00:20,  5.20s/it]
Warmup iterations:  40%|████      | 2/5 [00:10<00:15,  5.15s/it]
Warmup iterations:  60%|██████    | 3/5 [00:15<00:10,  5.13s/it]
Warmup iterations:  80%|████████  | 4/5 [00:20<00:05,  5.14s/it]
Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.14s/it]
Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.14s/it]

Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]
Profiling iterations:  10%|█         | 1/10 [00:05<00:45,  5.10s/it]
Profiling iterations:  20%|██        | 2/10 [00:10<00:40,  5.04s/it]
Profiling iterations:  30%|███       | 3/10 [00:15<00:34,  4.98s/it]
Profiling iterations:  40%|████      | 4/10 [00:19<00:29,  4.93s/it]
Profiling iterations:  50%|█████     | 5/10 [00:24<00:24,  4.91s/it]
Profiling iterations:  60%|██████    | 6/10 [00:29<00:19,  4.90s/it]
Profiling iterations:  70%|███████   | 7/10 [00:34<00:14,  4.92s/it]
Profiling iterations:  80%|████████  | 8/10 [00:39<00:09,  4.92s/it]
Profiling iterations:  90%|█████████ | 9/10 [00:44<00:04,  4.92s/it]
Profiling iterations: 100%|██████████| 10/10 [00:49<00:00,  4.92s/it]
Profiling iterations: 100%|██████████| 10/10 [00:49<00:00,  4.93s/it]
Avg latency: 4.930652057658881 seconds
10% percentile latency: 4.8673819050192835 seconds
25% percentile latency: 4.89176941011101 seconds
50% percentile latency: 4.915675948373973 seconds
75% percentile latency: 4.950195888988674 seconds
90% percentile latency: 5.002777318377047 seconds
99% percentile latency: 5.089856665758416 seconds
No optim
Batch-1 Input len-512 Output len-128
INFO 12-18 13:44:05 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:44:05 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:44:05 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:44:05 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:44:06 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:44:10 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 13:44:10 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 13:44:10 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 13:44:10 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 13:44:10 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 13:44:10 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 13:44:10 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 13:44:27 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 12-18 13:44:27 [config.py:1472] Using max model len 32768
INFO 12-18 13:44:27 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-18 13:44:27 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 13:44:27 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 13:44:27 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 13:44:27 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-18 13:44:28 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 13:44:37 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:44:37 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:44:37 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:44:37 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:44:38 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:44:40 [core.py:526] Waiting for init message from front-end.
INFO 12-18 13:44:42 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 13:44:42 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 13:44:42 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 13:44:42 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 13:44:42 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 13:44:42 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 13:44:42 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 13:44:42 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-18 13:44:55 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:44:55 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:44:55 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:44:55 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:44:56 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:45:02 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-18 13:45:03 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.52it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.09it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.01s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.20it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.16it/s]

INFO 12-18 13:45:08 [default_loader.py:272] Loading weights took 3.60 seconds
INFO 12-18 13:45:10 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-18 13:45:20 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-18 13:45:20 [backends.py:519] Dynamo bytecode transform time: 9.28 s
INFO 12-18 13:45:23 [backends.py:193] Compiling a graph for general shape takes 2.55 s
INFO 12-18 13:45:33 [monitor.py:34] torch.compile takes 11.83 s in total
INFO 12-18 13:45:34 [worker_v1.py:181] Available memory: 41210008268, total memory: 65464696832
INFO 12-18 13:45:34 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-18 13:45:34 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-18 13:45:38 [model_runner_v1.py:2074] Graph capturing finished in 4 secs, took 0.10 GiB
INFO 12-18 13:45:38 [core.py:172] init engine (profile, create kv cache, warmup model) took 28.55 seconds
WARNING 12-18 13:45:39 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-18 13:45:39 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 13:45:39 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 13:45:39 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 13:45:39 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...

Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]
Warmup iterations:  20%|██        | 1/5 [00:05<00:20,  5.16s/it]
Warmup iterations:  40%|████      | 2/5 [00:10<00:15,  5.12s/it]
Warmup iterations:  60%|██████    | 3/5 [00:15<00:10,  5.11s/it]
Warmup iterations:  80%|████████  | 4/5 [00:20<00:05,  5.10s/it]
Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.10s/it]
Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.10s/it]

Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]
Profiling iterations:  10%|█         | 1/10 [00:05<00:45,  5.09s/it]
Profiling iterations:  20%|██        | 2/10 [00:10<00:40,  5.09s/it]
Profiling iterations:  30%|███       | 3/10 [00:15<00:35,  5.09s/it]
Profiling iterations:  40%|████      | 4/10 [00:20<00:30,  5.09s/it]
Profiling iterations:  50%|█████     | 5/10 [00:25<00:25,  5.09s/it]
Profiling iterations:  60%|██████    | 6/10 [00:30<00:20,  5.09s/it]
Profiling iterations:  70%|███████   | 7/10 [00:35<00:15,  5.09s/it]
Profiling iterations:  80%|████████  | 8/10 [00:40<00:10,  5.09s/it]
Profiling iterations:  90%|█████████ | 9/10 [00:45<00:05,  5.09s/it]
Profiling iterations: 100%|██████████| 10/10 [00:50<00:00,  5.09s/it]
Profiling iterations: 100%|██████████| 10/10 [00:50<00:00,  5.09s/it]
Avg latency: 5.0880038786679505 seconds
10% percentile latency: 5.082322511542588 seconds
25% percentile latency: 5.083704255288467 seconds
50% percentile latency: 5.085941885132343 seconds
75% percentile latency: 5.090150059899315 seconds
90% percentile latency: 5.094461425207555 seconds
99% percentile latency: 5.107293099723757 seconds
No optim
Batch-1 Input len-2048 Output len-128
INFO 12-18 13:47:12 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:47:12 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:47:12 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:47:12 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:47:13 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:47:17 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 13:47:18 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 13:47:18 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 13:47:18 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 13:47:18 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 13:47:18 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 13:47:18 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 13:47:34 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 12-18 13:47:34 [config.py:1472] Using max model len 32768
INFO 12-18 13:47:34 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-18 13:47:34 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 13:47:34 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 13:47:34 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 13:47:34 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-18 13:47:35 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 13:47:43 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:47:43 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:47:43 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:47:43 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:47:45 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:47:47 [core.py:526] Waiting for init message from front-end.
INFO 12-18 13:47:49 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 13:47:49 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 13:47:49 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 13:47:49 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 13:47:49 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 13:47:49 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 13:47:49 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 13:47:49 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-18 13:48:02 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:48:02 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:48:02 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:48:02 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:48:03 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:48:09 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-18 13:48:09 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.61it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.21it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.12it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.34it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]

INFO 12-18 13:48:14 [default_loader.py:272] Loading weights took 3.16 seconds
INFO 12-18 13:48:16 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-18 13:48:26 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-18 13:48:26 [backends.py:519] Dynamo bytecode transform time: 9.30 s
INFO 12-18 13:48:30 [backends.py:193] Compiling a graph for general shape takes 2.55 s
INFO 12-18 13:48:38 [monitor.py:34] torch.compile takes 11.85 s in total
INFO 12-18 13:48:40 [worker_v1.py:181] Available memory: 41209094860, total memory: 65464696832
INFO 12-18 13:48:40 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-18 13:48:40 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-18 13:48:44 [model_runner_v1.py:2074] Graph capturing finished in 3 secs, took 0.10 GiB
INFO 12-18 13:48:44 [core.py:172] init engine (profile, create kv cache, warmup model) took 27.78 seconds
WARNING 12-18 13:48:45 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-18 13:48:45 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 13:48:45 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 13:48:45 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 13:48:45 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...

Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]
Warmup iterations:  20%|██        | 1/5 [00:05<00:20,  5.02s/it]
Warmup iterations:  40%|████      | 2/5 [00:10<00:15,  5.02s/it]
Warmup iterations:  60%|██████    | 3/5 [00:15<00:10,  5.01s/it]
Warmup iterations:  80%|████████  | 4/5 [00:20<00:05,  5.03s/it]
Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.02s/it]
Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.02s/it]

Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]
Profiling iterations:  10%|█         | 1/10 [00:05<00:45,  5.01s/it]
Profiling iterations:  20%|██        | 2/10 [00:09<00:39,  4.99s/it]
Profiling iterations:  30%|███       | 3/10 [00:14<00:34,  4.99s/it]
Profiling iterations:  40%|████      | 4/10 [00:19<00:29,  4.99s/it]
Profiling iterations:  50%|█████     | 5/10 [00:24<00:24,  4.99s/it]
Profiling iterations:  60%|██████    | 6/10 [00:29<00:19,  4.99s/it]
Profiling iterations:  70%|███████   | 7/10 [00:34<00:14,  4.99s/it]
Profiling iterations:  80%|████████  | 8/10 [00:39<00:09,  4.99s/it]
Profiling iterations:  90%|█████████ | 9/10 [00:44<00:04,  4.99s/it]
Profiling iterations: 100%|██████████| 10/10 [00:49<00:00,  4.99s/it]
Profiling iterations: 100%|██████████| 10/10 [00:49<00:00,  4.99s/it]
Avg latency: 4.990393354929983 seconds
10% percentile latency: 4.9793703473173085 seconds
25% percentile latency: 4.981352559523657 seconds
50% percentile latency: 4.991555265150964 seconds
75% percentile latency: 4.996611429611221 seconds
90% percentile latency: 4.999274272006005 seconds
99% percentile latency: 5.006994390813634 seconds
No optim
Batch-1 Input len-8192 Output len-128
INFO 12-18 13:50:18 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:50:18 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:50:18 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:50:18 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:50:19 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:50:23 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 13:50:23 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 13:50:23 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 13:50:23 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 13:50:23 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 13:50:23 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 13:50:23 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 13:50:40 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-18 13:50:40 [config.py:1472] Using max model len 32768
INFO 12-18 13:50:40 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-18 13:50:40 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 13:50:40 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 13:50:40 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 13:50:40 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-18 13:50:41 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 13:50:49 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:50:49 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:50:49 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:50:49 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:50:50 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:50:53 [core.py:526] Waiting for init message from front-end.
INFO 12-18 13:50:54 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 13:50:54 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 13:50:54 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 13:50:54 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 13:50:54 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 13:50:54 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 13:50:54 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 13:50:54 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-18 13:51:07 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 13:51:07 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 13:51:07 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 13:51:07 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 13:51:08 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 13:51:15 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-18 13:51:15 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.57it/s]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.17it/s]

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.31it/s]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.26it/s]

INFO 12-18 13:51:19 [default_loader.py:272] Loading weights took 3.35 seconds
INFO 12-18 13:51:20 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-18 13:51:29 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-18 13:51:29 [backends.py:519] Dynamo bytecode transform time: 8.70 s
INFO 12-18 13:51:32 [backends.py:193] Compiling a graph for general shape takes 2.31 s
INFO 12-18 13:51:41 [monitor.py:34] torch.compile takes 11.02 s in total
INFO 12-18 13:51:42 [worker_v1.py:181] Available memory: 41209352908, total memory: 65464696832
INFO 12-18 13:51:43 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-18 13:51:43 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-18 13:51:46 [model_runner_v1.py:2074] Graph capturing finished in 3 secs, took 0.10 GiB
INFO 12-18 13:51:46 [core.py:172] init engine (profile, create kv cache, warmup model) took 25.62 seconds
WARNING 12-18 13:51:47 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-18 13:51:47 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 13:51:47 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 13:51:47 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 13:51:47 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...

Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]
Warmup iterations:  20%|██        | 1/5 [00:05<00:22,  5.60s/it]
Warmup iterations:  40%|████      | 2/5 [00:11<00:16,  5.62s/it]
Warmup iterations:  60%|██████    | 3/5 [00:16<00:11,  5.63s/it]
Warmup iterations:  80%|████████  | 4/5 [00:22<00:05,  5.60s/it]
Warmup iterations: 100%|██████████| 5/5 [00:27<00:00,  5.58s/it]
Warmup iterations: 100%|██████████| 5/5 [00:27<00:00,  5.60s/it]

Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]
Profiling iterations:  10%|█         | 1/10 [00:05<00:49,  5.55s/it]
Profiling iterations:  20%|██        | 2/10 [00:11<00:44,  5.55s/it]
Profiling iterations:  30%|███       | 3/10 [00:16<00:38,  5.55s/it]
Profiling iterations:  40%|████      | 4/10 [00:22<00:33,  5.62s/it]
Profiling iterations:  50%|█████     | 5/10 [00:27<00:28,  5.62s/it]
Profiling iterations:  60%|██████    | 6/10 [00:33<00:22,  5.60s/it]
Profiling iterations:  70%|███████   | 7/10 [00:38<00:16,  5.47s/it]
Profiling iterations:  80%|████████  | 8/10 [00:44<00:10,  5.48s/it]
Profiling iterations:  90%|█████████ | 9/10 [00:49<00:05,  5.51s/it]
Profiling iterations: 100%|██████████| 10/10 [00:55<00:00,  5.53s/it]
Profiling iterations: 100%|██████████| 10/10 [00:55<00:00,  5.54s/it]
Avg latency: 5.539974341727794 seconds
10% percentile latency: 5.476258600410074 seconds
25% percentile latency: 5.546201089164242 seconds
50% percentile latency: 5.554067681077868 seconds
75% percentile latency: 5.569068569689989 seconds
90% percentile latency: 5.6280559648759665 seconds
99% percentile latency: 5.716063901064917 seconds
Batch-1 Input len-32 Output len-128
INFO 12-18 14:15:47 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:15:47 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:15:47 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:15:47 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:15:48 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:15:52 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 14:15:52 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 14:15:52 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 14:15:52 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 14:15:52 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 14:15:52 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 14:15:52 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 14:16:09 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 12-18 14:16:09 [config.py:1472] Using max model len 32768
WARNING 12-18 14:16:09 [config.py:960] ascend quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 12-18 14:16:09 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=4096.
WARNING 12-18 14:16:09 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 14:16:09 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 14:16:09 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 14:16:09 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-18 14:16:10 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 14:16:18 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:16:18 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:16:18 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:16:18 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:16:20 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:16:23 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
INFO 12-18 14:16:24 [core.py:526] Waiting for init message from front-end.
WARNING 12-18 14:16:24 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 14:16:24 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 14:16:24 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 14:16:24 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 14:16:24 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 14:16:24 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 14:16:24 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/run/models/openPangu-7B-a8w8', speculative_config=None, tokenizer='/run/models/openPangu-7B-a8w8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/run/models/openPangu-7B-a8w8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-18 14:16:37 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:16:37 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:16:37 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:16:37 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:16:38 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:16:44 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-18 14:16:45 [model_runner_v1.py:1746] Starting to load model /run/models/openPangu-7B-a8w8...
INFO 12-18 14:16:46 [quantizer.py:89] Using the vLLM Ascend Quantizer version now!

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.66s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.66s/it]

INFO 12-18 14:16:49 [default_loader.py:272] Loading weights took 2.48 seconds
INFO 12-18 14:16:51 [model_runner_v1.py:1778] Loading model weights took 10.3344 GB
INFO 12-18 14:17:04 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/393c0866bd/rank_0_0/backbone for vLLM's torch.compile
INFO 12-18 14:17:04 [backends.py:519] Dynamo bytecode transform time: 11.21 s
INFO 12-18 14:17:07 [backends.py:193] Compiling a graph for general shape takes 1.90 s
INFO 12-18 14:17:16 [monitor.py:34] torch.compile takes 13.11 s in total
INFO 12-18 14:17:18 [worker_v1.py:181] Available memory: 46443283148, total memory: 65464696832
INFO 12-18 14:17:18 [kv_cache_utils.py:716] GPU KV cache size: 333,440 tokens
INFO 12-18 14:17:18 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 10.18x
INFO 12-18 14:18:26 [model_runner_v1.py:2074] Graph capturing finished in 68 secs, took 0.28 GiB
INFO 12-18 14:18:26 [core.py:172] init engine (profile, create kv cache, warmup model) took 95.45 seconds
WARNING 12-18 14:18:27 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-18 14:18:28 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 14:18:28 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 14:18:28 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 14:18:28 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...

Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]
Warmup iterations:  20%|██        | 1/5 [00:02<00:08,  2.17s/it]
Warmup iterations:  40%|████      | 2/5 [00:04<00:06,  2.09s/it]
Warmup iterations:  60%|██████    | 3/5 [00:06<00:04,  2.07s/it]
Warmup iterations:  80%|████████  | 4/5 [00:08<00:02,  2.06s/it]
Warmup iterations: 100%|██████████| 5/5 [00:10<00:00,  2.05s/it]
Warmup iterations: 100%|██████████| 5/5 [00:10<00:00,  2.06s/it]

Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]
Profiling iterations:  10%|█         | 1/10 [00:02<00:18,  2.04s/it]
Profiling iterations:  20%|██        | 2/10 [00:04<00:16,  2.04s/it]
Profiling iterations:  30%|███       | 3/10 [00:06<00:14,  2.03s/it]
Profiling iterations:  40%|████      | 4/10 [00:08<00:12,  2.03s/it]
Profiling iterations:  50%|█████     | 5/10 [00:10<00:10,  2.03s/it]
Profiling iterations:  60%|██████    | 6/10 [00:12<00:08,  2.03s/it]
Profiling iterations:  70%|███████   | 7/10 [00:14<00:06,  2.03s/it]
Profiling iterations:  80%|████████  | 8/10 [00:16<00:04,  2.03s/it]
Profiling iterations:  90%|█████████ | 9/10 [00:18<00:02,  2.03s/it]
Profiling iterations: 100%|██████████| 10/10 [00:20<00:00,  2.03s/it]
Profiling iterations: 100%|██████████| 10/10 [00:20<00:00,  2.03s/it]
Avg latency: 2.0320536703802645 seconds
10% percentile latency: 2.028239606320858 seconds
25% percentile latency: 2.0297196919564158 seconds
50% percentile latency: 2.031953497324139 seconds
75% percentile latency: 2.033514458918944 seconds
90% percentile latency: 2.0350274978205563 seconds
99% percentile latency: 2.0399539225175976 seconds
Optim
Batch-1 Input len-128 Output len-128
INFO 12-18 14:19:16 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:19:16 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:19:16 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:19:16 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:19:18 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:19:21 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 14:19:22 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 14:19:22 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 14:19:22 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 14:19:22 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 14:19:22 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 14:19:22 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 14:19:39 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 12-18 14:19:39 [config.py:1472] Using max model len 32768
WARNING 12-18 14:19:39 [config.py:960] ascend quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 12-18 14:19:39 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=4096.
WARNING 12-18 14:19:39 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 14:19:39 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 14:19:39 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 14:19:39 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-18 14:19:41 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 14:19:49 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:19:49 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:19:49 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:19:49 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:19:50 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:19:54 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
INFO 12-18 14:19:54 [core.py:526] Waiting for init message from front-end.
WARNING 12-18 14:19:54 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 14:19:54 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 14:19:54 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 14:19:54 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 14:19:54 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 14:19:54 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 14:19:54 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/run/models/openPangu-7B-a8w8', speculative_config=None, tokenizer='/run/models/openPangu-7B-a8w8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/run/models/openPangu-7B-a8w8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-18 14:20:07 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:20:07 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:20:07 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:20:07 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:20:08 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:20:16 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-18 14:20:16 [model_runner_v1.py:1746] Starting to load model /run/models/openPangu-7B-a8w8...
INFO 12-18 14:20:17 [quantizer.py:89] Using the vLLM Ascend Quantizer version now!

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.76s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.76s/it]

INFO 12-18 14:20:20 [default_loader.py:272] Loading weights took 2.63 seconds
INFO 12-18 14:20:22 [model_runner_v1.py:1778] Loading model weights took 10.3344 GB
INFO 12-18 14:20:35 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/393c0866bd/rank_0_0/backbone for vLLM's torch.compile
INFO 12-18 14:20:35 [backends.py:519] Dynamo bytecode transform time: 11.18 s
INFO 12-18 14:20:38 [backends.py:193] Compiling a graph for general shape takes 1.98 s
INFO 12-18 14:20:47 [monitor.py:34] torch.compile takes 13.17 s in total
INFO 12-18 14:20:49 [worker_v1.py:181] Available memory: 46443598540, total memory: 65464696832
INFO 12-18 14:20:49 [kv_cache_utils.py:716] GPU KV cache size: 333,440 tokens
INFO 12-18 14:20:49 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 10.18x
INFO 12-18 14:21:58 [model_runner_v1.py:2074] Graph capturing finished in 69 secs, took 0.28 GiB
INFO 12-18 14:21:58 [core.py:172] init engine (profile, create kv cache, warmup model) took 95.72 seconds
WARNING 12-18 14:21:59 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-18 14:21:59 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 14:21:59 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 14:21:59 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 14:21:59 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...

Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]
Warmup iterations:  20%|██        | 1/5 [00:02<00:08,  2.16s/it]
Warmup iterations:  40%|████      | 2/5 [00:04<00:06,  2.12s/it]
Warmup iterations:  60%|██████    | 3/5 [00:06<00:04,  2.11s/it]
Warmup iterations:  80%|████████  | 4/5 [00:08<00:02,  2.10s/it]
Warmup iterations: 100%|██████████| 5/5 [00:10<00:00,  2.10s/it]
Warmup iterations: 100%|██████████| 5/5 [00:10<00:00,  2.11s/it]

Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]
Profiling iterations:  10%|█         | 1/10 [00:02<00:18,  2.10s/it]
Profiling iterations:  20%|██        | 2/10 [00:04<00:16,  2.09s/it]
Profiling iterations:  30%|███       | 3/10 [00:06<00:14,  2.09s/it]
Profiling iterations:  40%|████      | 4/10 [00:08<00:12,  2.09s/it]
Profiling iterations:  50%|█████     | 5/10 [00:10<00:10,  2.09s/it]
Profiling iterations:  60%|██████    | 6/10 [00:12<00:08,  2.09s/it]
Profiling iterations:  70%|███████   | 7/10 [00:14<00:06,  2.09s/it]
Profiling iterations:  80%|████████  | 8/10 [00:16<00:04,  2.09s/it]
Profiling iterations:  90%|█████████ | 9/10 [00:18<00:02,  2.09s/it]
Profiling iterations: 100%|██████████| 10/10 [00:20<00:00,  2.09s/it]
Profiling iterations: 100%|██████████| 10/10 [00:20<00:00,  2.09s/it]
Avg latency: 2.0886493323370816 seconds
10% percentile latency: 2.085324198193848 seconds
25% percentile latency: 2.0865217389073223 seconds
50% percentile latency: 2.0878095850348473 seconds
75% percentile latency: 2.0907172148581594 seconds
90% percentile latency: 2.091544743813574 seconds
99% percentile latency: 2.0947425402887165 seconds
Optim
Batch-1 Input len-512 Output len-128
INFO 12-18 14:22:49 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:22:49 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:22:49 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:22:49 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:22:50 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:22:54 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 14:22:54 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 14:22:54 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 14:22:54 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 14:22:54 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 14:22:54 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 14:22:54 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 14:23:11 [config.py:841] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 12-18 14:23:11 [config.py:1472] Using max model len 32768
WARNING 12-18 14:23:11 [config.py:960] ascend quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 12-18 14:23:11 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=4096.
WARNING 12-18 14:23:11 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 14:23:11 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 14:23:11 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 14:23:11 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-18 14:23:12 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 14:23:20 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:23:20 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:23:20 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:23:20 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:23:21 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:23:25 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
INFO 12-18 14:23:25 [core.py:526] Waiting for init message from front-end.
WARNING 12-18 14:23:26 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 14:23:26 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 14:23:26 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 14:23:26 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 14:23:26 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 14:23:26 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 14:23:26 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/run/models/openPangu-7B-a8w8', speculative_config=None, tokenizer='/run/models/openPangu-7B-a8w8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/run/models/openPangu-7B-a8w8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-18 14:23:38 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:23:38 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:23:38 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:23:38 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:23:40 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:23:46 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-18 14:23:46 [model_runner_v1.py:1746] Starting to load model /run/models/openPangu-7B-a8w8...
INFO 12-18 14:23:47 [quantizer.py:89] Using the vLLM Ascend Quantizer version now!

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.65s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.65s/it]

INFO 12-18 14:23:50 [default_loader.py:272] Loading weights took 2.39 seconds
INFO 12-18 14:23:52 [model_runner_v1.py:1778] Loading model weights took 10.3344 GB
INFO 12-18 14:24:05 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/393c0866bd/rank_0_0/backbone for vLLM's torch.compile
INFO 12-18 14:24:05 [backends.py:519] Dynamo bytecode transform time: 11.04 s
INFO 12-18 14:24:08 [backends.py:193] Compiling a graph for general shape takes 1.96 s
INFO 12-18 14:24:18 [monitor.py:34] torch.compile takes 13.00 s in total
INFO 12-18 14:24:19 [worker_v1.py:181] Available memory: 46442828492, total memory: 65464696832
INFO 12-18 14:24:19 [kv_cache_utils.py:716] GPU KV cache size: 333,440 tokens
INFO 12-18 14:24:19 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 10.18x
INFO 12-18 14:25:32 [model_runner_v1.py:2074] Graph capturing finished in 72 secs, took 0.28 GiB
INFO 12-18 14:25:32 [core.py:172] init engine (profile, create kv cache, warmup model) took 99.63 seconds
WARNING 12-18 14:25:33 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-18 14:25:33 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 14:25:33 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 14:25:33 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 14:25:33 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...

Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]
Warmup iterations:  20%|██        | 1/5 [00:02<00:08,  2.09s/it]
Warmup iterations:  40%|████      | 2/5 [00:04<00:06,  2.06s/it]
Warmup iterations:  60%|██████    | 3/5 [00:06<00:04,  2.04s/it]
Warmup iterations:  80%|████████  | 4/5 [00:08<00:02,  2.04s/it]
Warmup iterations: 100%|██████████| 5/5 [00:10<00:00,  2.04s/it]
Warmup iterations: 100%|██████████| 5/5 [00:10<00:00,  2.04s/it]

Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]
Profiling iterations:  10%|█         | 1/10 [00:02<00:18,  2.03s/it]
Profiling iterations:  20%|██        | 2/10 [00:04<00:16,  2.03s/it]
Profiling iterations:  30%|███       | 3/10 [00:06<00:14,  2.03s/it]
Profiling iterations:  40%|████      | 4/10 [00:08<00:12,  2.03s/it]
Profiling iterations:  50%|█████     | 5/10 [00:10<00:10,  2.03s/it]
Profiling iterations:  60%|██████    | 6/10 [00:12<00:08,  2.03s/it]
Profiling iterations:  70%|███████   | 7/10 [00:14<00:06,  2.03s/it]
Profiling iterations:  80%|████████  | 8/10 [00:16<00:04,  2.03s/it]
Profiling iterations:  90%|█████████ | 9/10 [00:18<00:02,  2.04s/it]
Profiling iterations: 100%|██████████| 10/10 [00:20<00:00,  2.04s/it]
Profiling iterations: 100%|██████████| 10/10 [00:20<00:00,  2.04s/it]
Avg latency: 2.0353746308013796 seconds
10% percentile latency: 2.03259809454903 seconds
25% percentile latency: 2.03322631213814 seconds
50% percentile latency: 2.0342956646345556 seconds
75% percentile latency: 2.0359819564037025 seconds
90% percentile latency: 2.0400510860607026 seconds
99% percentile latency: 2.0433983687497674 seconds
Optim
Batch-1 Input len-2048 Output len-128
INFO 12-18 14:26:22 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:26:22 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:26:22 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:26:22 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:26:23 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:26:27 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 14:26:27 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 14:26:27 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 14:26:27 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 14:26:27 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 14:26:27 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 14:26:27 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 14:26:44 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-18 14:26:44 [config.py:1472] Using max model len 32768
WARNING 12-18 14:26:44 [config.py:960] ascend quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 12-18 14:26:44 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=4096.
WARNING 12-18 14:26:44 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 14:26:44 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 14:26:44 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 14:26:44 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-18 14:26:45 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 14:26:53 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:26:53 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:26:53 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:26:53 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:26:55 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:26:58 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
INFO 12-18 14:26:59 [core.py:526] Waiting for init message from front-end.
WARNING 12-18 14:26:59 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 14:26:59 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 14:26:59 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 14:26:59 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 14:26:59 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 14:26:59 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 14:26:59 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/run/models/openPangu-7B-a8w8', speculative_config=None, tokenizer='/run/models/openPangu-7B-a8w8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/run/models/openPangu-7B-a8w8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-18 14:27:12 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:27:12 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:27:12 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:27:12 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:27:13 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:27:19 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-18 14:27:19 [model_runner_v1.py:1746] Starting to load model /run/models/openPangu-7B-a8w8...
INFO 12-18 14:27:20 [quantizer.py:89] Using the vLLM Ascend Quantizer version now!

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.41s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.41s/it]

INFO 12-18 14:27:22 [default_loader.py:272] Loading weights took 1.92 seconds
INFO 12-18 14:27:23 [model_runner_v1.py:1778] Loading model weights took 10.3344 GB
INFO 12-18 14:27:35 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/393c0866bd/rank_0_0/backbone for vLLM's torch.compile
INFO 12-18 14:27:35 [backends.py:519] Dynamo bytecode transform time: 9.97 s
INFO 12-18 14:27:37 [backends.py:193] Compiling a graph for general shape takes 1.80 s
INFO 12-18 14:27:46 [monitor.py:34] torch.compile takes 11.77 s in total
INFO 12-18 14:27:47 [worker_v1.py:181] Available memory: 46443516620, total memory: 65464696832
INFO 12-18 14:27:47 [kv_cache_utils.py:716] GPU KV cache size: 333,440 tokens
INFO 12-18 14:27:47 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 10.18x
INFO 12-18 14:28:28 [model_runner_v1.py:2074] Graph capturing finished in 40 secs, took 0.28 GiB
INFO 12-18 14:28:28 [core.py:172] init engine (profile, create kv cache, warmup model) took 64.22 seconds
WARNING 12-18 14:28:29 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-18 14:28:29 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 14:28:29 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 14:28:29 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 14:28:29 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...

Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]
Warmup iterations:  20%|██        | 1/5 [00:02<00:08,  2.15s/it]
Warmup iterations:  40%|████      | 2/5 [00:04<00:06,  2.13s/it]
Warmup iterations:  60%|██████    | 3/5 [00:06<00:04,  2.12s/it]
Warmup iterations:  80%|████████  | 4/5 [00:08<00:02,  2.12s/it]
Warmup iterations: 100%|██████████| 5/5 [00:10<00:00,  2.12s/it]
Warmup iterations: 100%|██████████| 5/5 [00:10<00:00,  2.12s/it]

Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]
Profiling iterations:  10%|█         | 1/10 [00:02<00:19,  2.12s/it]
Profiling iterations:  20%|██        | 2/10 [00:04<00:16,  2.12s/it]
Profiling iterations:  30%|███       | 3/10 [00:06<00:14,  2.12s/it]
Profiling iterations:  40%|████      | 4/10 [00:08<00:12,  2.12s/it]
Profiling iterations:  50%|█████     | 5/10 [00:10<00:10,  2.12s/it]
Profiling iterations:  60%|██████    | 6/10 [00:12<00:08,  2.11s/it]
Profiling iterations:  70%|███████   | 7/10 [00:14<00:06,  2.11s/it]
Profiling iterations:  80%|████████  | 8/10 [00:16<00:04,  2.11s/it]
Profiling iterations:  90%|█████████ | 9/10 [00:19<00:02,  2.11s/it]
Profiling iterations: 100%|██████████| 10/10 [00:21<00:00,  2.11s/it]
Profiling iterations: 100%|██████████| 10/10 [00:21<00:00,  2.11s/it]
Avg latency: 2.114170326385647 seconds
10% percentile latency: 2.11221291013062 seconds
25% percentile latency: 2.1126898631919175 seconds
50% percentile latency: 2.1145416367799044 seconds
75% percentile latency: 2.1155299618840218 seconds
90% percentile latency: 2.116091356985271 seconds
99% percentile latency: 2.1169478957913817 seconds
Optim
Batch-1 Input len-8192 Output len-128
INFO 12-18 14:29:19 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:29:19 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:29:19 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:29:19 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:29:20 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:29:24 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-18 14:29:25 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 14:29:25 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 14:29:25 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 14:29:25 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 14:29:25 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 14:29:25 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 14:29:41 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 12-18 14:29:41 [config.py:1472] Using max model len 32768
WARNING 12-18 14:29:41 [config.py:960] ascend quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 12-18 14:29:41 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=4096.
WARNING 12-18 14:29:41 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 14:29:41 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 14:29:41 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 14:29:41 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-18 14:29:42 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-18 14:29:50 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:29:50 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:29:50 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:29:50 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:29:52 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:29:55 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
INFO 12-18 14:29:56 [core.py:526] Waiting for init message from front-end.
WARNING 12-18 14:29:56 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-18 14:29:56 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-18 14:29:56 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-18 14:29:56 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-18 14:29:56 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-18 14:29:56 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-18 14:29:56 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/run/models/openPangu-7B-a8w8', speculative_config=None, tokenizer='/run/models/openPangu-7B-a8w8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=ascend, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/run/models/openPangu-7B-a8w8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-18 14:30:09 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-18 14:30:09 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-18 14:30:09 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-18 14:30:09 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-18 14:30:10 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-18 14:30:16 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-18 14:30:16 [model_runner_v1.py:1746] Starting to load model /run/models/openPangu-7B-a8w8...
INFO 12-18 14:30:17 [quantizer.py:89] Using the vLLM Ascend Quantizer version now!

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.69s/it]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.69s/it]

INFO 12-18 14:30:20 [default_loader.py:272] Loading weights took 2.55 seconds
INFO 12-18 14:30:21 [model_runner_v1.py:1778] Loading model weights took 10.3344 GB
INFO 12-18 14:30:32 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/393c0866bd/rank_0_0/backbone for vLLM's torch.compile
INFO 12-18 14:30:32 [backends.py:519] Dynamo bytecode transform time: 10.34 s
INFO 12-18 14:30:35 [backends.py:193] Compiling a graph for general shape takes 1.87 s
INFO 12-18 14:30:44 [monitor.py:34] torch.compile takes 12.21 s in total
INFO 12-18 14:30:46 [worker_v1.py:181] Available memory: 46443397836, total memory: 65464696832
INFO 12-18 14:30:46 [kv_cache_utils.py:716] GPU KV cache size: 333,440 tokens
INFO 12-18 14:30:46 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 10.18x
INFO 12-18 14:32:02 [model_runner_v1.py:2074] Graph capturing finished in 77 secs, took 0.28 GiB
INFO 12-18 14:32:02 [core.py:172] init engine (profile, create kv cache, warmup model) took 101.77 seconds
WARNING 12-18 14:32:03 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-18 14:32:04 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-18 14:32:04 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-18 14:32:04 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-18 14:32:04 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...

Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]
Warmup iterations:  20%|██        | 1/5 [00:02<00:11,  2.80s/it]
Warmup iterations:  40%|████      | 2/5 [00:05<00:08,  2.78s/it]
Warmup iterations:  60%|██████    | 3/5 [00:08<00:05,  2.78s/it]
Warmup iterations:  80%|████████  | 4/5 [00:11<00:02,  2.78s/it]
Warmup iterations: 100%|██████████| 5/5 [00:13<00:00,  2.78s/it]
Warmup iterations: 100%|██████████| 5/5 [00:13<00:00,  2.78s/it]

Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]
Profiling iterations:  10%|█         | 1/10 [00:02<00:25,  2.78s/it]
Profiling iterations:  20%|██        | 2/10 [00:05<00:22,  2.78s/it]
Profiling iterations:  30%|███       | 3/10 [00:08<00:19,  2.77s/it]
Profiling iterations:  40%|████      | 4/10 [00:11<00:16,  2.77s/it]
Profiling iterations:  50%|█████     | 5/10 [00:13<00:13,  2.75s/it]
Profiling iterations:  60%|██████    | 6/10 [00:16<00:10,  2.74s/it]
Profiling iterations:  70%|███████   | 7/10 [00:19<00:08,  2.74s/it]
Profiling iterations:  80%|████████  | 8/10 [00:22<00:05,  2.75s/it]
Profiling iterations:  90%|█████████ | 9/10 [00:24<00:02,  2.75s/it]
Profiling iterations: 100%|██████████| 10/10 [00:27<00:00,  2.74s/it]
Profiling iterations: 100%|██████████| 10/10 [00:27<00:00,  2.75s/it]
Avg latency: 2.7520739076659084 seconds
10% percentile latency: 2.720813524257392 seconds
25% percentile latency: 2.731560856802389 seconds
50% percentile latency: 2.7510036644525826 seconds
75% percentile latency: 2.7766330854501575 seconds
90% percentile latency: 2.779591010697186 seconds
99% percentile latency: 2.7828548870794476 seconds
