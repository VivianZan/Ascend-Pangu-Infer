Batch-8 Input len-32 Output len-32
INFO 12-17 19:50:00 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:50:00 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:50:00 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:50:00 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:50:01 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:50:05 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:50:06 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:50:06 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:50:06 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:50:06 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:50:06 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:50:06 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:50:22 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 12-17 19:50:22 [config.py:1472] Using max model len 32768
INFO 12-17 19:50:22 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 19:50:22 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:50:22 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:50:22 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:50:22 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:50:23 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:50:32 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:50:32 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:50:32 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:50:32 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:50:33 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:50:36 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:50:37 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:50:37 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:50:37 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:50:37 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:50:37 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:50:37 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:50:37 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:50:37 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:50:50 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:50:50 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:50:50 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:50:50 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:50:52 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:50:58 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:50:58 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.88it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.53it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.33it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.55it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.53it/s]

INFO 12-17 19:51:02 [default_loader.py:272] Loading weights took 2.68 seconds
INFO 12-17 19:51:04 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:51:14 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:51:14 [backends.py:519] Dynamo bytecode transform time: 9.29 s
INFO 12-17 19:51:18 [backends.py:193] Compiling a graph for general shape takes 2.58 s
INFO 12-17 19:51:26 [monitor.py:34] torch.compile takes 11.88 s in total
INFO 12-17 19:51:28 [worker_v1.py:181] Available memory: 41201177292, total memory: 65464696832
INFO 12-17 19:51:28 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 19:51:28 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 19:52:30 [model_runner_v1.py:2074] Graph capturing finished in 62 secs, took 0.36 GiB
INFO 12-17 19:52:30 [core.py:172] init engine (profile, create kv cache, warmup model) took 85.95 seconds
WARNING 12-17 19:52:31 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:52:31 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:52:31 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:52:31 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:52:31 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:03,  1.32it/s]Warmup iterations:  40%|████      | 2/5 [00:01<00:02,  1.42it/s]Warmup iterations:  60%|██████    | 3/5 [00:02<00:01,  1.46it/s]Warmup iterations:  80%|████████  | 4/5 [00:02<00:00,  1.48it/s]Warmup iterations: 100%|██████████| 5/5 [00:03<00:00,  1.49it/s]Warmup iterations: 100%|██████████| 5/5 [00:03<00:00,  1.47it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:05,  1.51it/s]Profiling iterations:  20%|██        | 2/10 [00:01<00:05,  1.51it/s]Profiling iterations:  30%|███       | 3/10 [00:01<00:04,  1.51it/s]Profiling iterations:  40%|████      | 4/10 [00:02<00:03,  1.51it/s]Profiling iterations:  50%|█████     | 5/10 [00:03<00:03,  1.51it/s]Profiling iterations:  60%|██████    | 6/10 [00:03<00:02,  1.52it/s]Profiling iterations:  70%|███████   | 7/10 [00:04<00:01,  1.51it/s]Profiling iterations:  80%|████████  | 8/10 [00:05<00:01,  1.51it/s]Profiling iterations:  90%|█████████ | 9/10 [00:05<00:00,  1.51it/s]Profiling iterations: 100%|██████████| 10/10 [00:06<00:00,  1.51it/s]Profiling iterations: 100%|██████████| 10/10 [00:06<00:00,  1.51it/s]
Avg latency: 0.6603575906250626 seconds
10% percentile latency: 0.6587155912537128 seconds
25% percentile latency: 0.6596569637767971 seconds
50% percentile latency: 0.6602627816610038 seconds
75% percentile latency: 0.660895633045584 seconds
90% percentile latency: 0.6622132441960276 seconds
99% percentile latency: 0.6623003605008125 seconds
Batch-8 Input len-32 Output len-64
INFO 12-17 19:52:59 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:52:59 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:52:59 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:52:59 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:53:01 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:53:05 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:53:05 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:53:05 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:53:05 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:53:05 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:53:05 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:53:05 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:53:22 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 12-17 19:53:22 [config.py:1472] Using max model len 32768
INFO 12-17 19:53:22 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 19:53:22 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:53:22 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:53:22 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:53:22 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:53:23 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:53:32 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:53:32 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:53:32 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:53:32 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:53:33 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:53:36 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:53:37 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:53:37 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:53:37 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:53:37 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:53:37 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:53:37 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:53:37 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:53:37 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:53:50 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:53:50 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:53:50 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:53:50 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:53:51 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:53:57 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:53:58 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.65it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.15it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.25it/s]

INFO 12-17 19:54:02 [default_loader.py:272] Loading weights took 3.34 seconds
INFO 12-17 19:54:04 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:54:14 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:54:14 [backends.py:519] Dynamo bytecode transform time: 9.22 s
INFO 12-17 19:54:17 [backends.py:193] Compiling a graph for general shape takes 2.57 s
INFO 12-17 19:54:26 [monitor.py:34] torch.compile takes 11.79 s in total
INFO 12-17 19:54:28 [worker_v1.py:181] Available memory: 41202344652, total memory: 65464696832
INFO 12-17 19:54:28 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 19:54:28 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 19:55:27 [model_runner_v1.py:2074] Graph capturing finished in 59 secs, took 0.36 GiB
INFO 12-17 19:55:27 [core.py:172] init engine (profile, create kv cache, warmup model) took 83.60 seconds
WARNING 12-17 19:55:29 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:55:29 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:55:29 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:55:29 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:55:29 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:05,  1.39s/it]Warmup iterations:  40%|████      | 2/5 [00:02<00:04,  1.34s/it]Warmup iterations:  60%|██████    | 3/5 [00:03<00:02,  1.32s/it]Warmup iterations:  80%|████████  | 4/5 [00:05<00:01,  1.31s/it]Warmup iterations: 100%|██████████| 5/5 [00:06<00:00,  1.31s/it]Warmup iterations: 100%|██████████| 5/5 [00:06<00:00,  1.32s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:11,  1.30s/it]Profiling iterations:  20%|██        | 2/10 [00:02<00:10,  1.30s/it]Profiling iterations:  30%|███       | 3/10 [00:03<00:09,  1.30s/it]Profiling iterations:  40%|████      | 4/10 [00:05<00:07,  1.30s/it]Profiling iterations:  50%|█████     | 5/10 [00:06<00:06,  1.30s/it]Profiling iterations:  60%|██████    | 6/10 [00:07<00:05,  1.29s/it]Profiling iterations:  70%|███████   | 7/10 [00:09<00:03,  1.30s/it]Profiling iterations:  80%|████████  | 8/10 [00:10<00:02,  1.30s/it]Profiling iterations:  90%|█████████ | 9/10 [00:11<00:01,  1.30s/it]Profiling iterations: 100%|██████████| 10/10 [00:12<00:00,  1.30s/it]Profiling iterations: 100%|██████████| 10/10 [00:12<00:00,  1.30s/it]
Avg latency: 1.2958969490136951 seconds
10% percentile latency: 1.2931030383799225 seconds
25% percentile latency: 1.2946062568807974 seconds
50% percentile latency: 1.2961378679610789 seconds
75% percentile latency: 1.2972526126541197 seconds
90% percentile latency: 1.2982211778406054 seconds
99% percentile latency: 1.3001379368314518 seconds
Batch-8 Input len-32 Output len-128
INFO 12-17 19:56:05 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:56:05 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:56:05 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:56:05 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:56:07 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:56:10 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:56:11 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:56:11 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:56:11 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:56:11 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:56:11 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:56:11 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:56:27 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 12-17 19:56:27 [config.py:1472] Using max model len 32768
INFO 12-17 19:56:27 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 19:56:27 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:56:27 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:56:27 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:56:27 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:56:28 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:56:37 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:56:37 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:56:37 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:56:37 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:56:38 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:56:41 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:56:42 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:56:42 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:56:42 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:56:42 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:56:42 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:56:42 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:56:42 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:56:42 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:56:55 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:56:55 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:56:55 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:56:55 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:56:57 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:57:03 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:57:03 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.65it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.14it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.03it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.26it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.22it/s]

INFO 12-17 19:57:08 [default_loader.py:272] Loading weights took 3.44 seconds
INFO 12-17 19:57:10 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:57:20 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:57:20 [backends.py:519] Dynamo bytecode transform time: 9.38 s
INFO 12-17 19:57:23 [backends.py:193] Compiling a graph for general shape takes 2.61 s
INFO 12-17 19:57:33 [monitor.py:34] torch.compile takes 11.99 s in total
INFO 12-17 19:57:35 [worker_v1.py:181] Available memory: 41201701580, total memory: 65464696832
INFO 12-17 19:57:35 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 19:57:35 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 19:58:15 [model_runner_v1.py:2074] Graph capturing finished in 40 secs, took 0.36 GiB
INFO 12-17 19:58:15 [core.py:172] init engine (profile, create kv cache, warmup model) took 65.10 seconds
WARNING 12-17 19:58:16 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:58:16 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:58:16 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:58:16 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:58:16 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:02<00:10,  2.67s/it]Warmup iterations:  40%|████      | 2/5 [00:05<00:07,  2.61s/it]Warmup iterations:  60%|██████    | 3/5 [00:07<00:05,  2.60s/it]Warmup iterations:  80%|████████  | 4/5 [00:10<00:02,  2.59s/it]Warmup iterations: 100%|██████████| 5/5 [00:12<00:00,  2.58s/it]Warmup iterations: 100%|██████████| 5/5 [00:12<00:00,  2.59s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:02<00:23,  2.57s/it]Profiling iterations:  20%|██        | 2/10 [00:05<00:20,  2.57s/it]Profiling iterations:  30%|███       | 3/10 [00:07<00:17,  2.57s/it]Profiling iterations:  40%|████      | 4/10 [00:10<00:15,  2.57s/it]Profiling iterations:  50%|█████     | 5/10 [00:12<00:12,  2.58s/it]Profiling iterations:  60%|██████    | 6/10 [00:15<00:10,  2.58s/it]Profiling iterations:  70%|███████   | 7/10 [00:18<00:07,  2.60s/it]Profiling iterations:  80%|████████  | 8/10 [00:20<00:05,  2.60s/it]Profiling iterations:  90%|█████████ | 9/10 [00:23<00:02,  2.61s/it]Profiling iterations: 100%|██████████| 10/10 [00:25<00:00,  2.62s/it]Profiling iterations: 100%|██████████| 10/10 [00:25<00:00,  2.60s/it]
Avg latency: 2.596192301763222 seconds
10% percentile latency: 2.5704754609148948 seconds
25% percentile latency: 2.5738277783384547 seconds
50% percentile latency: 2.5869827771093696 seconds
75% percentile latency: 2.6247585270320997 seconds
90% percentile latency: 2.626663459325209 seconds
99% percentile latency: 2.629230764727108 seconds
Batch-8 Input len-32 Output len-256
INFO 12-17 19:59:11 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:59:11 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:59:11 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:59:11 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:59:13 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:59:16 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:59:17 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:59:17 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:59:17 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:59:17 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:59:17 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:59:17 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:59:33 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-17 19:59:33 [config.py:1472] Using max model len 32768
INFO 12-17 19:59:33 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 19:59:33 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:59:33 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:59:33 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:59:33 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:59:34 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:59:43 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:59:43 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:59:43 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:59:43 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:59:44 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:59:47 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:59:48 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:59:48 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:59:48 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:59:48 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:59:48 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:59:48 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:59:48 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:59:48 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:00:01 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:00:01 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:00:01 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:00:01 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:00:03 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:00:09 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:00:09 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.15it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.61it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.30it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.49it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.50it/s]

INFO 12-17 20:00:13 [default_loader.py:272] Loading weights took 2.77 seconds
INFO 12-17 20:00:15 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:00:26 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:00:26 [backends.py:519] Dynamo bytecode transform time: 9.34 s
INFO 12-17 20:00:29 [backends.py:193] Compiling a graph for general shape takes 2.59 s
INFO 12-17 20:00:38 [monitor.py:34] torch.compile takes 11.93 s in total
INFO 12-17 20:00:40 [worker_v1.py:181] Available memory: 41201046220, total memory: 65464696832
INFO 12-17 20:00:40 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:00:40 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:01:45 [model_runner_v1.py:2074] Graph capturing finished in 65 secs, took 0.36 GiB
INFO 12-17 20:01:45 [core.py:172] init engine (profile, create kv cache, warmup model) took 89.92 seconds
WARNING 12-17 20:01:46 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:01:47 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:01:47 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:01:47 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:01:47 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:05<00:20,  5.21s/it]Warmup iterations:  40%|████      | 2/5 [00:10<00:15,  5.15s/it]Warmup iterations:  60%|██████    | 3/5 [00:15<00:10,  5.13s/it]Warmup iterations:  80%|████████  | 4/5 [00:20<00:05,  5.12s/it]Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.11s/it]Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.13s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:05<00:46,  5.11s/it]Profiling iterations:  20%|██        | 2/10 [00:10<00:41,  5.16s/it]Profiling iterations:  30%|███       | 3/10 [00:15<00:36,  5.18s/it]Profiling iterations:  40%|████      | 4/10 [00:20<00:31,  5.19s/it]Profiling iterations:  50%|█████     | 5/10 [00:25<00:25,  5.19s/it]Profiling iterations:  60%|██████    | 6/10 [00:31<00:20,  5.19s/it]Profiling iterations:  70%|███████   | 7/10 [00:36<00:15,  5.16s/it]Profiling iterations:  80%|████████  | 8/10 [00:41<00:10,  5.15s/it]Profiling iterations:  90%|█████████ | 9/10 [00:46<00:05,  5.17s/it]Profiling iterations: 100%|██████████| 10/10 [00:51<00:00,  5.18s/it]Profiling iterations: 100%|██████████| 10/10 [00:51<00:00,  5.17s/it]
Avg latency: 5.1740955804474655 seconds
10% percentile latency: 5.111038187099621 seconds
25% percentile latency: 5.150146261556074 seconds
50% percentile latency: 5.194691619137302 seconds
75% percentile latency: 5.201104547595605 seconds
90% percentile latency: 5.207861642260104 seconds
99% percentile latency: 5.208205529199913 seconds
Batch-8 Input len-32 Output len-512
INFO 12-17 20:03:22 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:03:22 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:03:22 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:03:22 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:03:23 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:03:27 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:03:28 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:03:28 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:03:28 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:03:28 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:03:28 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:03:28 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:03:44 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-17 20:03:44 [config.py:1472] Using max model len 32768
INFO 12-17 20:03:44 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:03:44 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:03:44 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:03:44 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:03:44 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:03:45 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:03:54 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:03:54 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:03:54 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:03:54 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:03:55 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:03:58 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:03:59 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:03:59 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:03:59 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:03:59 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:03:59 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:03:59 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:03:59 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:03:59 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:04:12 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:04:12 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:04:12 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:04:12 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:04:13 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:04:20 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:04:20 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.61it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.12it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.05it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.32it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.25it/s]

INFO 12-17 20:04:25 [default_loader.py:272] Loading weights took 3.35 seconds
INFO 12-17 20:04:27 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:04:37 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:04:37 [backends.py:519] Dynamo bytecode transform time: 9.30 s
INFO 12-17 20:04:40 [backends.py:193] Compiling a graph for general shape takes 2.59 s
INFO 12-17 20:04:49 [monitor.py:34] torch.compile takes 11.89 s in total
INFO 12-17 20:04:51 [worker_v1.py:181] Available memory: 41201382092, total memory: 65464696832
INFO 12-17 20:04:51 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:04:51 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:05:54 [model_runner_v1.py:2074] Graph capturing finished in 64 secs, took 0.36 GiB
INFO 12-17 20:05:54 [core.py:172] init engine (profile, create kv cache, warmup model) took 87.91 seconds
WARNING 12-17 20:05:55 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:05:56 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:05:56 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:05:56 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:05:56 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:10<00:40, 10.22s/it]Warmup iterations:  40%|████      | 2/5 [00:20<00:30, 10.18s/it]Warmup iterations:  60%|██████    | 3/5 [00:30<00:20, 10.21s/it]Warmup iterations:  80%|████████  | 4/5 [00:40<00:10, 10.23s/it]Warmup iterations: 100%|██████████| 5/5 [00:51<00:00, 10.24s/it]Warmup iterations: 100%|██████████| 5/5 [00:51<00:00, 10.23s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:10<01:32, 10.26s/it]Profiling iterations:  20%|██        | 2/10 [00:20<01:22, 10.26s/it]Profiling iterations:  30%|███       | 3/10 [00:30<01:11, 10.27s/it]Profiling iterations:  40%|████      | 4/10 [00:41<01:01, 10.26s/it]Profiling iterations:  50%|█████     | 5/10 [00:51<00:51, 10.27s/it]Profiling iterations:  60%|██████    | 6/10 [01:01<00:41, 10.26s/it]Profiling iterations:  70%|███████   | 7/10 [01:11<00:30, 10.25s/it]Profiling iterations:  80%|████████  | 8/10 [01:22<00:20, 10.24s/it]Profiling iterations:  90%|█████████ | 9/10 [01:31<00:10, 10.14s/it]Profiling iterations: 100%|██████████| 10/10 [01:42<00:00, 10.14s/it]Profiling iterations: 100%|██████████| 10/10 [01:42<00:00, 10.21s/it]
Avg latency: 10.209344730479643 seconds
10% percentile latency: 10.137582890316844 seconds
25% percentile latency: 10.222080605570227 seconds
50% percentile latency: 10.247689583338797 seconds
75% percentile latency: 10.265304740169086 seconds
90% percentile latency: 10.26841334765777 seconds
99% percentile latency: 10.282004677830264 seconds
Batch-8 Input len-32 Output len-1024
INFO 12-17 20:08:47 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:08:47 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:08:47 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:08:47 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:08:48 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:08:52 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:08:52 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:08:52 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:08:52 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:08:52 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:08:52 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:08:52 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:09:09 [config.py:841] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 12-17 20:09:09 [config.py:1472] Using max model len 32768
INFO 12-17 20:09:09 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:09:09 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:09:09 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:09:09 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:09:09 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:09:10 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:09:18 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:09:18 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:09:18 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:09:18 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:09:20 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:09:22 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:09:24 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:09:24 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:09:24 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:09:24 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:09:24 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:09:24 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:09:24 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:09:24 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:09:37 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:09:37 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:09:37 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:09:37 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:09:38 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:09:44 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:09:44 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.87it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.54it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.34it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.55it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.53it/s]

INFO 12-17 20:09:48 [default_loader.py:272] Loading weights took 2.68 seconds
INFO 12-17 20:09:50 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:10:00 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:10:00 [backends.py:519] Dynamo bytecode transform time: 9.26 s
INFO 12-17 20:10:04 [backends.py:193] Compiling a graph for general shape takes 2.58 s
INFO 12-17 20:10:12 [monitor.py:34] torch.compile takes 11.84 s in total
INFO 12-17 20:10:14 [worker_v1.py:181] Available memory: 41201234636, total memory: 65464696832
INFO 12-17 20:10:14 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:10:14 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:11:12 [model_runner_v1.py:2074] Graph capturing finished in 58 secs, took 0.36 GiB
INFO 12-17 20:11:12 [core.py:172] init engine (profile, create kv cache, warmup model) took 81.63 seconds
WARNING 12-17 20:11:13 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:11:13 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:11:13 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:11:13 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:11:13 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:20<01:23, 20.75s/it]Warmup iterations:  40%|████      | 2/5 [00:41<01:02, 20.69s/it]Warmup iterations:  60%|██████    | 3/5 [01:02<00:41, 20.67s/it]Warmup iterations:  80%|████████  | 4/5 [01:22<00:20, 20.67s/it]Warmup iterations: 100%|██████████| 5/5 [01:43<00:00, 20.66s/it]Warmup iterations: 100%|██████████| 5/5 [01:43<00:00, 20.67s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:20<03:05, 20.63s/it]Profiling iterations:  20%|██        | 2/10 [00:41<02:45, 20.68s/it]Profiling iterations:  30%|███       | 3/10 [01:02<02:24, 20.68s/it]Profiling iterations:  40%|████      | 4/10 [01:22<02:03, 20.65s/it]Profiling iterations:  50%|█████     | 5/10 [01:43<01:43, 20.63s/it]Profiling iterations:  60%|██████    | 6/10 [02:03<01:22, 20.63s/it]Profiling iterations:  70%|███████   | 7/10 [02:24<01:01, 20.62s/it]Profiling iterations:  80%|████████  | 8/10 [02:45<00:41, 20.62s/it]Profiling iterations:  90%|█████████ | 9/10 [03:05<00:20, 20.65s/it]Profiling iterations: 100%|██████████| 10/10 [03:26<00:00, 20.65s/it]Profiling iterations: 100%|██████████| 10/10 [03:26<00:00, 20.64s/it]
Avg latency: 20.644310185313223 seconds
10% percentile latency: 20.605970037402585 seconds
25% percentile latency: 20.611176156671718 seconds
50% percentile latency: 20.626637891633436 seconds
75% percentile latency: 20.682582818320952 seconds
90% percentile latency: 20.699329960113392 seconds
99% percentile latency: 20.705266556232235 seconds
Batch-8 Input len-64 Output len-32
INFO 12-17 20:16:39 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:16:39 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:16:39 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:16:39 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:16:40 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:16:44 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:16:44 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:16:44 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:16:44 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:16:44 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:16:44 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:16:44 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:17:01 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 12-17 20:17:01 [config.py:1472] Using max model len 32768
INFO 12-17 20:17:01 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:17:01 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:17:01 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:17:01 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:17:01 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:17:02 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:17:11 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:17:11 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:17:11 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:17:11 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:17:12 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:17:15 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:17:16 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:17:16 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:17:16 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:17:16 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:17:16 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:17:16 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:17:16 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:17:16 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:17:29 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:17:29 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:17:29 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:17:29 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:17:30 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:17:37 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:17:37 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.53it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.09it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.19it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.16it/s]

INFO 12-17 20:17:41 [default_loader.py:272] Loading weights took 3.62 seconds
INFO 12-17 20:17:42 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:17:52 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:17:52 [backends.py:519] Dynamo bytecode transform time: 9.45 s
INFO 12-17 20:17:55 [backends.py:193] Compiling a graph for general shape takes 2.53 s
INFO 12-17 20:18:04 [monitor.py:34] torch.compile takes 11.98 s in total
INFO 12-17 20:18:05 [worker_v1.py:181] Available memory: 41201091276, total memory: 65464696832
INFO 12-17 20:18:05 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:18:05 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:18:41 [model_runner_v1.py:2074] Graph capturing finished in 36 secs, took 0.36 GiB
INFO 12-17 20:18:41 [core.py:172] init engine (profile, create kv cache, warmup model) took 58.92 seconds
WARNING 12-17 20:18:42 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:18:43 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:18:43 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:18:43 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:18:43 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:03,  1.32it/s]Warmup iterations:  40%|████      | 2/5 [00:01<00:02,  1.42it/s]Warmup iterations:  60%|██████    | 3/5 [00:02<00:01,  1.45it/s]Warmup iterations:  80%|████████  | 4/5 [00:02<00:00,  1.46it/s]Warmup iterations: 100%|██████████| 5/5 [00:03<00:00,  1.47it/s]Warmup iterations: 100%|██████████| 5/5 [00:03<00:00,  1.45it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:06,  1.49it/s]Profiling iterations:  20%|██        | 2/10 [00:01<00:05,  1.49it/s]Profiling iterations:  30%|███       | 3/10 [00:02<00:04,  1.49it/s]Profiling iterations:  40%|████      | 4/10 [00:02<00:04,  1.49it/s]Profiling iterations:  50%|█████     | 5/10 [00:03<00:03,  1.49it/s]Profiling iterations:  60%|██████    | 6/10 [00:04<00:02,  1.49it/s]Profiling iterations:  70%|███████   | 7/10 [00:04<00:02,  1.49it/s]Profiling iterations:  80%|████████  | 8/10 [00:05<00:01,  1.49it/s]Profiling iterations:  90%|█████████ | 9/10 [00:06<00:00,  1.49it/s]Profiling iterations: 100%|██████████| 10/10 [00:06<00:00,  1.49it/s]Profiling iterations: 100%|██████████| 10/10 [00:06<00:00,  1.49it/s]
Avg latency: 0.6713390816003084 seconds
10% percentile latency: 0.6692077569663525 seconds
25% percentile latency: 0.6697755252243951 seconds
50% percentile latency: 0.6717438180930912 seconds
75% percentile latency: 0.6727192648686469 seconds
90% percentile latency: 0.6736138443928212 seconds
99% percentile latency: 0.6738321326626465 seconds
Batch-8 Input len-64 Output len-64
INFO 12-17 20:19:11 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:19:11 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:19:11 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:19:11 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:19:12 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:19:16 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:19:16 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:19:16 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:19:16 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:19:16 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:19:16 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:19:16 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:19:33 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-17 20:19:33 [config.py:1472] Using max model len 32768
INFO 12-17 20:19:33 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:19:33 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:19:33 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:19:33 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:19:33 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:19:34 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:19:42 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:19:42 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:19:42 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:19:42 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:19:44 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:19:47 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:19:48 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:19:48 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:19:48 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:19:48 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:19:48 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:19:48 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:19:48 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:19:48 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:20:01 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:20:01 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:20:01 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:20:01 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:20:03 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:20:09 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:20:09 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.65it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.16it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.24it/s]

INFO 12-17 20:20:14 [default_loader.py:272] Loading weights took 3.38 seconds
INFO 12-17 20:20:15 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:20:25 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:20:25 [backends.py:519] Dynamo bytecode transform time: 9.16 s
INFO 12-17 20:20:29 [backends.py:193] Compiling a graph for general shape takes 2.47 s
INFO 12-17 20:20:37 [monitor.py:34] torch.compile takes 11.63 s in total
INFO 12-17 20:20:39 [worker_v1.py:181] Available memory: 41200960204, total memory: 65464696832
INFO 12-17 20:20:39 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:20:39 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:21:37 [model_runner_v1.py:2074] Graph capturing finished in 58 secs, took 0.36 GiB
INFO 12-17 20:21:37 [core.py:172] init engine (profile, create kv cache, warmup model) took 81.14 seconds
WARNING 12-17 20:21:38 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:21:38 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:21:38 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:21:38 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:21:38 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:05,  1.40s/it]Warmup iterations:  40%|████      | 2/5 [00:02<00:04,  1.35s/it]Warmup iterations:  60%|██████    | 3/5 [00:04<00:02,  1.33s/it]Warmup iterations:  80%|████████  | 4/5 [00:05<00:01,  1.33s/it]Warmup iterations: 100%|██████████| 5/5 [00:06<00:00,  1.32s/it]Warmup iterations: 100%|██████████| 5/5 [00:06<00:00,  1.33s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:11,  1.31s/it]Profiling iterations:  20%|██        | 2/10 [00:02<00:10,  1.31s/it]Profiling iterations:  30%|███       | 3/10 [00:03<00:09,  1.31s/it]Profiling iterations:  40%|████      | 4/10 [00:05<00:07,  1.31s/it]Profiling iterations:  50%|█████     | 5/10 [00:06<00:06,  1.31s/it]Profiling iterations:  60%|██████    | 6/10 [00:07<00:05,  1.31s/it]Profiling iterations:  70%|███████   | 7/10 [00:09<00:03,  1.31s/it]Profiling iterations:  80%|████████  | 8/10 [00:10<00:02,  1.31s/it]Profiling iterations:  90%|█████████ | 9/10 [00:11<00:01,  1.31s/it]Profiling iterations: 100%|██████████| 10/10 [00:13<00:00,  1.31s/it]Profiling iterations: 100%|██████████| 10/10 [00:13<00:00,  1.31s/it]
Avg latency: 1.3114314183127136 seconds
10% percentile latency: 1.309636210044846 seconds
25% percentile latency: 1.3104793382808566 seconds
50% percentile latency: 1.3113098908215761 seconds
75% percentile latency: 1.312645228812471 seconds
90% percentile latency: 1.3138566731475294 seconds
99% percentile latency: 1.3142896762304008 seconds
Batch-8 Input len-64 Output len-128
INFO 12-17 20:22:14 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:22:14 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:22:14 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:22:14 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:22:16 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:22:20 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:22:20 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:22:20 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:22:20 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:22:20 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:22:20 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:22:20 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:22:36 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 12-17 20:22:36 [config.py:1472] Using max model len 32768
INFO 12-17 20:22:36 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:22:36 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:22:36 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:22:36 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:22:36 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:22:37 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:22:46 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:22:46 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:22:46 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:22:46 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:22:47 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:22:50 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:22:51 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:22:51 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:22:51 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:22:51 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:22:51 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:22:51 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:22:51 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:22:51 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:23:05 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:23:05 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:23:05 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:23:05 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:23:06 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:23:12 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:23:12 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.62it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.22it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.14it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.35it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.31it/s]

INFO 12-17 20:23:17 [default_loader.py:272] Loading weights took 3.11 seconds
INFO 12-17 20:23:19 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:23:29 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:23:29 [backends.py:519] Dynamo bytecode transform time: 9.29 s
INFO 12-17 20:23:32 [backends.py:193] Compiling a graph for general shape takes 2.61 s
INFO 12-17 20:23:41 [monitor.py:34] torch.compile takes 11.90 s in total
INFO 12-17 20:23:43 [worker_v1.py:181] Available memory: 41200997068, total memory: 65464696832
INFO 12-17 20:23:43 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:23:43 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:24:48 [model_runner_v1.py:2074] Graph capturing finished in 65 secs, took 0.36 GiB
INFO 12-17 20:24:48 [core.py:172] init engine (profile, create kv cache, warmup model) took 89.43 seconds
WARNING 12-17 20:24:49 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:24:50 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:24:50 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:24:50 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:24:50 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:02<00:10,  2.67s/it]Warmup iterations:  40%|████      | 2/5 [00:05<00:07,  2.61s/it]Warmup iterations:  60%|██████    | 3/5 [00:07<00:05,  2.59s/it]Warmup iterations:  80%|████████  | 4/5 [00:10<00:02,  2.58s/it]Warmup iterations: 100%|██████████| 5/5 [00:12<00:00,  2.58s/it]Warmup iterations: 100%|██████████| 5/5 [00:12<00:00,  2.59s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:02<00:23,  2.57s/it]Profiling iterations:  20%|██        | 2/10 [00:05<00:20,  2.57s/it]Profiling iterations:  30%|███       | 3/10 [00:07<00:18,  2.57s/it]Profiling iterations:  40%|████      | 4/10 [00:10<00:15,  2.57s/it]Profiling iterations:  50%|█████     | 5/10 [00:12<00:12,  2.57s/it]Profiling iterations:  60%|██████    | 6/10 [00:15<00:10,  2.57s/it]Profiling iterations:  70%|███████   | 7/10 [00:18<00:07,  2.57s/it]Profiling iterations:  80%|████████  | 8/10 [00:20<00:05,  2.57s/it]Profiling iterations:  90%|█████████ | 9/10 [00:23<00:02,  2.57s/it]Profiling iterations: 100%|██████████| 10/10 [00:25<00:00,  2.57s/it]Profiling iterations: 100%|██████████| 10/10 [00:25<00:00,  2.57s/it]
Avg latency: 2.5704502465669066 seconds
10% percentile latency: 2.565148942032829 seconds
25% percentile latency: 2.569744061329402 seconds
50% percentile latency: 2.5709373466670513 seconds
75% percentile latency: 2.5727789140073583 seconds
90% percentile latency: 2.573273359425366 seconds
99% percentile latency: 2.574240811178461 seconds
Batch-8 Input len-64 Output len-256
INFO 12-17 20:25:44 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:25:44 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:25:44 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:25:44 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:25:46 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:25:50 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:25:50 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:25:50 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:25:50 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:25:50 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:25:50 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:25:50 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:26:06 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 12-17 20:26:06 [config.py:1472] Using max model len 32768
INFO 12-17 20:26:06 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:26:06 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:26:06 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:26:06 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:26:06 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:26:07 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:26:16 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:26:16 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:26:16 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:26:16 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:26:17 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:26:20 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:26:21 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:26:21 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:26:21 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:26:21 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:26:21 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:26:21 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:26:21 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:26:21 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:26:34 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:26:34 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:26:34 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:26:34 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:26:36 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:26:42 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:26:42 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.57it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.12it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.02it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.23it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.19it/s]

INFO 12-17 20:26:47 [default_loader.py:272] Loading weights took 3.50 seconds
INFO 12-17 20:26:48 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:26:57 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:26:57 [backends.py:519] Dynamo bytecode transform time: 8.87 s
INFO 12-17 20:27:00 [backends.py:193] Compiling a graph for general shape takes 2.40 s
INFO 12-17 20:27:09 [monitor.py:34] torch.compile takes 11.27 s in total
INFO 12-17 20:27:10 [worker_v1.py:181] Available memory: 41201394380, total memory: 65464696832
INFO 12-17 20:27:10 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:27:10 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:27:54 [model_runner_v1.py:2074] Graph capturing finished in 44 secs, took 0.36 GiB
INFO 12-17 20:27:54 [core.py:172] init engine (profile, create kv cache, warmup model) took 66.17 seconds
WARNING 12-17 20:27:55 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:27:55 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:27:55 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:27:55 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:27:55 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:05<00:20,  5.18s/it]Warmup iterations:  40%|████      | 2/5 [00:10<00:15,  5.12s/it]Warmup iterations:  60%|██████    | 3/5 [00:15<00:10,  5.10s/it]Warmup iterations:  80%|████████  | 4/5 [00:20<00:05,  5.09s/it]Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.09s/it]Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.10s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:05<00:45,  5.07s/it]Profiling iterations:  20%|██        | 2/10 [00:10<00:40,  5.08s/it]Profiling iterations:  30%|███       | 3/10 [00:15<00:35,  5.08s/it]Profiling iterations:  40%|████      | 4/10 [00:20<00:30,  5.08s/it]Profiling iterations:  50%|█████     | 5/10 [00:25<00:25,  5.08s/it]Profiling iterations:  60%|██████    | 6/10 [00:30<00:20,  5.09s/it]Profiling iterations:  70%|███████   | 7/10 [00:35<00:15,  5.08s/it]Profiling iterations:  80%|████████  | 8/10 [00:40<00:10,  5.08s/it]Profiling iterations:  90%|█████████ | 9/10 [00:45<00:05,  5.09s/it]Profiling iterations: 100%|██████████| 10/10 [00:50<00:00,  5.09s/it]Profiling iterations: 100%|██████████| 10/10 [00:50<00:00,  5.08s/it]
Avg latency: 5.0831860568840055 seconds
10% percentile latency: 5.074430978717283 seconds
25% percentile latency: 5.078783049131744 seconds
50% percentile latency: 5.083781472640112 seconds
75% percentile latency: 5.086256712325849 seconds
90% percentile latency: 5.09002816719003 seconds
99% percentile latency: 5.094145932649262 seconds
Batch-8 Input len-64 Output len-512
INFO 12-17 20:29:30 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:29:30 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:29:30 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:29:30 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:29:31 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:29:35 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:29:35 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:29:35 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:29:35 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:29:35 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:29:35 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:29:35 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:29:52 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-17 20:29:52 [config.py:1472] Using max model len 32768
INFO 12-17 20:29:52 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:29:52 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:29:52 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:29:52 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:29:52 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:29:53 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:30:01 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:30:01 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:30:01 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:30:01 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:30:02 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:30:05 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:30:06 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:30:07 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:30:07 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:30:07 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:30:07 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:30:07 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:30:07 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:30:07 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:30:20 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:30:20 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:30:20 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:30:20 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:30:21 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:30:27 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:30:27 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.52it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.08it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.19it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.15it/s]

INFO 12-17 20:30:32 [default_loader.py:272] Loading weights took 3.63 seconds
INFO 12-17 20:30:33 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:30:43 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:30:43 [backends.py:519] Dynamo bytecode transform time: 9.45 s
INFO 12-17 20:30:46 [backends.py:193] Compiling a graph for general shape takes 2.57 s
INFO 12-17 20:30:55 [monitor.py:34] torch.compile takes 12.02 s in total
INFO 12-17 20:30:56 [worker_v1.py:181] Available memory: 41201918668, total memory: 65464696832
INFO 12-17 20:30:56 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:30:56 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:31:33 [model_runner_v1.py:2074] Graph capturing finished in 37 secs, took 0.36 GiB
INFO 12-17 20:31:33 [core.py:172] init engine (profile, create kv cache, warmup model) took 60.14 seconds
WARNING 12-17 20:31:34 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:31:34 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:31:34 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:31:34 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:31:34 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:10<00:41, 10.27s/it]Warmup iterations:  40%|████      | 2/5 [00:20<00:30, 10.19s/it]Warmup iterations:  60%|██████    | 3/5 [00:30<00:20, 10.20s/it]Warmup iterations:  80%|████████  | 4/5 [00:40<00:10, 10.18s/it]Warmup iterations: 100%|██████████| 5/5 [00:51<00:00, 10.21s/it]Warmup iterations: 100%|██████████| 5/5 [00:51<00:00, 10.21s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:10<01:33, 10.35s/it]Profiling iterations:  20%|██        | 2/10 [00:20<01:22, 10.35s/it]Profiling iterations:  30%|███       | 3/10 [00:31<01:12, 10.34s/it]Profiling iterations:  40%|████      | 4/10 [00:41<01:02, 10.34s/it]Profiling iterations:  50%|█████     | 5/10 [00:51<00:51, 10.35s/it]Profiling iterations:  60%|██████    | 6/10 [01:02<00:41, 10.35s/it]Profiling iterations:  70%|███████   | 7/10 [01:12<00:31, 10.33s/it]Profiling iterations:  80%|████████  | 8/10 [01:22<00:20, 10.29s/it]Profiling iterations:  90%|█████████ | 9/10 [01:32<00:10, 10.29s/it]Profiling iterations: 100%|██████████| 10/10 [01:43<00:00, 10.24s/it]Profiling iterations: 100%|██████████| 10/10 [01:43<00:00, 10.30s/it]
Avg latency: 10.300188745791093 seconds
10% percentile latency: 10.182614976540208 seconds
25% percentile latency: 10.29212162620388 seconds
50% percentile latency: 10.338428658433259 seconds
75% percentile latency: 10.348919483367354 seconds
90% percentile latency: 10.349316379381344 seconds
99% percentile latency: 10.349483015094883 seconds
Batch-8 Input len-64 Output len-1024
INFO 12-17 20:34:26 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:34:26 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:34:26 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:34:26 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:34:28 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:34:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:34:32 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:34:32 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:34:32 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:34:32 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:34:32 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:34:32 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:34:48 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 12-17 20:34:48 [config.py:1472] Using max model len 32768
INFO 12-17 20:34:48 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:34:48 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:34:48 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:34:48 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:34:48 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:34:49 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:34:58 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:34:58 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:34:58 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:34:58 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:34:59 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:35:02 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:35:03 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:35:03 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:35:03 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:35:03 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:35:03 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:35:03 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:35:03 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:35:03 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:35:16 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:35:16 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:35:16 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:35:16 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:35:18 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:35:24 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:35:24 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.14it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.60it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.30it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.47it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.49it/s]

INFO 12-17 20:35:28 [default_loader.py:272] Loading weights took 2.79 seconds
INFO 12-17 20:35:30 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:35:41 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:35:41 [backends.py:519] Dynamo bytecode transform time: 9.26 s
INFO 12-17 20:35:44 [backends.py:193] Compiling a graph for general shape takes 2.55 s
INFO 12-17 20:35:53 [monitor.py:34] torch.compile takes 11.81 s in total
INFO 12-17 20:35:55 [worker_v1.py:181] Available memory: 41201496780, total memory: 65464696832
INFO 12-17 20:35:55 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:35:55 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:37:00 [model_runner_v1.py:2074] Graph capturing finished in 65 secs, took 0.36 GiB
INFO 12-17 20:37:00 [core.py:172] init engine (profile, create kv cache, warmup model) took 89.59 seconds
WARNING 12-17 20:37:01 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:37:01 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:37:01 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:37:01 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:37:01 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:20<01:22, 20.67s/it]Warmup iterations:  40%|████      | 2/5 [00:41<01:01, 20.61s/it]Warmup iterations:  60%|██████    | 3/5 [01:01<00:41, 20.58s/it]Warmup iterations:  80%|████████  | 4/5 [01:22<00:20, 20.62s/it]Warmup iterations: 100%|██████████| 5/5 [01:43<00:00, 20.69s/it]Warmup iterations: 100%|██████████| 5/5 [01:43<00:00, 20.66s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:20<03:05, 20.57s/it]Profiling iterations:  20%|██        | 2/10 [00:41<02:44, 20.53s/it]Profiling iterations:  30%|███       | 3/10 [01:01<02:23, 20.52s/it]Profiling iterations:  40%|████      | 4/10 [01:22<02:03, 20.51s/it]Profiling iterations:  50%|█████     | 5/10 [01:42<01:42, 20.50s/it]Profiling iterations:  60%|██████    | 6/10 [02:03<01:21, 20.50s/it]Profiling iterations:  70%|███████   | 7/10 [02:23<01:01, 20.51s/it]Profiling iterations:  80%|████████  | 8/10 [02:44<00:41, 20.50s/it]Profiling iterations:  90%|█████████ | 9/10 [03:04<00:20, 20.51s/it]Profiling iterations: 100%|██████████| 10/10 [03:25<00:00, 20.51s/it]Profiling iterations: 100%|██████████| 10/10 [03:25<00:00, 20.51s/it]
Avg latency: 20.50936862686649 seconds
10% percentile latency: 20.491226848959922 seconds
25% percentile latency: 20.496476591099054 seconds
50% percentile latency: 20.502299974439666 seconds
75% percentile latency: 20.517053510877304 seconds
90% percentile latency: 20.523161360342055 seconds
99% percentile latency: 20.562914888681846 seconds
Batch-8 Input len-128 Output len-32
INFO 12-17 20:42:28 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:42:28 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:42:28 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:42:28 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:42:29 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:42:33 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:42:33 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:42:33 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:42:33 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:42:33 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:42:33 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:42:33 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:42:50 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 12-17 20:42:50 [config.py:1472] Using max model len 32768
INFO 12-17 20:42:50 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:42:50 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:42:50 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:42:50 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:42:50 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:42:51 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:42:59 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:42:59 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:42:59 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:43:00 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:43:01 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:43:03 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:43:05 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:43:05 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:43:05 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:43:05 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:43:05 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:43:05 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:43:05 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:43:05 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:43:18 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:43:18 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:43:18 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:43:18 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:43:19 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:43:26 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:43:26 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.58it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.19it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.10it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.33it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]

INFO 12-17 20:43:30 [default_loader.py:272] Loading weights took 3.22 seconds
INFO 12-17 20:43:31 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:43:40 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:43:40 [backends.py:519] Dynamo bytecode transform time: 8.93 s
INFO 12-17 20:43:44 [backends.py:193] Compiling a graph for general shape takes 2.42 s
INFO 12-17 20:43:52 [monitor.py:34] torch.compile takes 11.35 s in total
INFO 12-17 20:43:54 [worker_v1.py:181] Available memory: 41201078988, total memory: 65464696832
INFO 12-17 20:43:54 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:43:54 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:44:31 [model_runner_v1.py:2074] Graph capturing finished in 37 secs, took 0.36 GiB
INFO 12-17 20:44:31 [core.py:172] init engine (profile, create kv cache, warmup model) took 59.74 seconds
WARNING 12-17 20:44:32 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:44:32 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:44:32 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:44:32 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:44:32 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:03,  1.27it/s]Warmup iterations:  40%|████      | 2/5 [00:01<00:02,  1.36it/s]Warmup iterations:  60%|██████    | 3/5 [00:02<00:01,  1.38it/s]Warmup iterations:  80%|████████  | 4/5 [00:02<00:00,  1.39it/s]Warmup iterations: 100%|██████████| 5/5 [00:03<00:00,  1.40it/s]Warmup iterations: 100%|██████████| 5/5 [00:03<00:00,  1.39it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:06,  1.42it/s]Profiling iterations:  20%|██        | 2/10 [00:01<00:05,  1.41it/s]Profiling iterations:  30%|███       | 3/10 [00:02<00:04,  1.42it/s]Profiling iterations:  40%|████      | 4/10 [00:02<00:04,  1.42it/s]Profiling iterations:  50%|█████     | 5/10 [00:03<00:03,  1.42it/s]Profiling iterations:  60%|██████    | 6/10 [00:04<00:02,  1.42it/s]Profiling iterations:  70%|███████   | 7/10 [00:04<00:02,  1.42it/s]Profiling iterations:  80%|████████  | 8/10 [00:05<00:01,  1.41it/s]Profiling iterations:  90%|█████████ | 9/10 [00:06<00:00,  1.41it/s]Profiling iterations: 100%|██████████| 10/10 [00:07<00:00,  1.41it/s]Profiling iterations: 100%|██████████| 10/10 [00:07<00:00,  1.41it/s]
Avg latency: 0.7066119168419391 seconds
10% percentile latency: 0.7016944378148764 seconds
25% percentile latency: 0.7049334708135575 seconds
50% percentile latency: 0.7064290421549231 seconds
75% percentile latency: 0.7102814152603969 seconds
90% percentile latency: 0.7107751662842929 seconds
99% percentile latency: 0.7110216985922306 seconds
Batch-8 Input len-128 Output len-64
INFO 12-17 20:44:59 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:44:59 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:44:59 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:44:59 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:45:00 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:45:04 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:45:04 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:45:04 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:45:04 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:45:04 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:45:04 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:45:04 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:45:21 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-17 20:45:21 [config.py:1472] Using max model len 32768
INFO 12-17 20:45:21 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:45:21 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:45:21 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:45:21 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:45:21 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:45:22 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:45:31 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:45:31 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:45:31 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:45:31 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:45:32 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:45:35 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:45:36 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:45:36 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:45:36 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:45:36 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:45:36 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:45:36 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:45:36 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:45:36 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:45:50 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:45:50 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:45:50 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:45:50 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:45:51 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:45:58 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:45:58 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.63it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.21it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.12it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.35it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]

INFO 12-17 20:46:03 [default_loader.py:272] Loading weights took 3.13 seconds
INFO 12-17 20:46:05 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:46:15 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:46:15 [backends.py:519] Dynamo bytecode transform time: 9.37 s
INFO 12-17 20:46:18 [backends.py:193] Compiling a graph for general shape takes 2.61 s
INFO 12-17 20:46:27 [monitor.py:34] torch.compile takes 11.99 s in total
INFO 12-17 20:46:29 [worker_v1.py:181] Available memory: 41201504972, total memory: 65464696832
INFO 12-17 20:46:29 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:46:29 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:47:33 [model_runner_v1.py:2074] Graph capturing finished in 64 secs, took 0.36 GiB
INFO 12-17 20:47:33 [core.py:172] init engine (profile, create kv cache, warmup model) took 88.64 seconds
WARNING 12-17 20:47:34 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:47:35 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:47:35 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:47:35 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:47:35 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:05,  1.43s/it]Warmup iterations:  40%|████      | 2/5 [00:02<00:04,  1.37s/it]Warmup iterations:  60%|██████    | 3/5 [00:04<00:02,  1.35s/it]Warmup iterations:  80%|████████  | 4/5 [00:05<00:01,  1.34s/it]Warmup iterations: 100%|██████████| 5/5 [00:06<00:00,  1.35s/it]Warmup iterations: 100%|██████████| 5/5 [00:06<00:00,  1.35s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:12,  1.36s/it]Profiling iterations:  20%|██        | 2/10 [00:02<00:10,  1.35s/it]Profiling iterations:  30%|███       | 3/10 [00:04<00:09,  1.35s/it]Profiling iterations:  40%|████      | 4/10 [00:05<00:08,  1.35s/it]Profiling iterations:  50%|█████     | 5/10 [00:06<00:06,  1.36s/it]Profiling iterations:  60%|██████    | 6/10 [00:08<00:05,  1.35s/it]Profiling iterations:  70%|███████   | 7/10 [00:09<00:04,  1.35s/it]Profiling iterations:  80%|████████  | 8/10 [00:10<00:02,  1.35s/it]Profiling iterations:  90%|█████████ | 9/10 [00:12<00:01,  1.35s/it]Profiling iterations: 100%|██████████| 10/10 [00:13<00:00,  1.35s/it]Profiling iterations: 100%|██████████| 10/10 [00:13<00:00,  1.35s/it]
Avg latency: 1.354304731823504 seconds
10% percentile latency: 1.3523749845102429 seconds
25% percentile latency: 1.3527735254028812 seconds
50% percentile latency: 1.3540057006757706 seconds
75% percentile latency: 1.3559298986801878 seconds
90% percentile latency: 1.3568211487960071 seconds
99% percentile latency: 1.3569058922259136 seconds
Batch-8 Input len-128 Output len-128
INFO 12-17 20:48:13 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:48:13 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:48:13 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:48:13 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:48:14 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:48:18 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:48:18 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:48:18 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:48:18 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:48:18 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:48:18 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:48:18 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:48:35 [config.py:841] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 12-17 20:48:35 [config.py:1472] Using max model len 32768
INFO 12-17 20:48:35 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:48:35 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:48:35 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:48:35 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:48:35 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:48:36 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:48:44 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:48:44 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:48:44 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:48:44 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:48:46 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:48:48 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:48:50 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:48:50 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:48:50 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:48:50 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:48:50 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:48:50 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:48:50 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:48:50 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:49:03 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:49:03 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:49:03 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:49:03 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:49:04 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:49:10 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:49:10 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.61it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.13it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.05it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.25it/s]

INFO 12-17 20:49:15 [default_loader.py:272] Loading weights took 3.35 seconds
INFO 12-17 20:49:17 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:49:27 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:49:27 [backends.py:519] Dynamo bytecode transform time: 9.14 s
INFO 12-17 20:49:30 [backends.py:193] Compiling a graph for general shape takes 2.49 s
INFO 12-17 20:49:39 [monitor.py:34] torch.compile takes 11.63 s in total
INFO 12-17 20:49:41 [worker_v1.py:181] Available memory: 41200489164, total memory: 65464696832
INFO 12-17 20:49:41 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:49:41 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:50:33 [model_runner_v1.py:2074] Graph capturing finished in 52 secs, took 0.36 GiB
INFO 12-17 20:50:33 [core.py:172] init engine (profile, create kv cache, warmup model) took 76.09 seconds
WARNING 12-17 20:50:34 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:50:34 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:50:34 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:50:34 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:50:34 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:02<00:10,  2.69s/it]Warmup iterations:  40%|████      | 2/5 [00:05<00:07,  2.63s/it]Warmup iterations:  60%|██████    | 3/5 [00:07<00:05,  2.61s/it]Warmup iterations:  80%|████████  | 4/5 [00:10<00:02,  2.61s/it]Warmup iterations: 100%|██████████| 5/5 [00:13<00:00,  2.60s/it]Warmup iterations: 100%|██████████| 5/5 [00:13<00:00,  2.61s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:02<00:23,  2.60s/it]Profiling iterations:  20%|██        | 2/10 [00:05<00:20,  2.59s/it]Profiling iterations:  30%|███       | 3/10 [00:07<00:18,  2.59s/it]Profiling iterations:  40%|████      | 4/10 [00:10<00:15,  2.59s/it]Profiling iterations:  50%|█████     | 5/10 [00:12<00:12,  2.59s/it]Profiling iterations:  60%|██████    | 6/10 [00:15<00:10,  2.59s/it]Profiling iterations:  70%|███████   | 7/10 [00:18<00:07,  2.59s/it]Profiling iterations:  80%|████████  | 8/10 [00:20<00:05,  2.59s/it]Profiling iterations:  90%|█████████ | 9/10 [00:23<00:02,  2.59s/it]Profiling iterations: 100%|██████████| 10/10 [00:25<00:00,  2.59s/it]Profiling iterations: 100%|██████████| 10/10 [00:25<00:00,  2.59s/it]
Avg latency: 2.5919270203914495 seconds
10% percentile latency: 2.589052613545209 seconds
25% percentile latency: 2.590446516056545 seconds
50% percentile latency: 2.5923706421162933 seconds
75% percentile latency: 2.592560693039559 seconds
90% percentile latency: 2.5941196735017003 seconds
99% percentile latency: 2.5971426383964715 seconds
Batch-8 Input len-128 Output len-256
INFO 12-17 20:51:29 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:51:29 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:51:29 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:51:30 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:51:31 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:51:35 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:51:35 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:51:35 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:51:35 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:51:35 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:51:35 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:51:35 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:51:51 [config.py:841] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 12-17 20:51:51 [config.py:1472] Using max model len 32768
INFO 12-17 20:51:51 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:51:51 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:51:51 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:51:51 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:51:51 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:51:52 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:52:01 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:52:01 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:52:01 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:52:01 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:52:02 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:52:05 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:52:06 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:52:06 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:52:06 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:52:06 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:52:06 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:52:06 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:52:06 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:52:06 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:52:20 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:52:20 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:52:20 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:52:20 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:52:21 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:52:27 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:52:28 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.58it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.11it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.01it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.24it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.19it/s]

INFO 12-17 20:52:32 [default_loader.py:272] Loading weights took 3.50 seconds
INFO 12-17 20:52:33 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:52:42 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:52:42 [backends.py:519] Dynamo bytecode transform time: 8.86 s
INFO 12-17 20:52:45 [backends.py:193] Compiling a graph for general shape takes 2.41 s
INFO 12-17 20:52:54 [monitor.py:34] torch.compile takes 11.27 s in total
INFO 12-17 20:52:55 [worker_v1.py:181] Available memory: 41200861900, total memory: 65464696832
INFO 12-17 20:52:55 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:52:55 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:53:32 [model_runner_v1.py:2074] Graph capturing finished in 36 secs, took 0.36 GiB
INFO 12-17 20:53:32 [core.py:172] init engine (profile, create kv cache, warmup model) took 58.67 seconds
WARNING 12-17 20:53:33 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:53:33 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:53:33 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:53:33 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:53:33 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:05<00:20,  5.25s/it]Warmup iterations:  40%|████      | 2/5 [00:10<00:15,  5.20s/it]Warmup iterations:  60%|██████    | 3/5 [00:15<00:10,  5.19s/it]Warmup iterations:  80%|████████  | 4/5 [00:20<00:05,  5.18s/it]Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.18s/it]Warmup iterations: 100%|██████████| 5/5 [00:25<00:00,  5.19s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:05<00:46,  5.18s/it]Profiling iterations:  20%|██        | 2/10 [00:10<00:41,  5.18s/it]Profiling iterations:  30%|███       | 3/10 [00:15<00:36,  5.18s/it]Profiling iterations:  40%|████      | 4/10 [00:20<00:31,  5.18s/it]Profiling iterations:  50%|█████     | 5/10 [00:25<00:25,  5.17s/it]Profiling iterations:  60%|██████    | 6/10 [00:31<00:20,  5.17s/it]Profiling iterations:  70%|███████   | 7/10 [00:36<00:15,  5.17s/it]Profiling iterations:  80%|████████  | 8/10 [00:41<00:10,  5.17s/it]Profiling iterations:  90%|█████████ | 9/10 [00:46<00:05,  5.17s/it]Profiling iterations: 100%|██████████| 10/10 [00:51<00:00,  5.18s/it]Profiling iterations: 100%|██████████| 10/10 [00:51<00:00,  5.17s/it]
Avg latency: 5.174648742144927 seconds
10% percentile latency: 5.162933086045086 seconds
25% percentile latency: 5.171375457430258 seconds
50% percentile latency: 5.1765998005867 seconds
75% percentile latency: 5.1802631670143455 seconds
90% percentile latency: 5.1820049522910265 seconds
99% percentile latency: 5.182426996245049 seconds
Batch-8 Input len-128 Output len-512
INFO 12-17 20:55:07 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:55:07 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:55:07 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:55:07 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:55:08 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:55:12 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:55:12 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:55:12 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:55:12 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:55:12 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:55:12 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:55:12 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:55:29 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-17 20:55:29 [config.py:1472] Using max model len 32768
INFO 12-17 20:55:29 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 20:55:29 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:55:29 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:55:29 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:55:29 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 20:55:30 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 20:55:38 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:55:38 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:55:38 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:55:39 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:55:40 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:55:42 [core.py:526] Waiting for init message from front-end.
INFO 12-17 20:55:44 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 20:55:44 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 20:55:44 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 20:55:44 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 20:55:44 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 20:55:44 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 20:55:44 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 20:55:44 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 20:55:57 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 20:55:57 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 20:55:57 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 20:55:57 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 20:55:58 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 20:56:04 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 20:56:05 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.62it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.21it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.12it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.35it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]

INFO 12-17 20:56:09 [default_loader.py:272] Loading weights took 3.14 seconds
INFO 12-17 20:56:11 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 20:56:22 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 20:56:22 [backends.py:519] Dynamo bytecode transform time: 9.23 s
INFO 12-17 20:56:25 [backends.py:193] Compiling a graph for general shape takes 2.56 s
INFO 12-17 20:56:34 [monitor.py:34] torch.compile takes 11.79 s in total
INFO 12-17 20:56:36 [worker_v1.py:181] Available memory: 41200816844, total memory: 65464696832
INFO 12-17 20:56:36 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 20:56:36 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 20:57:41 [model_runner_v1.py:2074] Graph capturing finished in 65 secs, took 0.36 GiB
INFO 12-17 20:57:41 [core.py:172] init engine (profile, create kv cache, warmup model) took 89.36 seconds
WARNING 12-17 20:57:42 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 20:57:42 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 20:57:42 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 20:57:42 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 20:57:42 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:10<00:41, 10.40s/it]Warmup iterations:  40%|████      | 2/5 [00:20<00:31, 10.35s/it]Warmup iterations:  60%|██████    | 3/5 [00:31<00:20, 10.33s/it]Warmup iterations:  80%|████████  | 4/5 [00:41<00:10, 10.32s/it]Warmup iterations: 100%|██████████| 5/5 [00:51<00:00, 10.32s/it]Warmup iterations: 100%|██████████| 5/5 [00:51<00:00, 10.33s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:10<01:32, 10.32s/it]Profiling iterations:  20%|██        | 2/10 [00:20<01:22, 10.31s/it]Profiling iterations:  30%|███       | 3/10 [00:30<01:12, 10.31s/it]Profiling iterations:  40%|████      | 4/10 [00:41<01:01, 10.31s/it]Profiling iterations:  50%|█████     | 5/10 [00:51<00:51, 10.31s/it]Profiling iterations:  60%|██████    | 6/10 [01:01<00:41, 10.31s/it]Profiling iterations:  70%|███████   | 7/10 [01:12<00:30, 10.32s/it]Profiling iterations:  80%|████████  | 8/10 [01:22<00:20, 10.32s/it]Profiling iterations:  90%|█████████ | 9/10 [01:32<00:10, 10.31s/it]Profiling iterations: 100%|██████████| 10/10 [01:43<00:00, 10.31s/it]Profiling iterations: 100%|██████████| 10/10 [01:43<00:00, 10.31s/it]
Avg latency: 10.312011075299234 seconds
10% percentile latency: 10.306548939505593 seconds
25% percentile latency: 10.3074663085863 seconds
50% percentile latency: 10.313094707904384 seconds
75% percentile latency: 10.315936145256273 seconds
90% percentile latency: 10.317886527022347 seconds
99% percentile latency: 10.319998814524151 seconds
Batch-8 Input len-128 Output len-1024
INFO 12-17 21:00:33 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:00:33 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:00:33 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:00:33 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:00:35 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:00:38 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:00:39 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:00:39 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:00:39 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:00:39 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:00:39 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:00:39 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:00:55 [config.py:841] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 12-17 21:00:55 [config.py:1472] Using max model len 32768
INFO 12-17 21:00:55 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:00:55 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:00:55 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:00:55 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:00:55 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:00:56 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:01:05 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:01:05 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:01:05 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:01:05 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:01:06 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:01:09 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:01:10 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:01:10 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:01:10 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:01:10 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:01:10 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:01:10 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:01:10 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:01:10 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:01:23 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:01:23 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:01:23 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:01:23 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:01:25 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:01:31 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:01:31 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.56it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.08it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.02it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.22it/s]

INFO 12-17 21:01:36 [default_loader.py:272] Loading weights took 3.45 seconds
INFO 12-17 21:01:38 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:01:48 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:01:48 [backends.py:519] Dynamo bytecode transform time: 9.18 s
INFO 12-17 21:01:52 [backends.py:193] Compiling a graph for general shape takes 2.51 s
INFO 12-17 21:02:00 [monitor.py:34] torch.compile takes 11.69 s in total
INFO 12-17 21:02:02 [worker_v1.py:181] Available memory: 41202295500, total memory: 65464696832
INFO 12-17 21:02:02 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:02:02 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:03:07 [model_runner_v1.py:2074] Graph capturing finished in 64 secs, took 0.36 GiB
INFO 12-17 21:03:07 [core.py:172] init engine (profile, create kv cache, warmup model) took 88.56 seconds
WARNING 12-17 21:03:08 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:03:08 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:03:08 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:03:08 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:03:08 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:20<01:23, 20.81s/it]Warmup iterations:  40%|████      | 2/5 [00:41<01:02, 20.77s/it]Warmup iterations:  60%|██████    | 3/5 [01:02<00:41, 20.76s/it]Warmup iterations:  80%|████████  | 4/5 [01:23<00:20, 20.75s/it]Warmup iterations: 100%|██████████| 5/5 [01:43<00:00, 20.75s/it]Warmup iterations: 100%|██████████| 5/5 [01:43<00:00, 20.75s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:20<03:06, 20.77s/it]Profiling iterations:  20%|██        | 2/10 [00:41<02:46, 20.77s/it]Profiling iterations:  30%|███       | 3/10 [01:02<02:25, 20.77s/it]Profiling iterations:  40%|████      | 4/10 [01:23<02:04, 20.81s/it]Profiling iterations:  50%|█████     | 5/10 [01:44<01:44, 20.83s/it]Profiling iterations:  60%|██████    | 6/10 [02:04<01:23, 20.84s/it]Profiling iterations:  70%|███████   | 7/10 [02:25<01:02, 20.84s/it]Profiling iterations:  80%|████████  | 8/10 [02:46<00:41, 20.83s/it]Profiling iterations:  90%|█████████ | 9/10 [03:07<00:20, 20.83s/it]Profiling iterations: 100%|██████████| 10/10 [03:28<00:00, 20.82s/it]Profiling iterations: 100%|██████████| 10/10 [03:28<00:00, 20.82s/it]
Avg latency: 20.81709281504154 seconds
10% percentile latency: 20.766758005740122 seconds
25% percentile latency: 20.78114740678575 seconds
50% percentile latency: 20.81778946937993 seconds
75% percentile latency: 20.8445304997731 seconds
90% percentile latency: 20.874504790920763 seconds
99% percentile latency: 20.87556192006916 seconds
Batch-8 Input len-256 Output len-32
INFO 12-17 21:08:38 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:08:38 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:08:38 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:08:38 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:08:40 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:08:43 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:08:44 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:08:44 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:08:44 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:08:44 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:08:44 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:08:44 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:09:00 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 12-17 21:09:00 [config.py:1472] Using max model len 32768
INFO 12-17 21:09:00 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:09:00 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:09:00 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:09:00 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:09:00 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:09:01 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:09:10 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:09:10 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:09:10 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:09:10 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:09:11 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:09:14 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:09:15 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:09:15 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:09:15 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:09:15 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:09:15 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:09:15 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:09:15 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:09:15 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:09:28 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:09:28 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:09:28 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:09:28 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:09:30 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:09:36 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:09:36 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.11it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.64it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.49it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.51it/s]

INFO 12-17 21:09:40 [default_loader.py:272] Loading weights took 2.75 seconds
INFO 12-17 21:09:42 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:09:53 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:09:53 [backends.py:519] Dynamo bytecode transform time: 9.29 s
INFO 12-17 21:09:56 [backends.py:193] Compiling a graph for general shape takes 2.56 s
INFO 12-17 21:10:05 [monitor.py:34] torch.compile takes 11.85 s in total
INFO 12-17 21:10:07 [worker_v1.py:181] Available memory: 41201066700, total memory: 65464696832
INFO 12-17 21:10:07 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:10:07 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:11:12 [model_runner_v1.py:2074] Graph capturing finished in 65 secs, took 0.36 GiB
INFO 12-17 21:11:12 [core.py:172] init engine (profile, create kv cache, warmup model) took 89.55 seconds
WARNING 12-17 21:11:13 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:11:13 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:11:13 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:11:13 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:11:13 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:03,  1.18it/s]Warmup iterations:  40%|████      | 2/5 [00:01<00:02,  1.26it/s]Warmup iterations:  60%|██████    | 3/5 [00:02<00:01,  1.28it/s]Warmup iterations:  80%|████████  | 4/5 [00:03<00:00,  1.29it/s]Warmup iterations: 100%|██████████| 5/5 [00:03<00:00,  1.29it/s]Warmup iterations: 100%|██████████| 5/5 [00:03<00:00,  1.28it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:06,  1.30it/s]Profiling iterations:  20%|██        | 2/10 [00:01<00:06,  1.30it/s]Profiling iterations:  30%|███       | 3/10 [00:02<00:05,  1.30it/s]Profiling iterations:  40%|████      | 4/10 [00:03<00:04,  1.30it/s]Profiling iterations:  50%|█████     | 5/10 [00:03<00:03,  1.30it/s]Profiling iterations:  60%|██████    | 6/10 [00:04<00:03,  1.30it/s]Profiling iterations:  70%|███████   | 7/10 [00:05<00:02,  1.30it/s]Profiling iterations:  80%|████████  | 8/10 [00:06<00:01,  1.30it/s]Profiling iterations:  90%|█████████ | 9/10 [00:06<00:00,  1.30it/s]Profiling iterations: 100%|██████████| 10/10 [00:07<00:00,  1.30it/s]Profiling iterations: 100%|██████████| 10/10 [00:07<00:00,  1.30it/s]
Avg latency: 0.7678968911990524 seconds
10% percentile latency: 0.764668523799628 seconds
25% percentile latency: 0.766331950086169 seconds
50% percentile latency: 0.7680394821800292 seconds
75% percentile latency: 0.769839703803882 seconds
90% percentile latency: 0.7707001180853694 seconds
99% percentile latency: 0.7712931297300384 seconds
Batch-8 Input len-256 Output len-64
INFO 12-17 21:11:43 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:11:43 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:11:43 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:11:43 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:11:44 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:11:48 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:11:48 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:11:48 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:11:48 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:11:48 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:11:48 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:11:48 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:12:05 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 12-17 21:12:05 [config.py:1472] Using max model len 32768
INFO 12-17 21:12:05 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:12:05 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:12:05 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:12:05 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:12:05 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:12:06 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:12:15 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:12:15 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:12:15 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:12:15 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:12:16 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:12:19 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:12:20 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:12:20 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:12:20 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:12:20 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:12:20 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:12:20 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:12:20 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:12:20 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:12:33 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:12:33 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:12:33 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:12:33 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:12:35 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:12:41 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:12:41 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.11it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.63it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.49it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.50it/s]

INFO 12-17 21:12:45 [default_loader.py:272] Loading weights took 2.76 seconds
INFO 12-17 21:12:47 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:12:58 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:12:58 [backends.py:519] Dynamo bytecode transform time: 9.34 s
INFO 12-17 21:13:01 [backends.py:193] Compiling a graph for general shape takes 2.59 s
INFO 12-17 21:13:10 [monitor.py:34] torch.compile takes 11.93 s in total
INFO 12-17 21:13:12 [worker_v1.py:181] Available memory: 41176920780, total memory: 65464696832
INFO 12-17 21:13:12 [kv_cache_utils.py:716] GPU KV cache size: 295,552 tokens
INFO 12-17 21:13:12 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.02x
INFO 12-17 21:14:17 [model_runner_v1.py:2074] Graph capturing finished in 65 secs, took 0.34 GiB
INFO 12-17 21:14:17 [core.py:172] init engine (profile, create kv cache, warmup model) took 89.53 seconds
WARNING 12-17 21:14:18 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:14:18 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:14:18 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:14:18 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:14:18 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:05,  1.48s/it]Warmup iterations:  40%|████      | 2/5 [00:02<00:04,  1.44s/it]Warmup iterations:  60%|██████    | 3/5 [00:04<00:02,  1.43s/it]Warmup iterations:  80%|████████  | 4/5 [00:05<00:01,  1.42s/it]Warmup iterations: 100%|██████████| 5/5 [00:07<00:00,  1.42s/it]Warmup iterations: 100%|██████████| 5/5 [00:07<00:00,  1.43s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:12,  1.42s/it]Profiling iterations:  20%|██        | 2/10 [00:02<00:11,  1.42s/it]Profiling iterations:  30%|███       | 3/10 [00:04<00:09,  1.42s/it]Profiling iterations:  40%|████      | 4/10 [00:05<00:08,  1.42s/it]Profiling iterations:  50%|█████     | 5/10 [00:07<00:07,  1.42s/it]Profiling iterations:  60%|██████    | 6/10 [00:08<00:05,  1.42s/it]Profiling iterations:  70%|███████   | 7/10 [00:09<00:04,  1.42s/it]Profiling iterations:  80%|████████  | 8/10 [00:11<00:02,  1.41s/it]Profiling iterations:  90%|█████████ | 9/10 [00:12<00:01,  1.41s/it]Profiling iterations: 100%|██████████| 10/10 [00:14<00:00,  1.41s/it]Profiling iterations: 100%|██████████| 10/10 [00:14<00:00,  1.42s/it]
Avg latency: 1.4148666747845708 seconds
10% percentile latency: 1.4132124117575586 seconds
25% percentile latency: 1.414096332155168 seconds
50% percentile latency: 1.4149484005756676 seconds
75% percentile latency: 1.4156407851260155 seconds
90% percentile latency: 1.4168187047354877 seconds
99% percentile latency: 1.4181150902807713 seconds
Batch-8 Input len-256 Output len-128
INFO 12-17 21:14:58 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:14:58 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:14:58 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:14:58 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:14:59 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:15:03 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:15:03 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:15:03 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:15:03 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:15:03 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:15:03 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:15:03 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:15:20 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 12-17 21:15:20 [config.py:1472] Using max model len 32768
INFO 12-17 21:15:20 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:15:20 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:15:20 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:15:20 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:15:20 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:15:21 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:15:29 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:15:29 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:15:29 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:15:29 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:15:31 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:15:33 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:15:35 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:15:35 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:15:35 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:15:35 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:15:35 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:15:35 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:15:35 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:15:35 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:15:48 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:15:48 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:15:48 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:15:48 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:15:49 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:15:55 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:15:56 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.53it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.15it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.24it/s]

INFO 12-17 21:16:00 [default_loader.py:272] Loading weights took 3.34 seconds
INFO 12-17 21:16:01 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:16:11 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:16:11 [backends.py:519] Dynamo bytecode transform time: 9.37 s
INFO 12-17 21:16:14 [backends.py:193] Compiling a graph for general shape takes 2.53 s
INFO 12-17 21:16:22 [monitor.py:34] torch.compile takes 11.90 s in total
INFO 12-17 21:16:24 [worker_v1.py:181] Available memory: 41201959628, total memory: 65464696832
INFO 12-17 21:16:24 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:16:24 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:16:57 [model_runner_v1.py:2074] Graph capturing finished in 33 secs, took 0.36 GiB
INFO 12-17 21:16:57 [core.py:172] init engine (profile, create kv cache, warmup model) took 56.50 seconds
WARNING 12-17 21:16:58 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:16:59 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:16:59 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:16:59 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:16:59 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:02<00:10,  2.74s/it]Warmup iterations:  40%|████      | 2/5 [00:05<00:08,  2.69s/it]Warmup iterations:  60%|██████    | 3/5 [00:08<00:05,  2.67s/it]Warmup iterations:  80%|████████  | 4/5 [00:10<00:02,  2.66s/it]Warmup iterations: 100%|██████████| 5/5 [00:13<00:00,  2.66s/it]Warmup iterations: 100%|██████████| 5/5 [00:13<00:00,  2.67s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:02<00:23,  2.65s/it]Profiling iterations:  20%|██        | 2/10 [00:05<00:21,  2.65s/it]Profiling iterations:  30%|███       | 3/10 [00:07<00:18,  2.65s/it]Profiling iterations:  40%|████      | 4/10 [00:10<00:15,  2.65s/it]Profiling iterations:  50%|█████     | 5/10 [00:13<00:13,  2.65s/it]Profiling iterations:  60%|██████    | 6/10 [00:15<00:10,  2.65s/it]Profiling iterations:  70%|███████   | 7/10 [00:18<00:07,  2.65s/it]Profiling iterations:  80%|████████  | 8/10 [00:21<00:05,  2.65s/it]Profiling iterations:  90%|█████████ | 9/10 [00:23<00:02,  2.66s/it]Profiling iterations: 100%|██████████| 10/10 [00:26<00:00,  2.66s/it]Profiling iterations: 100%|██████████| 10/10 [00:26<00:00,  2.66s/it]
Avg latency: 2.655239250184968 seconds
10% percentile latency: 2.650660031614825 seconds
25% percentile latency: 2.6517977688927203 seconds
50% percentile latency: 2.6535350715275854 seconds
75% percentile latency: 2.656103156739846 seconds
90% percentile latency: 2.6637028568889947 seconds
99% percentile latency: 2.667483012531884 seconds
Batch-8 Input len-256 Output len-256
INFO 12-17 21:17:55 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:17:55 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:17:55 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:17:55 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:17:56 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:18:00 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:18:00 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:18:00 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:18:00 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:18:00 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:18:00 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:18:00 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:18:17 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 12-17 21:18:17 [config.py:1472] Using max model len 32768
INFO 12-17 21:18:17 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:18:17 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:18:17 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:18:17 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:18:17 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:18:18 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:18:27 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:18:27 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:18:27 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:18:27 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:18:28 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:18:31 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:18:32 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:18:32 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:18:32 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:18:32 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:18:32 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:18:32 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:18:32 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:18:32 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:18:45 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:18:45 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:18:45 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:18:45 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:18:46 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:18:53 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:18:53 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.61it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.12it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.05it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.25it/s]

INFO 12-17 21:18:58 [default_loader.py:272] Loading weights took 3.36 seconds
INFO 12-17 21:19:00 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:19:10 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:19:10 [backends.py:519] Dynamo bytecode transform time: 9.19 s
INFO 12-17 21:19:13 [backends.py:193] Compiling a graph for general shape takes 2.52 s
INFO 12-17 21:19:22 [monitor.py:34] torch.compile takes 11.71 s in total
INFO 12-17 21:19:24 [worker_v1.py:181] Available memory: 41201640140, total memory: 65464696832
INFO 12-17 21:19:24 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:19:24 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:20:24 [model_runner_v1.py:2074] Graph capturing finished in 60 secs, took 0.36 GiB
INFO 12-17 21:20:24 [core.py:172] init engine (profile, create kv cache, warmup model) took 84.71 seconds
WARNING 12-17 21:20:25 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:20:26 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:20:26 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:20:26 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:20:26 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:05<00:21,  5.29s/it]Warmup iterations:  40%|████      | 2/5 [00:10<00:15,  5.23s/it]Warmup iterations:  60%|██████    | 3/5 [00:15<00:10,  5.22s/it]Warmup iterations:  80%|████████  | 4/5 [00:20<00:05,  5.21s/it]Warmup iterations: 100%|██████████| 5/5 [00:26<00:00,  5.20s/it]Warmup iterations: 100%|██████████| 5/5 [00:26<00:00,  5.21s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:05<00:46,  5.19s/it]Profiling iterations:  20%|██        | 2/10 [00:10<00:41,  5.19s/it]Profiling iterations:  30%|███       | 3/10 [00:15<00:36,  5.20s/it]Profiling iterations:  40%|████      | 4/10 [00:20<00:31,  5.19s/it]Profiling iterations:  50%|█████     | 5/10 [00:25<00:25,  5.19s/it]Profiling iterations:  60%|██████    | 6/10 [00:31<00:20,  5.19s/it]Profiling iterations:  70%|███████   | 7/10 [00:36<00:15,  5.19s/it]Profiling iterations:  80%|████████  | 8/10 [00:41<00:10,  5.19s/it]Profiling iterations:  90%|█████████ | 9/10 [00:46<00:05,  5.19s/it]Profiling iterations: 100%|██████████| 10/10 [00:51<00:00,  5.19s/it]Profiling iterations: 100%|██████████| 10/10 [00:51<00:00,  5.19s/it]
Avg latency: 5.1932050289586185 seconds
10% percentile latency: 5.188423692248762 seconds
25% percentile latency: 5.19253113202285 seconds
50% percentile latency: 5.194129640469328 seconds
75% percentile latency: 5.1949546886608005 seconds
90% percentile latency: 5.195678198430687 seconds
99% percentile latency: 5.19645342959091 seconds
Batch-8 Input len-256 Output len-512
INFO 12-17 21:22:02 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:22:02 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:22:02 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:22:02 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:22:03 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:22:07 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:22:07 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:22:07 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:22:07 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:22:07 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:22:07 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:22:07 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:22:24 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 12-17 21:22:24 [config.py:1472] Using max model len 32768
INFO 12-17 21:22:24 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:22:24 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:22:24 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:22:24 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:22:24 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:22:25 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:22:33 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:22:33 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:22:33 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:22:33 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:22:35 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:22:37 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:22:38 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:22:39 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:22:39 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:22:39 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:22:39 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:22:39 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:22:39 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:22:39 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:22:52 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:22:52 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:22:52 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:22:52 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:22:53 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:22:59 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:22:59 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.61it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.16it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.33it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.27it/s]

INFO 12-17 21:23:04 [default_loader.py:272] Loading weights took 3.31 seconds
INFO 12-17 21:23:06 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:23:16 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:23:16 [backends.py:519] Dynamo bytecode transform time: 9.17 s
INFO 12-17 21:23:19 [backends.py:193] Compiling a graph for general shape takes 2.51 s
INFO 12-17 21:23:28 [monitor.py:34] torch.compile takes 11.68 s in total
INFO 12-17 21:23:30 [worker_v1.py:181] Available memory: 41201611468, total memory: 65464696832
INFO 12-17 21:23:30 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:23:30 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:24:30 [model_runner_v1.py:2074] Graph capturing finished in 60 secs, took 0.36 GiB
INFO 12-17 21:24:30 [core.py:172] init engine (profile, create kv cache, warmup model) took 83.73 seconds
WARNING 12-17 21:24:31 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:24:31 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:24:31 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:24:31 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:24:31 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:10<00:41, 10.42s/it]Warmup iterations:  40%|████      | 2/5 [00:20<00:31, 10.36s/it]Warmup iterations:  60%|██████    | 3/5 [00:31<00:20, 10.35s/it]Warmup iterations:  80%|████████  | 4/5 [00:41<00:10, 10.34s/it]Warmup iterations: 100%|██████████| 5/5 [00:51<00:00, 10.34s/it]Warmup iterations: 100%|██████████| 5/5 [00:51<00:00, 10.35s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:10<01:32, 10.33s/it]Profiling iterations:  20%|██        | 2/10 [00:20<01:22, 10.33s/it]Profiling iterations:  30%|███       | 3/10 [00:30<01:12, 10.33s/it]Profiling iterations:  40%|████      | 4/10 [00:41<01:01, 10.33s/it]Profiling iterations:  50%|█████     | 5/10 [00:51<00:51, 10.33s/it]Profiling iterations:  60%|██████    | 6/10 [01:01<00:41, 10.33s/it]Profiling iterations:  70%|███████   | 7/10 [01:12<00:30, 10.33s/it]Profiling iterations:  80%|████████  | 8/10 [01:22<00:20, 10.33s/it]Profiling iterations:  90%|█████████ | 9/10 [01:32<00:10, 10.33s/it]Profiling iterations: 100%|██████████| 10/10 [01:43<00:00, 10.33s/it]Profiling iterations: 100%|██████████| 10/10 [01:43<00:00, 10.33s/it]
Avg latency: 10.327189806383103 seconds
10% percentile latency: 10.319662561733276 seconds
25% percentile latency: 10.325665518292226 seconds
50% percentile latency: 10.328375718556345 seconds
75% percentile latency: 10.32949421426747 seconds
90% percentile latency: 10.330880497116596 seconds
99% percentile latency: 10.334819869231433 seconds
Batch-8 Input len-256 Output len-1024
INFO 12-17 21:27:24 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:27:24 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:27:24 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:27:24 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:27:25 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:27:29 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:27:29 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:27:29 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:27:29 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:27:29 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:27:29 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:27:29 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:27:46 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 12-17 21:27:46 [config.py:1472] Using max model len 32768
INFO 12-17 21:27:46 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:27:46 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:27:46 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:27:46 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:27:46 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:27:47 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:27:56 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:27:56 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:27:56 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:27:56 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:27:57 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:28:00 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:28:01 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:28:01 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:28:01 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:28:01 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:28:01 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:28:01 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:28:01 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:28:01 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:28:14 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:28:14 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:28:14 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:28:14 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:28:15 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:28:22 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:28:22 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.16it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.62it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.30it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.48it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.50it/s]

INFO 12-17 21:28:26 [default_loader.py:272] Loading weights took 2.77 seconds
INFO 12-17 21:28:28 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:28:39 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:28:39 [backends.py:519] Dynamo bytecode transform time: 9.27 s
INFO 12-17 21:28:42 [backends.py:193] Compiling a graph for general shape takes 2.57 s
INFO 12-17 21:28:51 [monitor.py:34] torch.compile takes 11.84 s in total
INFO 12-17 21:28:53 [worker_v1.py:181] Available memory: 41200915148, total memory: 65464696832
INFO 12-17 21:28:53 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:28:53 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:29:58 [model_runner_v1.py:2074] Graph capturing finished in 65 secs, took 0.36 GiB
INFO 12-17 21:29:58 [core.py:172] init engine (profile, create kv cache, warmup model) took 89.73 seconds
WARNING 12-17 21:29:59 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:29:59 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:29:59 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:29:59 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:29:59 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:21<01:24, 21.01s/it]Warmup iterations:  40%|████      | 2/5 [00:41<01:02, 20.93s/it]Warmup iterations:  60%|██████    | 3/5 [01:02<00:41, 20.91s/it]Warmup iterations:  80%|████████  | 4/5 [01:23<00:20, 20.90s/it]Warmup iterations: 100%|██████████| 5/5 [01:44<00:00, 20.88s/it]Warmup iterations: 100%|██████████| 5/5 [01:44<00:00, 20.90s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:20<03:07, 20.86s/it]Profiling iterations:  20%|██        | 2/10 [00:41<02:46, 20.85s/it]Profiling iterations:  30%|███       | 3/10 [01:02<02:25, 20.85s/it]Profiling iterations:  40%|████      | 4/10 [01:23<02:05, 20.86s/it]Profiling iterations:  50%|█████     | 5/10 [01:44<01:44, 20.86s/it]Profiling iterations:  60%|██████    | 6/10 [02:05<01:23, 20.86s/it]Profiling iterations:  70%|███████   | 7/10 [02:26<01:02, 20.87s/it]Profiling iterations:  80%|████████  | 8/10 [02:46<00:41, 20.88s/it]Profiling iterations:  90%|█████████ | 9/10 [03:07<00:20, 20.87s/it]Profiling iterations: 100%|██████████| 10/10 [03:28<00:00, 20.86s/it]Profiling iterations: 100%|██████████| 10/10 [03:28<00:00, 20.86s/it]
Avg latency: 20.862009185319767 seconds
10% percentile latency: 20.841352583002298 seconds
25% percentile latency: 20.845650880248286 seconds
50% percentile latency: 20.86136587499641 seconds
75% percentile latency: 20.874639087473042 seconds
90% percentile latency: 20.880933398939668 seconds
99% percentile latency: 20.89153890568763 seconds
Batch-8 Input len-512 Output len-32
INFO 12-17 21:35:29 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:35:29 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:35:29 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:35:29 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:35:30 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:35:34 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:35:35 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:35:35 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:35:35 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:35:35 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:35:35 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:35:35 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:35:51 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-17 21:35:51 [config.py:1472] Using max model len 32768
INFO 12-17 21:35:51 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:35:51 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:35:51 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:35:51 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:35:51 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:35:52 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:36:01 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:36:01 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:36:01 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:36:01 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:36:02 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:36:05 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:36:06 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:36:06 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:36:06 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:36:06 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:36:06 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:36:06 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:36:06 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:36:06 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:36:19 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:36:19 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:36:19 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:36:19 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:36:21 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:36:27 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:36:27 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.53it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.13it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.01it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.21it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.18it/s]

INFO 12-17 21:36:31 [default_loader.py:272] Loading weights took 3.56 seconds
INFO 12-17 21:36:32 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:36:42 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:36:42 [backends.py:519] Dynamo bytecode transform time: 9.38 s
INFO 12-17 21:36:45 [backends.py:193] Compiling a graph for general shape takes 2.57 s
INFO 12-17 21:36:54 [monitor.py:34] torch.compile takes 11.95 s in total
INFO 12-17 21:36:55 [worker_v1.py:181] Available memory: 41201287884, total memory: 65464696832
INFO 12-17 21:36:55 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:36:55 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:37:29 [model_runner_v1.py:2074] Graph capturing finished in 33 secs, took 0.36 GiB
INFO 12-17 21:37:29 [core.py:172] init engine (profile, create kv cache, warmup model) took 56.59 seconds
WARNING 12-17 21:37:30 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:37:30 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:37:30 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:37:30 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:37:30 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:00<00:03,  1.04it/s]Warmup iterations:  40%|████      | 2/5 [00:01<00:02,  1.08it/s]Warmup iterations:  60%|██████    | 3/5 [00:02<00:01,  1.10it/s]Warmup iterations:  80%|████████  | 4/5 [00:03<00:00,  1.11it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.11it/s]Warmup iterations: 100%|██████████| 5/5 [00:04<00:00,  1.10it/s]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:00<00:08,  1.12it/s]Profiling iterations:  20%|██        | 2/10 [00:01<00:07,  1.12it/s]Profiling iterations:  30%|███       | 3/10 [00:02<00:06,  1.12it/s]Profiling iterations:  40%|████      | 4/10 [00:03<00:05,  1.12it/s]Profiling iterations:  50%|█████     | 5/10 [00:04<00:04,  1.12it/s]Profiling iterations:  60%|██████    | 6/10 [00:05<00:03,  1.12it/s]Profiling iterations:  70%|███████   | 7/10 [00:06<00:02,  1.12it/s]Profiling iterations:  80%|████████  | 8/10 [00:07<00:01,  1.12it/s]Profiling iterations:  90%|█████████ | 9/10 [00:08<00:00,  1.12it/s]Profiling iterations: 100%|██████████| 10/10 [00:08<00:00,  1.12it/s]Profiling iterations: 100%|██████████| 10/10 [00:08<00:00,  1.12it/s]
Avg latency: 0.890863663237542 seconds
10% percentile latency: 0.8892573745455593 seconds
25% percentile latency: 0.890324811800383 seconds
50% percentile latency: 0.8904943764209747 seconds
75% percentile latency: 0.8911277734441683 seconds
90% percentile latency: 0.8928598016966134 seconds
99% percentile latency: 0.8935486373538152 seconds
Batch-8 Input len-512 Output len-64
INFO 12-17 21:38:00 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:38:00 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:38:00 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:38:00 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:38:02 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:38:05 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:38:06 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:38:06 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:38:06 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:38:06 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:38:06 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:38:06 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:38:22 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 12-17 21:38:22 [config.py:1472] Using max model len 32768
INFO 12-17 21:38:22 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:38:22 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:38:22 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:38:22 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:38:22 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:38:23 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:38:32 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:38:32 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:38:32 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:38:32 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:38:33 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:38:36 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:38:37 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:38:37 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:38:37 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:38:37 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:38:37 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:38:37 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:38:37 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:38:37 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:38:50 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:38:50 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:38:50 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:38:50 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:38:52 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:38:58 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:38:58 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.57it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.13it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.03it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.25it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.21it/s]

INFO 12-17 21:39:03 [default_loader.py:272] Loading weights took 3.46 seconds
INFO 12-17 21:39:04 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:39:13 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:39:13 [backends.py:519] Dynamo bytecode transform time: 8.87 s
INFO 12-17 21:39:16 [backends.py:193] Compiling a graph for general shape takes 2.40 s
INFO 12-17 21:39:25 [monitor.py:34] torch.compile takes 11.27 s in total
INFO 12-17 21:39:27 [worker_v1.py:181] Available memory: 41201160908, total memory: 65464696832
INFO 12-17 21:39:27 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:39:27 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:40:03 [model_runner_v1.py:2074] Graph capturing finished in 36 secs, took 0.36 GiB
INFO 12-17 21:40:03 [core.py:172] init engine (profile, create kv cache, warmup model) took 59.28 seconds
WARNING 12-17 21:40:04 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:40:04 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:40:04 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:40:04 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:40:04 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:06,  1.61s/it]Warmup iterations:  40%|████      | 2/5 [00:03<00:04,  1.58s/it]Warmup iterations:  60%|██████    | 3/5 [00:04<00:03,  1.57s/it]Warmup iterations:  80%|████████  | 4/5 [00:06<00:01,  1.56s/it]Warmup iterations: 100%|██████████| 5/5 [00:07<00:00,  1.56s/it]Warmup iterations: 100%|██████████| 5/5 [00:07<00:00,  1.57s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:14,  1.56s/it]Profiling iterations:  20%|██        | 2/10 [00:03<00:12,  1.56s/it]Profiling iterations:  30%|███       | 3/10 [00:04<00:10,  1.56s/it]Profiling iterations:  40%|████      | 4/10 [00:06<00:09,  1.55s/it]Profiling iterations:  50%|█████     | 5/10 [00:07<00:07,  1.55s/it]Profiling iterations:  60%|██████    | 6/10 [00:09<00:06,  1.55s/it]Profiling iterations:  70%|███████   | 7/10 [00:10<00:04,  1.55s/it]Profiling iterations:  80%|████████  | 8/10 [00:12<00:03,  1.54s/it]Profiling iterations:  90%|█████████ | 9/10 [00:13<00:01,  1.54s/it]Profiling iterations: 100%|██████████| 10/10 [00:15<00:00,  1.54s/it]Profiling iterations: 100%|██████████| 10/10 [00:15<00:00,  1.55s/it]
Avg latency: 1.5481269063428045 seconds
10% percentile latency: 1.540913600521162 seconds
25% percentile latency: 1.5453911318909377 seconds
50% percentile latency: 1.5506035403814167 seconds
75% percentile latency: 1.5512360148131847 seconds
90% percentile latency: 1.555993250850588 seconds
99% percentile latency: 1.5620194161124528 seconds
Batch-8 Input len-512 Output len-128
INFO 12-17 21:40:46 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:40:46 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:40:46 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:40:46 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:40:47 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:40:51 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:40:51 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:40:51 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:40:51 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:40:51 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:40:51 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:40:51 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:41:07 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-17 21:41:07 [config.py:1472] Using max model len 32768
INFO 12-17 21:41:08 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:41:08 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:41:08 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:41:08 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:41:08 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:41:09 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:41:17 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:41:17 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:41:17 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:41:17 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:41:18 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:41:21 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:41:22 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:41:23 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:41:23 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:41:23 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:41:23 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:41:23 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:41:23 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:41:23 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:41:36 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:41:36 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:41:36 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:41:36 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:41:37 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:41:43 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:41:44 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.61it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.14it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.33it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.27it/s]

INFO 12-17 21:41:48 [default_loader.py:272] Loading weights took 3.32 seconds
INFO 12-17 21:41:50 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:42:00 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:42:00 [backends.py:519] Dynamo bytecode transform time: 9.10 s
INFO 12-17 21:42:03 [backends.py:193] Compiling a graph for general shape takes 2.50 s
INFO 12-17 21:42:12 [monitor.py:34] torch.compile takes 11.60 s in total
INFO 12-17 21:42:14 [worker_v1.py:181] Available memory: 41201222348, total memory: 65464696832
INFO 12-17 21:42:14 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:42:14 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:43:22 [model_runner_v1.py:2074] Graph capturing finished in 68 secs, took 0.36 GiB
INFO 12-17 21:43:22 [core.py:172] init engine (profile, create kv cache, warmup model) took 91.89 seconds
WARNING 12-17 21:43:23 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:43:23 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:43:23 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:43:23 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:43:23 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:02<00:11,  2.89s/it]Warmup iterations:  40%|████      | 2/5 [00:05<00:08,  2.85s/it]Warmup iterations:  60%|██████    | 3/5 [00:08<00:05,  2.83s/it]Warmup iterations:  80%|████████  | 4/5 [00:11<00:02,  2.82s/it]Warmup iterations: 100%|██████████| 5/5 [00:14<00:00,  2.82s/it]Warmup iterations: 100%|██████████| 5/5 [00:14<00:00,  2.83s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:02<00:25,  2.82s/it]Profiling iterations:  20%|██        | 2/10 [00:05<00:22,  2.81s/it]Profiling iterations:  30%|███       | 3/10 [00:08<00:19,  2.81s/it]Profiling iterations:  40%|████      | 4/10 [00:11<00:16,  2.81s/it]Profiling iterations:  50%|█████     | 5/10 [00:14<00:14,  2.81s/it]Profiling iterations:  60%|██████    | 6/10 [00:16<00:11,  2.81s/it]Profiling iterations:  70%|███████   | 7/10 [00:19<00:08,  2.81s/it]Profiling iterations:  80%|████████  | 8/10 [00:22<00:05,  2.81s/it]Profiling iterations:  90%|█████████ | 9/10 [00:25<00:02,  2.81s/it]Profiling iterations: 100%|██████████| 10/10 [00:28<00:00,  2.81s/it]Profiling iterations: 100%|██████████| 10/10 [00:28<00:00,  2.81s/it]
Avg latency: 2.8112806058023123 seconds
10% percentile latency: 2.8076362908817827 seconds
25% percentile latency: 2.8081112889340147 seconds
50% percentile latency: 2.81027292762883 seconds
75% percentile latency: 2.8144641092512757 seconds
90% percentile latency: 2.8154641430359333 seconds
99% percentile latency: 2.8174607929633932 seconds
Batch-8 Input len-512 Output len-256
INFO 12-17 21:44:24 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:44:24 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:44:24 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:44:24 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:44:25 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:44:29 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:44:29 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:44:29 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:44:29 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:44:29 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:44:29 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:44:29 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:44:46 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 12-17 21:44:46 [config.py:1472] Using max model len 32768
INFO 12-17 21:44:46 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:44:46 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:44:46 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:44:46 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:44:46 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:44:47 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:44:55 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:44:55 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:44:55 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:44:55 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:44:57 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:44:59 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:45:00 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:45:01 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:45:01 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:45:01 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:45:01 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:45:01 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:45:01 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:45:01 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:45:14 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:45:14 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:45:14 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:45:14 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:45:15 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:45:21 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:45:22 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.52it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.15it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.24it/s]

INFO 12-17 21:45:26 [default_loader.py:272] Loading weights took 3.35 seconds
INFO 12-17 21:45:27 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:45:36 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:45:36 [backends.py:519] Dynamo bytecode transform time: 9.32 s
INFO 12-17 21:45:40 [backends.py:193] Compiling a graph for general shape takes 2.47 s
INFO 12-17 21:45:48 [monitor.py:34] torch.compile takes 11.79 s in total
INFO 12-17 21:45:49 [worker_v1.py:181] Available memory: 41200063180, total memory: 65464696832
INFO 12-17 21:45:49 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:45:49 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:46:25 [model_runner_v1.py:2074] Graph capturing finished in 36 secs, took 0.36 GiB
INFO 12-17 21:46:25 [core.py:172] init engine (profile, create kv cache, warmup model) took 58.39 seconds
WARNING 12-17 21:46:26 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:46:26 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:46:26 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:46:26 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:46:26 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:05<00:22,  5.51s/it]Warmup iterations:  40%|████      | 2/5 [00:10<00:16,  5.48s/it]Warmup iterations:  60%|██████    | 3/5 [00:16<00:10,  5.48s/it]Warmup iterations:  80%|████████  | 4/5 [00:21<00:05,  5.48s/it]Warmup iterations: 100%|██████████| 5/5 [00:27<00:00,  5.48s/it]Warmup iterations: 100%|██████████| 5/5 [00:27<00:00,  5.48s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:05<00:49,  5.49s/it]Profiling iterations:  20%|██        | 2/10 [00:10<00:43,  5.48s/it]Profiling iterations:  30%|███       | 3/10 [00:16<00:38,  5.49s/it]Profiling iterations:  40%|████      | 4/10 [00:21<00:32,  5.48s/it]Profiling iterations:  50%|█████     | 5/10 [00:27<00:27,  5.48s/it]Profiling iterations:  60%|██████    | 6/10 [00:32<00:21,  5.49s/it]Profiling iterations:  70%|███████   | 7/10 [00:38<00:16,  5.49s/it]Profiling iterations:  80%|████████  | 8/10 [00:43<00:10,  5.50s/it]Profiling iterations:  90%|█████████ | 9/10 [00:49<00:05,  5.50s/it]Profiling iterations: 100%|██████████| 10/10 [00:54<00:00,  5.50s/it]Profiling iterations: 100%|██████████| 10/10 [00:54<00:00,  5.49s/it]
Avg latency: 5.492205634340644 seconds
10% percentile latency: 5.477919207978994 seconds
25% percentile latency: 5.478624106035568 seconds
50% percentile latency: 5.489218615461141 seconds
75% percentile latency: 5.506132056820206 seconds
90% percentile latency: 5.510687324311585 seconds
99% percentile latency: 5.511918821102008 seconds
Batch-8 Input len-512 Output len-512
INFO 12-17 21:48:05 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:48:05 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:48:05 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:48:05 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:48:06 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:48:10 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:48:10 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:48:10 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:48:10 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:48:10 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:48:10 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:48:10 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:48:27 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 12-17 21:48:27 [config.py:1472] Using max model len 32768
INFO 12-17 21:48:27 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:48:27 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:48:27 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:48:27 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:48:27 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:48:28 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:48:37 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:48:37 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:48:37 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:48:37 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:48:38 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:48:41 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:48:42 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:48:42 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:48:42 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:48:42 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:48:42 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:48:42 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:48:42 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:48:42 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:48:55 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:48:55 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:48:55 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:48:55 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:48:56 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:49:03 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:49:03 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.65it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.14it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.03it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.25it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.21it/s]

INFO 12-17 21:49:08 [default_loader.py:272] Loading weights took 3.45 seconds
INFO 12-17 21:49:10 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:49:20 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:49:20 [backends.py:519] Dynamo bytecode transform time: 9.29 s
INFO 12-17 21:49:23 [backends.py:193] Compiling a graph for general shape takes 2.58 s
INFO 12-17 21:49:32 [monitor.py:34] torch.compile takes 11.87 s in total
INFO 12-17 21:49:34 [worker_v1.py:181] Available memory: 41200181964, total memory: 65464696832
INFO 12-17 21:49:34 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:49:34 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:50:27 [model_runner_v1.py:2074] Graph capturing finished in 52 secs, took 0.36 GiB
INFO 12-17 21:50:27 [core.py:172] init engine (profile, create kv cache, warmup model) took 76.95 seconds
WARNING 12-17 21:50:28 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:50:28 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:50:28 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:50:28 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:50:28 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:10<00:43, 10.79s/it]Warmup iterations:  40%|████      | 2/5 [00:21<00:32, 10.75s/it]Warmup iterations:  60%|██████    | 3/5 [00:32<00:21, 10.73s/it]Warmup iterations:  80%|████████  | 4/5 [00:42<00:10, 10.72s/it]Warmup iterations: 100%|██████████| 5/5 [00:53<00:00, 10.72s/it]Warmup iterations: 100%|██████████| 5/5 [00:53<00:00, 10.73s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:10<01:36, 10.70s/it]Profiling iterations:  20%|██        | 2/10 [00:21<01:25, 10.72s/it]Profiling iterations:  30%|███       | 3/10 [00:32<01:15, 10.72s/it]Profiling iterations:  40%|████      | 4/10 [00:42<01:04, 10.72s/it]Profiling iterations:  50%|█████     | 5/10 [00:53<00:53, 10.71s/it]Profiling iterations:  60%|██████    | 6/10 [01:04<00:42, 10.71s/it]Profiling iterations:  70%|███████   | 7/10 [01:14<00:32, 10.71s/it]Profiling iterations:  80%|████████  | 8/10 [01:25<00:21, 10.71s/it]Profiling iterations:  90%|█████████ | 9/10 [01:36<00:10, 10.71s/it]Profiling iterations: 100%|██████████| 10/10 [01:47<00:00, 10.71s/it]Profiling iterations: 100%|██████████| 10/10 [01:47<00:00, 10.71s/it]
Avg latency: 10.710386183997617 seconds
10% percentile latency: 10.701122605986892 seconds
25% percentile latency: 10.70374991837889 seconds
50% percentile latency: 10.7056306861341 seconds
75% percentile latency: 10.71893474261742 seconds
90% percentile latency: 10.72313807979226 seconds
99% percentile latency: 10.724430180508643 seconds
Batch-8 Input len-512 Output len-1024
INFO 12-17 21:53:26 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:53:26 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:53:26 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:53:26 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:53:27 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:53:31 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:53:31 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:53:31 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:53:31 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:53:31 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:53:31 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:53:31 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:53:47 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 12-17 21:53:47 [config.py:1472] Using max model len 32768
INFO 12-17 21:53:47 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 21:53:47 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:53:47 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:53:47 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:53:47 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 21:53:49 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 21:53:57 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:53:57 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:53:57 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:53:57 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:53:58 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:54:01 [core.py:526] Waiting for init message from front-end.
INFO 12-17 21:54:02 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 21:54:03 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 21:54:03 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 21:54:03 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 21:54:03 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 21:54:03 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 21:54:03 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 21:54:03 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 21:54:15 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 21:54:15 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 21:54:15 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 21:54:16 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 21:54:17 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 21:54:23 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 21:54:24 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.12it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.63it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.48it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.50it/s]

INFO 12-17 21:54:28 [default_loader.py:272] Loading weights took 2.77 seconds
INFO 12-17 21:54:30 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 21:54:40 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 21:54:40 [backends.py:519] Dynamo bytecode transform time: 9.23 s
INFO 12-17 21:54:43 [backends.py:193] Compiling a graph for general shape takes 2.55 s
INFO 12-17 21:54:52 [monitor.py:34] torch.compile takes 11.78 s in total
INFO 12-17 21:54:54 [worker_v1.py:181] Available memory: 41200403148, total memory: 65464696832
INFO 12-17 21:54:54 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 21:54:54 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 21:55:59 [model_runner_v1.py:2074] Graph capturing finished in 65 secs, took 0.36 GiB
INFO 12-17 21:55:59 [core.py:172] init engine (profile, create kv cache, warmup model) took 89.30 seconds
WARNING 12-17 21:56:00 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 21:56:01 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 21:56:01 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 21:56:01 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 21:56:01 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:21<01:25, 21.33s/it]Warmup iterations:  40%|████      | 2/5 [00:42<01:03, 21.27s/it]Warmup iterations:  60%|██████    | 3/5 [01:03<00:42, 21.27s/it]Warmup iterations:  80%|████████  | 4/5 [01:25<00:21, 21.27s/it]Warmup iterations: 100%|██████████| 5/5 [01:46<00:00, 21.28s/it]Warmup iterations: 100%|██████████| 5/5 [01:46<00:00, 21.28s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:21<03:11, 21.29s/it]Profiling iterations:  20%|██        | 2/10 [00:42<02:50, 21.30s/it]Profiling iterations:  30%|███       | 3/10 [01:03<02:28, 21.28s/it]Profiling iterations:  40%|████      | 4/10 [01:25<02:08, 21.36s/it]Profiling iterations:  50%|█████     | 5/10 [01:46<01:46, 21.32s/it]Profiling iterations:  60%|██████    | 6/10 [02:07<01:25, 21.29s/it]Profiling iterations:  70%|███████   | 7/10 [02:29<01:03, 21.27s/it]Profiling iterations:  80%|████████  | 8/10 [02:50<00:42, 21.27s/it]Profiling iterations:  90%|█████████ | 9/10 [03:11<00:21, 21.26s/it]Profiling iterations: 100%|██████████| 10/10 [03:32<00:00, 21.26s/it]Profiling iterations: 100%|██████████| 10/10 [03:32<00:00, 21.28s/it]
Avg latency: 21.2788542480208 seconds
10% percentile latency: 21.23302911934443 seconds
25% percentile latency: 21.2456632188987 seconds
50% percentile latency: 21.254775006556883 seconds
75% percentile latency: 21.286452416330576 seconds
90% percentile latency: 21.32100168345496 seconds
99% percentile latency: 21.456064213542266 seconds
Batch-8 Input len-1024 Output len-32
INFO 12-17 22:01:38 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:01:38 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:01:38 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:01:38 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:01:39 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:01:43 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:01:43 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:01:43 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:01:43 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:01:43 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:01:43 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:01:43 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:02:01 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-17 22:02:01 [config.py:1472] Using max model len 32768
INFO 12-17 22:02:01 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 22:02:01 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:02:01 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:02:01 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:02:01 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 22:02:02 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 22:02:11 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:02:11 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:02:11 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:02:11 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:02:12 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:02:15 [core.py:526] Waiting for init message from front-end.
INFO 12-17 22:02:16 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:02:16 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:02:16 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:02:16 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:02:16 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:02:16 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:02:16 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:02:16 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 22:02:29 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:02:29 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:02:29 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:02:29 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:02:30 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:02:37 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 22:02:37 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.16it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.64it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.48it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.50it/s]

INFO 12-17 22:02:41 [default_loader.py:272] Loading weights took 2.76 seconds
INFO 12-17 22:02:43 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 22:02:53 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 22:02:53 [backends.py:519] Dynamo bytecode transform time: 9.20 s
INFO 12-17 22:02:57 [backends.py:193] Compiling a graph for general shape takes 2.55 s
INFO 12-17 22:03:05 [monitor.py:34] torch.compile takes 11.76 s in total
INFO 12-17 22:03:07 [worker_v1.py:181] Available memory: 41201885900, total memory: 65464696832
INFO 12-17 22:03:07 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 22:03:07 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 22:04:12 [model_runner_v1.py:2074] Graph capturing finished in 65 secs, took 0.36 GiB
INFO 12-17 22:04:12 [core.py:172] init engine (profile, create kv cache, warmup model) took 89.07 seconds
WARNING 12-17 22:04:13 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 22:04:14 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:04:14 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:04:14 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:04:14 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:04,  1.22s/it]Warmup iterations:  40%|████      | 2/5 [00:02<00:03,  1.18s/it]Warmup iterations:  60%|██████    | 3/5 [00:03<00:02,  1.16s/it]Warmup iterations:  80%|████████  | 4/5 [00:04<00:01,  1.16s/it]Warmup iterations: 100%|██████████| 5/5 [00:05<00:00,  1.15s/it]Warmup iterations: 100%|██████████| 5/5 [00:05<00:00,  1.16s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:10,  1.15s/it]Profiling iterations:  20%|██        | 2/10 [00:02<00:09,  1.15s/it]Profiling iterations:  30%|███       | 3/10 [00:03<00:08,  1.15s/it]Profiling iterations:  40%|████      | 4/10 [00:04<00:06,  1.15s/it]Profiling iterations:  50%|█████     | 5/10 [00:05<00:05,  1.15s/it]Profiling iterations:  60%|██████    | 6/10 [00:06<00:04,  1.15s/it]Profiling iterations:  70%|███████   | 7/10 [00:08<00:03,  1.15s/it]Profiling iterations:  80%|████████  | 8/10 [00:09<00:02,  1.15s/it]Profiling iterations:  90%|█████████ | 9/10 [00:10<00:01,  1.15s/it]Profiling iterations: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it]Profiling iterations: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it]
Avg latency: 1.1459828129503875 seconds
10% percentile latency: 1.1448167895898222 seconds
25% percentile latency: 1.1454871919704601 seconds
50% percentile latency: 1.1458560079336166 seconds
75% percentile latency: 1.146684077451937 seconds
90% percentile latency: 1.1474019902292638 seconds
99% percentile latency: 1.1474472287436948 seconds
Batch-8 Input len-1024 Output len-64
INFO 12-17 22:04:49 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:04:49 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:04:49 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:04:49 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:04:50 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:04:54 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:04:54 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:04:54 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:04:54 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:04:54 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:04:54 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:04:54 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:05:11 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 12-17 22:05:11 [config.py:1472] Using max model len 32768
INFO 12-17 22:05:11 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 22:05:11 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:05:11 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:05:11 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:05:11 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 22:05:12 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 22:05:21 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:05:21 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:05:21 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:05:21 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:05:22 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:05:25 [core.py:526] Waiting for init message from front-end.
INFO 12-17 22:05:26 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:05:26 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:05:26 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:05:26 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:05:26 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:05:26 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:05:26 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:05:26 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 22:05:39 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:05:39 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:05:39 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:05:39 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:05:40 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:05:47 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 22:05:47 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.52it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.08it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.00it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.21it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.17it/s]

INFO 12-17 22:05:52 [default_loader.py:272] Loading weights took 3.58 seconds
INFO 12-17 22:05:53 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 22:06:02 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 22:06:02 [backends.py:519] Dynamo bytecode transform time: 9.45 s
INFO 12-17 22:06:06 [backends.py:193] Compiling a graph for general shape takes 2.50 s
INFO 12-17 22:06:15 [monitor.py:34] torch.compile takes 11.95 s in total
INFO 12-17 22:06:16 [worker_v1.py:181] Available memory: 41201791692, total memory: 65464696832
INFO 12-17 22:06:16 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 22:06:16 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 22:06:50 [model_runner_v1.py:2074] Graph capturing finished in 34 secs, took 0.36 GiB
INFO 12-17 22:06:50 [core.py:172] init engine (profile, create kv cache, warmup model) took 57.62 seconds
WARNING 12-17 22:06:51 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 22:06:52 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:06:52 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:06:52 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:06:52 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:01<00:07,  1.88s/it]Warmup iterations:  40%|████      | 2/5 [00:03<00:05,  1.84s/it]Warmup iterations:  60%|██████    | 3/5 [00:05<00:03,  1.82s/it]Warmup iterations:  80%|████████  | 4/5 [00:07<00:01,  1.81s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.81s/it]Warmup iterations: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:01<00:16,  1.80s/it]Profiling iterations:  20%|██        | 2/10 [00:03<00:14,  1.79s/it]Profiling iterations:  30%|███       | 3/10 [00:05<00:12,  1.79s/it]Profiling iterations:  40%|████      | 4/10 [00:07<00:10,  1.80s/it]Profiling iterations:  50%|█████     | 5/10 [00:09<00:09,  1.81s/it]Profiling iterations:  60%|██████    | 6/10 [00:10<00:07,  1.81s/it]Profiling iterations:  70%|███████   | 7/10 [00:12<00:05,  1.81s/it]Profiling iterations:  80%|████████  | 8/10 [00:14<00:03,  1.81s/it]Profiling iterations:  90%|█████████ | 9/10 [00:16<00:01,  1.81s/it]Profiling iterations: 100%|██████████| 10/10 [00:18<00:00,  1.81s/it]Profiling iterations: 100%|██████████| 10/10 [00:18<00:00,  1.81s/it]
Avg latency: 1.8073917274363338 seconds
10% percentile latency: 1.7938501325435936 seconds
25% percentile latency: 1.799753408995457 seconds
50% percentile latency: 1.8121634502895176 seconds
75% percentile latency: 1.8132653796346858 seconds
90% percentile latency: 1.813751597609371 seconds
99% percentile latency: 1.814628231357783 seconds
Batch-8 Input len-1024 Output len-128
INFO 12-17 22:07:37 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:07:37 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:07:37 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:07:37 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:07:38 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:07:42 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:07:42 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:07:42 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:07:42 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:07:42 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:07:42 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:07:42 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:07:59 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 12-17 22:07:59 [config.py:1472] Using max model len 32768
INFO 12-17 22:07:59 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 22:07:59 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:07:59 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:07:59 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:07:59 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 22:08:00 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 22:08:08 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:08:08 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:08:08 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:08:08 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:08:10 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:08:12 [core.py:526] Waiting for init message from front-end.
INFO 12-17 22:08:13 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:08:14 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:08:14 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:08:14 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:08:14 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:08:14 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:08:14 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:08:14 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 22:08:27 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:08:27 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:08:27 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:08:27 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:08:28 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:08:34 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 22:08:35 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.58it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.19it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.10it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.34it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]

INFO 12-17 22:08:39 [default_loader.py:272] Loading weights took 3.21 seconds
INFO 12-17 22:08:40 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 22:08:49 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 22:08:49 [backends.py:519] Dynamo bytecode transform time: 8.91 s
INFO 12-17 22:08:52 [backends.py:193] Compiling a graph for general shape takes 2.40 s
INFO 12-17 22:09:01 [monitor.py:34] torch.compile takes 11.31 s in total
INFO 12-17 22:09:02 [worker_v1.py:181] Available memory: 41201918668, total memory: 65464696832
INFO 12-17 22:09:02 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 22:09:02 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 22:09:39 [model_runner_v1.py:2074] Graph capturing finished in 37 secs, took 0.36 GiB
INFO 12-17 22:09:39 [core.py:172] init engine (profile, create kv cache, warmup model) took 59.67 seconds
WARNING 12-17 22:09:40 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 22:09:41 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:09:41 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:09:41 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:09:41 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:03<00:12,  3.21s/it]Warmup iterations:  40%|████      | 2/5 [00:06<00:09,  3.18s/it]Warmup iterations:  60%|██████    | 3/5 [00:09<00:06,  3.17s/it]Warmup iterations:  80%|████████  | 4/5 [00:12<00:03,  3.16s/it]Warmup iterations: 100%|██████████| 5/5 [00:15<00:00,  3.16s/it]Warmup iterations: 100%|██████████| 5/5 [00:15<00:00,  3.16s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:03<00:28,  3.16s/it]Profiling iterations:  20%|██        | 2/10 [00:06<00:24,  3.11s/it]Profiling iterations:  30%|███       | 3/10 [00:09<00:21,  3.11s/it]Profiling iterations:  40%|████      | 4/10 [00:12<00:18,  3.13s/it]Profiling iterations:  50%|█████     | 5/10 [00:15<00:15,  3.14s/it]Profiling iterations:  60%|██████    | 6/10 [00:18<00:12,  3.14s/it]Profiling iterations:  70%|███████   | 7/10 [00:21<00:09,  3.14s/it]Profiling iterations:  80%|████████  | 8/10 [00:25<00:06,  3.13s/it]Profiling iterations:  90%|█████████ | 9/10 [00:28<00:03,  3.14s/it]Profiling iterations: 100%|██████████| 10/10 [00:31<00:00,  3.14s/it]Profiling iterations: 100%|██████████| 10/10 [00:31<00:00,  3.13s/it]
Avg latency: 3.1341333923861385 seconds
10% percentile latency: 3.1136352598667143 seconds
25% percentile latency: 3.1253086992073804 seconds
50% percentile latency: 3.1416007755324244 seconds
75% percentile latency: 3.1492013847455382 seconds
90% percentile latency: 3.1557580062188206 seconds
99% percentile latency: 3.157637067390606 seconds
Batch-8 Input len-1024 Output len-256
INFO 12-17 22:10:46 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:10:46 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:10:46 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:10:46 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:10:47 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:10:51 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:10:51 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:10:51 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:10:51 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:10:51 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:10:51 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:10:51 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:11:08 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-17 22:11:08 [config.py:1472] Using max model len 32768
INFO 12-17 22:11:08 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 22:11:08 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:11:08 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:11:08 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:11:08 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 22:11:09 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 22:11:18 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:11:18 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:11:18 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:11:18 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:11:19 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:11:22 [core.py:526] Waiting for init message from front-end.
INFO 12-17 22:11:23 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:11:23 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:11:23 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:11:23 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:11:23 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:11:23 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:11:23 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:11:23 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 22:11:36 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:11:36 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:11:36 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:11:36 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:11:37 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:11:45 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 22:11:46 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.65it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.15it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.04it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.25it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.22it/s]

INFO 12-17 22:11:51 [default_loader.py:272] Loading weights took 3.43 seconds
INFO 12-17 22:11:52 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 22:12:03 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 22:12:03 [backends.py:519] Dynamo bytecode transform time: 9.19 s
INFO 12-17 22:12:06 [backends.py:193] Compiling a graph for general shape takes 2.38 s
INFO 12-17 22:12:14 [monitor.py:34] torch.compile takes 11.58 s in total
INFO 12-17 22:12:16 [worker_v1.py:181] Available memory: 41201922764, total memory: 65464696832
INFO 12-17 22:12:16 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 22:12:16 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 22:13:23 [model_runner_v1.py:2074] Graph capturing finished in 67 secs, took 0.36 GiB
INFO 12-17 22:13:23 [core.py:172] init engine (profile, create kv cache, warmup model) took 90.79 seconds
WARNING 12-17 22:13:24 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 22:13:25 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:13:25 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:13:25 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:13:25 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:05<00:23,  5.85s/it]Warmup iterations:  40%|████      | 2/5 [00:11<00:17,  5.80s/it]Warmup iterations:  60%|██████    | 3/5 [00:17<00:11,  5.82s/it]Warmup iterations:  80%|████████  | 4/5 [00:23<00:05,  5.83s/it]Warmup iterations: 100%|██████████| 5/5 [00:29<00:00,  5.84s/it]Warmup iterations: 100%|██████████| 5/5 [00:29<00:00,  5.83s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:05<00:52,  5.83s/it]Profiling iterations:  20%|██        | 2/10 [00:11<00:46,  5.84s/it]Profiling iterations:  30%|███       | 3/10 [00:17<00:40,  5.84s/it]Profiling iterations:  40%|████      | 4/10 [00:23<00:35,  5.84s/it]Profiling iterations:  50%|█████     | 5/10 [00:29<00:29,  5.84s/it]Profiling iterations:  60%|██████    | 6/10 [00:35<00:23,  5.84s/it]Profiling iterations:  70%|███████   | 7/10 [00:40<00:17,  5.84s/it]Profiling iterations:  80%|████████  | 8/10 [00:46<00:11,  5.84s/it]Profiling iterations:  90%|█████████ | 9/10 [00:52<00:05,  5.85s/it]Profiling iterations: 100%|██████████| 10/10 [00:58<00:00,  5.85s/it]Profiling iterations: 100%|██████████| 10/10 [00:58<00:00,  5.84s/it]
Avg latency: 5.844215325405822 seconds
10% percentile latency: 5.835785134229809 seconds
25% percentile latency: 5.84261072287336 seconds
50% percentile latency: 5.8436786809470505 seconds
75% percentile latency: 5.849526617210358 seconds
90% percentile latency: 5.851123728509992 seconds
99% percentile latency: 5.851609990205616 seconds
Batch-8 Input len-1024 Output len-512
INFO 12-17 22:15:10 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:15:10 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:15:10 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:15:10 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:15:11 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:15:15 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:15:15 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:15:15 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:15:15 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:15:15 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:15:15 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:15:15 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:15:32 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-17 22:15:32 [config.py:1472] Using max model len 32768
INFO 12-17 22:15:32 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 22:15:32 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:15:32 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:15:32 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:15:32 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 22:15:33 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 22:15:42 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:15:42 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:15:42 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:15:42 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:15:43 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:15:45 [core.py:526] Waiting for init message from front-end.
INFO 12-17 22:15:47 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:15:47 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:15:47 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:15:47 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:15:47 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:15:47 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:15:47 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:15:47 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 22:16:00 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:16:00 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:16:00 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:16:00 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:16:01 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:16:08 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 22:16:08 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.52it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.07it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:01,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.19it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.15it/s]

INFO 12-17 22:16:13 [default_loader.py:272] Loading weights took 3.63 seconds
INFO 12-17 22:16:14 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 22:16:24 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 22:16:24 [backends.py:519] Dynamo bytecode transform time: 9.42 s
INFO 12-17 22:16:27 [backends.py:193] Compiling a graph for general shape takes 2.57 s
INFO 12-17 22:16:36 [monitor.py:34] torch.compile takes 11.98 s in total
INFO 12-17 22:16:37 [worker_v1.py:181] Available memory: 41200734924, total memory: 65464696832
INFO 12-17 22:16:37 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 22:16:37 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 22:17:17 [model_runner_v1.py:2074] Graph capturing finished in 40 secs, took 0.36 GiB
INFO 12-17 22:17:17 [core.py:172] init engine (profile, create kv cache, warmup model) took 63.51 seconds
WARNING 12-17 22:17:18 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 22:17:19 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:17:19 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:17:19 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:17:19 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:11<00:45, 11.30s/it]Warmup iterations:  40%|████      | 2/5 [00:22<00:33, 11.26s/it]Warmup iterations:  60%|██████    | 3/5 [00:33<00:22, 11.27s/it]Warmup iterations:  80%|████████  | 4/5 [00:45<00:11, 11.26s/it]Warmup iterations: 100%|██████████| 5/5 [00:56<00:00, 11.27s/it]Warmup iterations: 100%|██████████| 5/5 [00:56<00:00, 11.27s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:11<01:41, 11.33s/it]Profiling iterations:  20%|██        | 2/10 [00:22<01:30, 11.34s/it]Profiling iterations:  30%|███       | 3/10 [00:33<01:19, 11.30s/it]Profiling iterations:  40%|████      | 4/10 [00:45<01:07, 11.25s/it]Profiling iterations:  50%|█████     | 5/10 [00:56<00:56, 11.22s/it]Profiling iterations:  60%|██████    | 6/10 [01:07<00:44, 11.22s/it]Profiling iterations:  70%|███████   | 7/10 [01:18<00:33, 11.22s/it]Profiling iterations:  80%|████████  | 8/10 [01:30<00:22, 11.27s/it]Profiling iterations:  90%|█████████ | 9/10 [01:41<00:11, 11.33s/it]Profiling iterations: 100%|██████████| 10/10 [01:53<00:00, 11.37s/it]Profiling iterations: 100%|██████████| 10/10 [01:53<00:00, 11.30s/it]
Avg latency: 11.302739851595835 seconds
10% percentile latency: 11.175796787627041 seconds
25% percentile latency: 11.209567924961448 seconds
50% percentile latency: 11.296237954404205 seconds
75% percentile latency: 11.368095406098291 seconds
90% percentile latency: 11.469026346178726 seconds
99% percentile latency: 11.471641992446967 seconds
Batch-8 Input len-1024 Output len-1024
INFO 12-17 22:20:26 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:20:26 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:20:26 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:20:26 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:20:27 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:20:31 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:20:31 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:20:31 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:20:31 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:20:31 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:20:31 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:20:31 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:20:48 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 12-17 22:20:48 [config.py:1472] Using max model len 32768
INFO 12-17 22:20:48 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 22:20:48 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:20:48 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:20:48 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:20:48 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 22:20:49 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 22:20:58 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:20:58 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:20:58 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:20:58 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:20:59 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:21:02 [core.py:526] Waiting for init message from front-end.
INFO 12-17 22:21:03 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 22:21:03 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 22:21:03 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 22:21:03 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 22:21:03 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 22:21:03 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 22:21:03 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 22:21:03 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 22:21:16 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 22:21:16 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 22:21:16 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 22:21:16 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 22:21:17 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 22:21:26 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 22:21:26 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.16it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.62it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.49it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.51it/s]

INFO 12-17 22:21:30 [default_loader.py:272] Loading weights took 2.76 seconds
INFO 12-17 22:21:32 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 22:21:43 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 22:21:43 [backends.py:519] Dynamo bytecode transform time: 9.32 s
INFO 12-17 22:21:46 [backends.py:193] Compiling a graph for general shape takes 2.57 s
INFO 12-17 22:21:54 [monitor.py:34] torch.compile takes 11.89 s in total
INFO 12-17 22:21:56 [worker_v1.py:181] Available memory: 41201144524, total memory: 65464696832
INFO 12-17 22:21:56 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 22:21:56 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 22:23:01 [model_runner_v1.py:2074] Graph capturing finished in 65 secs, took 0.36 GiB
INFO 12-17 22:23:01 [core.py:172] init engine (profile, create kv cache, warmup model) took 88.91 seconds
WARNING 12-17 22:23:02 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 22:23:03 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 22:23:03 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 22:23:03 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 22:23:03 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:22<01:28, 22.03s/it]Warmup iterations:  40%|████      | 2/5 [00:43<01:05, 21.98s/it]Warmup iterations:  60%|██████    | 3/5 [01:05<00:43, 21.98s/it]Warmup iterations:  80%|████████  | 4/5 [01:27<00:21, 21.96s/it]Warmup iterations: 100%|██████████| 5/5 [01:49<00:00, 21.97s/it]Warmup iterations: 100%|██████████| 5/5 [01:49<00:00, 21.98s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:21<03:17, 21.92s/it]Profiling iterations:  20%|██        | 2/10 [00:43<02:55, 21.91s/it]Profiling iterations:  30%|███       | 3/10 [01:05<02:33, 21.92s/it]Profiling iterations:  40%|████      | 4/10 [01:27<02:11, 21.99s/it]Profiling iterations:  50%|█████     | 5/10 [01:49<01:50, 22.00s/it]Profiling iterations:  60%|██████    | 6/10 [02:11<01:27, 21.97s/it]Profiling iterations:  70%|███████   | 7/10 [02:33<01:05, 21.95s/it]Profiling iterations:  80%|████████  | 8/10 [02:55<00:43, 21.93s/it]Profiling iterations:  90%|█████████ | 9/10 [03:17<00:21, 21.94s/it]Profiling iterations: 100%|██████████| 10/10 [03:39<00:00, 21.99s/it]Profiling iterations: 100%|██████████| 10/10 [03:39<00:00, 21.96s/it]
Avg latency: 21.963808229612187 seconds
10% percentile latency: 21.89905708678998 seconds
25% percentile latency: 21.903138723922893 seconds
50% percentile latency: 21.92663279757835 seconds
75% percentile latency: 22.010184733662754 seconds
90% percentile latency: 22.094519409397616 seconds
99% percentile latency: 22.100763955716975 seconds
