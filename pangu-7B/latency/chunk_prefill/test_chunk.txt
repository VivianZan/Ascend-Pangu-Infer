Batch-1 Input len-2048 Output len-1
INFO 12-17 19:20:15 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:20:15 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:20:15 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:20:15 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:20:16 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:20:20 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:20:20 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:20:20 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:20:20 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:20:20 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:20:20 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:20:20 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:20:37 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 12-17 19:20:37 [config.py:1472] Using max model len 32768
INFO 12-17 19:20:37 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 19:20:37 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:20:37 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:20:37 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:20:37 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:20:38 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:20:46 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:20:46 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:20:46 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:20:46 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:20:48 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:20:50 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:20:51 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:20:52 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:20:52 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:20:52 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:20:52 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:20:52 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:20:52 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:20:52 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:21:05 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:21:05 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:21:05 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:21:05 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:21:06 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:21:13 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:21:13 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
INFO 12-17 19:21:17 [default_loader.py:272] Loading weights took 3.42 seconds
INFO 12-17 19:21:18 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:21:28 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:21:28 [backends.py:519] Dynamo bytecode transform time: 8.89 s
INFO 12-17 19:21:31 [backends.py:193] Compiling a graph for general shape takes 2.40 s
INFO 12-17 19:21:40 [monitor.py:34] torch.compile takes 11.29 s in total
INFO 12-17 19:21:42 [worker_v1.py:181] Available memory: 41209553612, total memory: 65464696832
INFO 12-17 19:21:42 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 19:21:42 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 19:22:41 [model_runner_v1.py:2074] Graph capturing finished in 60 secs, took 0.36 GiB
INFO 12-17 19:22:41 [core.py:172] init engine (profile, create kv cache, warmup model) took 82.95 seconds
WARNING 12-17 19:22:42 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:22:43 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:22:43 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:22:43 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:22:43 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Avg latency: 0.14095819578506052 seconds
10% percentile latency: 0.14078641198575498 seconds
25% percentile latency: 0.14085995871573687 seconds
50% percentile latency: 0.14091597171500325 seconds
75% percentile latency: 0.14096781169064343 seconds
90% percentile latency: 0.14119089716114103 seconds
99% percentile latency: 0.1413722476409748 seconds
Batch-1 Input len-4096 Output len-1
INFO 12-17 19:23:03 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:23:03 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:23:03 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:23:03 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:23:05 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:23:08 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:23:09 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:23:09 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:23:09 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:23:09 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:23:09 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:23:09 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:23:25 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-17 19:23:25 [config.py:1472] Using max model len 32768
INFO 12-17 19:23:25 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 19:23:25 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:23:25 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:23:25 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:23:25 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:23:26 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:23:35 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:23:35 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:23:35 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:23:35 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:23:36 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:23:39 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:23:40 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:23:40 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:23:40 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:23:40 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:23:40 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:23:40 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:23:40 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:23:40 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:23:53 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:23:53 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:23:53 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:23:53 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:23:55 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:24:01 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:24:01 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
INFO 12-17 19:24:06 [default_loader.py:272] Loading weights took 3.44 seconds
INFO 12-17 19:24:07 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:24:16 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:24:16 [backends.py:519] Dynamo bytecode transform time: 8.78 s
INFO 12-17 19:24:19 [backends.py:193] Compiling a graph for general shape takes 2.39 s
INFO 12-17 19:24:28 [monitor.py:34] torch.compile takes 11.16 s in total
INFO 12-17 19:24:29 [worker_v1.py:181] Available memory: 41210200780, total memory: 65464696832
INFO 12-17 19:24:29 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 19:24:29 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 19:25:06 [model_runner_v1.py:2074] Graph capturing finished in 37 secs, took 0.36 GiB
INFO 12-17 19:25:06 [core.py:172] init engine (profile, create kv cache, warmup model) took 59.73 seconds
WARNING 12-17 19:25:07 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:25:08 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:25:08 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:25:08 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:25:08 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Avg latency: 0.3031063803471625 seconds
10% percentile latency: 0.3029487501829863 seconds
25% percentile latency: 0.30299598979763687 seconds
50% percentile latency: 0.3031059503555298 seconds
75% percentile latency: 0.30320585577283055 seconds
90% percentile latency: 0.3032745690550655 seconds
99% percentile latency: 0.303280547070317 seconds
Batch-1 Input len-8192 Output len-1
INFO 12-17 19:25:31 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:25:31 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:25:31 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:25:31 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:25:32 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:25:36 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:25:36 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:25:36 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:25:36 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:25:36 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:25:36 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:25:36 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:25:53 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-17 19:25:53 [config.py:1472] Using max model len 32768
INFO 12-17 19:25:53 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 19:25:53 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:25:53 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:25:53 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:25:53 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:25:54 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:26:02 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:26:02 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:26:02 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:26:02 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:26:04 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:26:06 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:26:08 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:26:08 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:26:08 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:26:08 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:26:08 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:26:08 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:26:08 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:26:08 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:26:21 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:26:21 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:26:21 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:26:21 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:26:22 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:26:29 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:26:29 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
INFO 12-17 19:26:33 [default_loader.py:272] Loading weights took 2.89 seconds
INFO 12-17 19:26:35 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:26:45 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:26:45 [backends.py:519] Dynamo bytecode transform time: 9.32 s
INFO 12-17 19:26:49 [backends.py:193] Compiling a graph for general shape takes 2.58 s
INFO 12-17 19:26:58 [monitor.py:34] torch.compile takes 11.91 s in total
INFO 12-17 19:27:00 [worker_v1.py:181] Available memory: 41209574092, total memory: 65464696832
INFO 12-17 19:27:00 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 19:27:00 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 19:28:01 [model_runner_v1.py:2074] Graph capturing finished in 61 secs, took 0.36 GiB
INFO 12-17 19:28:01 [core.py:172] init engine (profile, create kv cache, warmup model) took 85.79 seconds
WARNING 12-17 19:28:02 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:28:02 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:28:02 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:28:02 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:28:02 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Avg latency: 0.7556327444501221 seconds
10% percentile latency: 0.7553849097341299 seconds
25% percentile latency: 0.7554655948188156 seconds
50% percentile latency: 0.7556087418925017 seconds
75% percentile latency: 0.755719177890569 seconds
90% percentile latency: 0.7559411982540041 seconds
99% percentile latency: 0.7561070471582935 seconds
Batch-1 Input len-16384 Output len-1
INFO 12-17 19:28:32 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:28:32 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:28:32 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:28:32 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:28:33 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:28:37 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:28:37 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:28:37 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:28:37 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:28:37 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:28:37 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:28:37 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:28:54 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 12-17 19:28:54 [config.py:1472] Using max model len 32768
INFO 12-17 19:28:54 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 19:28:54 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:28:54 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:28:54 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:28:54 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:28:55 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:29:04 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:29:04 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:29:04 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:29:04 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:29:05 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:29:08 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:29:09 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:29:09 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:29:09 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:29:09 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:29:09 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:29:09 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:29:09 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:29:09 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:29:22 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:29:22 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:29:22 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:29:22 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:29:23 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:29:30 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:29:30 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
INFO 12-17 19:29:34 [default_loader.py:272] Loading weights took 2.77 seconds
INFO 12-17 19:29:36 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:29:46 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:29:46 [backends.py:519] Dynamo bytecode transform time: 9.23 s
INFO 12-17 19:29:49 [backends.py:193] Compiling a graph for general shape takes 2.55 s
INFO 12-17 19:29:58 [monitor.py:34] torch.compile takes 11.78 s in total
INFO 12-17 19:30:00 [worker_v1.py:181] Available memory: 41209569996, total memory: 65464696832
INFO 12-17 19:30:00 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 19:30:00 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 19:31:02 [model_runner_v1.py:2074] Graph capturing finished in 62 secs, took 0.36 GiB
INFO 12-17 19:31:02 [core.py:172] init engine (profile, create kv cache, warmup model) took 86.16 seconds
WARNING 12-17 19:31:03 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:31:03 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:31:03 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:31:03 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:31:03 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Avg latency: 1.8799693584442139 seconds
10% percentile latency: 1.8781162058468908 seconds
25% percentile latency: 1.878603247110732 seconds
50% percentile latency: 1.8794803083874285 seconds
75% percentile latency: 1.8809123390819877 seconds
90% percentile latency: 1.88158387709409 seconds
99% percentile latency: 1.8839081417955459 seconds
