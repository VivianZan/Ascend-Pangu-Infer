Batch-1 Input len-16384 Output len-128
No Chunked Prefill
INFO 12-17 19:31:50 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:31:50 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:31:50 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:31:50 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:31:51 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:31:55 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:31:55 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:31:55 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:31:55 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:31:55 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:31:55 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:31:55 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:32:12 [config.py:841] This model supports multiple tasks: {'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 12-17 19:32:12 [config.py:1472] Using max model len 32768
INFO 12-17 19:32:12 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 19:32:12 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:32:12 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:32:12 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:32:12 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:32:13 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:32:22 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:32:22 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:32:22 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:32:22 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:32:23 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:32:25 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:32:27 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:32:27 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:32:27 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:32:27 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:32:27 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:32:27 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:32:27 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:32:27 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:32:40 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:32:40 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:32:40 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:32:40 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:32:41 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:32:47 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:32:48 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.57it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.10it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.01it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.24it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.20it/s]

INFO 12-17 19:32:53 [default_loader.py:272] Loading weights took 3.49 seconds
INFO 12-17 19:32:54 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:33:05 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:33:05 [backends.py:519] Dynamo bytecode transform time: 9.37 s
INFO 12-17 19:33:08 [backends.py:193] Compiling a graph for general shape takes 2.59 s
INFO 12-17 19:33:17 [monitor.py:34] torch.compile takes 11.95 s in total
INFO 12-17 19:33:19 [worker_v1.py:181] Available memory: 41208697548, total memory: 65464696832
INFO 12-17 19:33:19 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 19:33:19 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 19:33:58 [model_runner_v1.py:2074] Graph capturing finished in 39 secs, took 0.36 GiB
INFO 12-17 19:33:58 [core.py:172] init engine (profile, create kv cache, warmup model) took 63.33 seconds
WARNING 12-17 19:33:59 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:33:59 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:33:59 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:33:59 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:33:59 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:04<00:17,  4.41s/it]Warmup iterations:  40%|████      | 2/5 [00:08<00:13,  4.39s/it]Warmup iterations:  60%|██████    | 3/5 [00:13<00:09,  4.56s/it]Warmup iterations:  80%|████████  | 4/5 [00:17<00:04,  4.46s/it]Warmup iterations: 100%|██████████| 5/5 [00:22<00:00,  4.40s/it]Warmup iterations: 100%|██████████| 5/5 [00:22<00:00,  4.43s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:04<00:38,  4.29s/it]Profiling iterations:  20%|██        | 2/10 [00:08<00:34,  4.29s/it]Profiling iterations:  30%|███       | 3/10 [00:12<00:30,  4.29s/it]Profiling iterations:  40%|████      | 4/10 [00:17<00:25,  4.31s/it]Profiling iterations:  50%|█████     | 5/10 [00:21<00:21,  4.30s/it]Profiling iterations:  60%|██████    | 6/10 [00:25<00:17,  4.31s/it]Profiling iterations:  70%|███████   | 7/10 [00:30<00:12,  4.31s/it]Profiling iterations:  80%|████████  | 8/10 [00:34<00:08,  4.31s/it]Profiling iterations:  90%|█████████ | 9/10 [00:38<00:04,  4.30s/it]Profiling iterations: 100%|██████████| 10/10 [00:43<00:00,  4.30s/it]Profiling iterations: 100%|██████████| 10/10 [00:43<00:00,  4.30s/it]
Avg latency: 4.3029310429468755 seconds
10% percentile latency: 4.291299756895751 seconds
25% percentile latency: 4.291928880149499 seconds
50% percentile latency: 4.297124947886914 seconds
75% percentile latency: 4.3070846979971975 seconds
90% percentile latency: 4.328304251190275 seconds
99% percentile latency: 4.328466916270554 seconds
INIT_CHUNK: 1024
INFO 12-17 19:35:22 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:35:22 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:35:22 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:35:22 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:35:24 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:35:28 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:35:28 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:35:28 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:35:28 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:35:28 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:35:28 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:35:28 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:35:44 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-17 19:35:44 [config.py:1472] Using max model len 32768
INFO 12-17 19:35:44 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=1024.
WARNING 12-17 19:35:44 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:35:44 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:35:44 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:35:44 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:35:45 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:35:54 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:35:54 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:35:54 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:35:54 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:35:55 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:35:58 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:35:59 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:35:59 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:35:59 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:35:59 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:35:59 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:35:59 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:35:59 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:35:59 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:36:12 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:36:12 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:36:12 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:36:12 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:36:14 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:36:20 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:36:20 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.61it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.12it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.01it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.23it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.19it/s]

INFO 12-17 19:36:24 [default_loader.py:272] Loading weights took 3.51 seconds
INFO 12-17 19:36:25 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:36:35 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:36:35 [backends.py:519] Dynamo bytecode transform time: 9.39 s
INFO 12-17 19:36:39 [backends.py:193] Compiling a graph for general shape takes 2.54 s
INFO 12-17 19:36:48 [monitor.py:34] torch.compile takes 11.93 s in total
INFO 12-17 19:36:48 [worker_v1.py:181] Available memory: 41853686988, total memory: 65464696832
INFO 12-17 19:36:48 [kv_cache_utils.py:716] GPU KV cache size: 300,416 tokens
INFO 12-17 19:36:48 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.17x
INFO 12-17 19:37:25 [model_runner_v1.py:2074] Graph capturing finished in 36 secs, took 0.32 GiB
INFO 12-17 19:37:25 [core.py:172] init engine (profile, create kv cache, warmup model) took 59.20 seconds
WARNING 12-17 19:37:26 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:37:26 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:37:26 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:37:26 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:37:26 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:04<00:17,  4.49s/it]Warmup iterations:  40%|████      | 2/5 [00:08<00:13,  4.46s/it]Warmup iterations:  60%|██████    | 3/5 [00:13<00:08,  4.44s/it]Warmup iterations:  80%|████████  | 4/5 [00:17<00:04,  4.46s/it]Warmup iterations: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]Warmup iterations: 100%|██████████| 5/5 [00:22<00:00,  4.45s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:04<00:40,  4.46s/it]Profiling iterations:  20%|██        | 2/10 [00:08<00:35,  4.48s/it]Profiling iterations:  30%|███       | 3/10 [00:13<00:31,  4.48s/it]Profiling iterations:  40%|████      | 4/10 [00:17<00:26,  4.48s/it]Profiling iterations:  50%|█████     | 5/10 [00:22<00:22,  4.49s/it]Profiling iterations:  60%|██████    | 6/10 [00:26<00:17,  4.48s/it]Profiling iterations:  70%|███████   | 7/10 [00:31<00:13,  4.49s/it]Profiling iterations:  80%|████████  | 8/10 [00:35<00:08,  4.48s/it]Profiling iterations:  90%|█████████ | 9/10 [00:40<00:04,  4.49s/it]Profiling iterations: 100%|██████████| 10/10 [00:44<00:00,  4.48s/it]Profiling iterations: 100%|██████████| 10/10 [00:44<00:00,  4.48s/it]
Avg latency: 4.480939547996968 seconds
10% percentile latency: 4.461624001897872 seconds
25% percentile latency: 4.468420683173463 seconds
50% percentile latency: 4.477907411521301 seconds
75% percentile latency: 4.4958979829680175 seconds
90% percentile latency: 4.500035718781874 seconds
99% percentile latency: 4.505566680408083 seconds
INIT_CHUNK: 2048
INFO 12-17 19:38:51 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:38:51 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:38:51 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:38:51 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:38:52 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:38:56 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:38:56 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:38:56 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:38:56 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:38:56 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:38:56 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:38:56 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:39:13 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 12-17 19:39:13 [config.py:1472] Using max model len 32768
INFO 12-17 19:39:13 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 12-17 19:39:13 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:39:13 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:39:13 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:39:13 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:39:14 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:39:23 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:39:23 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:39:23 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:39:23 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:39:24 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:39:26 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:39:28 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:39:28 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:39:28 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:39:28 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:39:28 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:39:28 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:39:28 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:39:28 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:39:41 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:39:41 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:39:41 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:39:41 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:39:42 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:39:48 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:39:48 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.56it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.11it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.01it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.25it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.20it/s]

INFO 12-17 19:39:53 [default_loader.py:272] Loading weights took 3.48 seconds
INFO 12-17 19:39:55 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:40:05 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:40:05 [backends.py:519] Dynamo bytecode transform time: 9.27 s
INFO 12-17 19:40:09 [backends.py:193] Compiling a graph for general shape takes 2.56 s
INFO 12-17 19:40:18 [monitor.py:34] torch.compile takes 11.83 s in total
INFO 12-17 19:40:19 [worker_v1.py:181] Available memory: 41690858188, total memory: 65464696832
INFO 12-17 19:40:19 [kv_cache_utils.py:716] GPU KV cache size: 299,264 tokens
INFO 12-17 19:40:19 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.13x
INFO 12-17 19:41:22 [model_runner_v1.py:2074] Graph capturing finished in 63 secs, took 0.26 GiB
INFO 12-17 19:41:22 [core.py:172] init engine (profile, create kv cache, warmup model) took 87.05 seconds
WARNING 12-17 19:41:23 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:41:24 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:41:24 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:41:24 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:41:24 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:04<00:17,  4.37s/it]Warmup iterations:  40%|████      | 2/5 [00:08<00:13,  4.35s/it]Warmup iterations:  60%|██████    | 3/5 [00:13<00:08,  4.33s/it]Warmup iterations:  80%|████████  | 4/5 [00:17<00:04,  4.33s/it]Warmup iterations: 100%|██████████| 5/5 [00:21<00:00,  4.32s/it]Warmup iterations: 100%|██████████| 5/5 [00:21<00:00,  4.33s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:04<00:38,  4.32s/it]Profiling iterations:  20%|██        | 2/10 [00:08<00:34,  4.31s/it]Profiling iterations:  30%|███       | 3/10 [00:12<00:30,  4.31s/it]Profiling iterations:  40%|████      | 4/10 [00:17<00:25,  4.30s/it]Profiling iterations:  50%|█████     | 5/10 [00:21<00:21,  4.31s/it]Profiling iterations:  60%|██████    | 6/10 [00:25<00:17,  4.30s/it]Profiling iterations:  70%|███████   | 7/10 [00:30<00:12,  4.31s/it]Profiling iterations:  80%|████████  | 8/10 [00:34<00:08,  4.30s/it]Profiling iterations:  90%|█████████ | 9/10 [00:38<00:04,  4.31s/it]Profiling iterations: 100%|██████████| 10/10 [00:43<00:00,  4.31s/it]Profiling iterations: 100%|██████████| 10/10 [00:43<00:00,  4.31s/it]
Avg latency: 4.308569704648107 seconds
10% percentile latency: 4.291949497675523 seconds
25% percentile latency: 4.301696272217669 seconds
50% percentile latency: 4.3101709224283695 seconds
75% percentile latency: 4.316903919796459 seconds
90% percentile latency: 4.3227801453787835 seconds
99% percentile latency: 4.324055795143359 seconds
INIT_CHUNK: 4096
INFO 12-17 19:42:47 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:42:47 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:42:47 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:42:47 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:42:48 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:42:52 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:42:52 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:42:52 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:42:52 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:42:52 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:42:52 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:42:52 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:43:09 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 12-17 19:43:09 [config.py:1472] Using max model len 32768
INFO 12-17 19:43:09 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=4096.
WARNING 12-17 19:43:09 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:43:09 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:43:09 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:43:09 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:43:10 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:43:18 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:43:18 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:43:18 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:43:18 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:43:20 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:43:22 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:43:23 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:43:24 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:43:24 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:43:24 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:43:24 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:43:24 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:43:24 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:43:24 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:43:36 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:43:36 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:43:36 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:43:36 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:43:38 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:43:44 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:43:44 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.67it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.25it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.20it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.49it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.41it/s]

INFO 12-17 19:43:48 [default_loader.py:272] Loading weights took 2.89 seconds
INFO 12-17 19:43:50 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:44:00 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:44:00 [backends.py:519] Dynamo bytecode transform time: 9.10 s
INFO 12-17 19:44:03 [backends.py:193] Compiling a graph for general shape takes 2.49 s
INFO 12-17 19:44:11 [monitor.py:34] torch.compile takes 11.58 s in total
INFO 12-17 19:44:13 [worker_v1.py:181] Available memory: 41502290636, total memory: 65464696832
INFO 12-17 19:44:13 [kv_cache_utils.py:716] GPU KV cache size: 297,984 tokens
INFO 12-17 19:44:13 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.09x
INFO 12-17 19:45:10 [model_runner_v1.py:2074] Graph capturing finished in 57 secs, took 0.27 GiB
INFO 12-17 19:45:10 [core.py:172] init engine (profile, create kv cache, warmup model) took 79.93 seconds
WARNING 12-17 19:45:11 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:45:11 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:45:11 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:45:11 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:45:11 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:04<00:17,  4.25s/it]Warmup iterations:  40%|████      | 2/5 [00:08<00:12,  4.23s/it]Warmup iterations:  60%|██████    | 3/5 [00:12<00:08,  4.24s/it]Warmup iterations:  80%|████████  | 4/5 [00:16<00:04,  4.25s/it]Warmup iterations: 100%|██████████| 5/5 [00:21<00:00,  4.24s/it]Warmup iterations: 100%|██████████| 5/5 [00:21<00:00,  4.24s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:04<00:38,  4.22s/it]Profiling iterations:  20%|██        | 2/10 [00:08<00:33,  4.21s/it]Profiling iterations:  30%|███       | 3/10 [00:12<00:29,  4.22s/it]Profiling iterations:  40%|████      | 4/10 [00:16<00:25,  4.21s/it]Profiling iterations:  50%|█████     | 5/10 [00:21<00:21,  4.22s/it]Profiling iterations:  60%|██████    | 6/10 [00:25<00:16,  4.22s/it]Profiling iterations:  70%|███████   | 7/10 [00:29<00:12,  4.22s/it]Profiling iterations:  80%|████████  | 8/10 [00:33<00:08,  4.21s/it]Profiling iterations:  90%|█████████ | 9/10 [00:37<00:04,  4.22s/it]Profiling iterations: 100%|██████████| 10/10 [00:42<00:00,  4.21s/it]Profiling iterations: 100%|██████████| 10/10 [00:42<00:00,  4.22s/it]
Avg latency: 4.2153254395816475 seconds
10% percentile latency: 4.199659192562104 seconds
25% percentile latency: 4.202450719312765 seconds
50% percentile latency: 4.213414101861417 seconds
75% percentile latency: 4.223351197084412 seconds
90% percentile latency: 4.232226904295385 seconds
99% percentile latency: 4.244366675503552 seconds
INIT_CHUNK: 8192
INFO 12-17 19:46:33 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:46:33 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:46:33 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:46:33 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:46:34 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:46:38 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:46:38 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:46:38 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:46:38 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:46:38 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:46:38 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:46:38 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:46:55 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 12-17 19:46:55 [config.py:1472] Using max model len 32768
INFO 12-17 19:46:55 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 12-17 19:46:55 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:46:55 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:46:55 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:46:55 [utils.py:336] Adjusted ACL graph batch sizes for PanguEmbeddedForCausalLM model (layers: 34): 67 → 54 sizes
WARNING 12-17 19:46:56 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 12-17 19:47:05 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:47:05 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:47:05 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:47:05 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:47:06 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:47:09 [core.py:526] Waiting for init message from front-end.
INFO 12-17 19:47:10 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 12-17 19:47:10 [registry.py:413] Model architecture DeepSeekMTPModel is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_mtp:CustomDeepSeekMTP.
WARNING 12-17 19:47:10 [registry.py:413] Model architecture Qwen2VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_vl:AscendQwen2VLForConditionalGeneration.
WARNING 12-17 19:47:10 [registry.py:413] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen2_5_vl:AscendQwen2_5_VLForConditionalGeneration.
WARNING 12-17 19:47:10 [registry.py:413] Model architecture DeepseekV2ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV2ForCausalLM.
WARNING 12-17 19:47:10 [registry.py:413] Model architecture DeepseekV3ForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.deepseek_v2:CustomDeepseekV3ForCausalLM.
WARNING 12-17 19:47:10 [registry.py:413] Model architecture Qwen3MoeForCausalLM is already registered, and will be overwritten by the new model class vllm_ascend.models.qwen3_moe:CustomQwen3MoeForCausalLM.
INFO 12-17 19:47:10 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/opt/pangu/openPangu-Embedded-7B-V1.1', speculative_config=None, tokenizer='/opt/pangu/openPangu-Embedded-7B-V1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=npu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/opt/pangu/openPangu-Embedded-7B-V1.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["all"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.unified_ascend_attention_with_output"],"use_inductor":false,"compile_sizes":[],"inductor_compile_config":{},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,480,472,464,456,440,432,424,416,400,392,384,376,360,352,344,336,320,312,304,296,280,272,264,256,240,232,224,216,200,192,184,176,160,152,144,136,120,112,104,96,80,72,64,56,40,32,24,16,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-17 19:47:23 [__init__.py:39] Available plugins for group vllm.platform_plugins:
INFO 12-17 19:47:23 [__init__.py:41] - ascend -> vllm_ascend:register
INFO 12-17 19:47:23 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 12-17 19:47:23 [__init__.py:235] Platform plugin ascend is activated
WARNING 12-17 19:47:24 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('/root/miniconda3/envs/pangu/lib/python3.11/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE')
INFO 12-17 19:47:30 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-17 19:47:30 [model_runner_v1.py:1746] Starting to load model /opt/pangu/openPangu-Embedded-7B-V1.1...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.66it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.16it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.04it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.27it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.23it/s]

INFO 12-17 19:47:35 [default_loader.py:272] Loading weights took 3.40 seconds
INFO 12-17 19:47:36 [model_runner_v1.py:1778] Loading model weights took 14.9672 GB
INFO 12-17 19:47:45 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/9709abf489/rank_0_0/backbone for vLLM's torch.compile
INFO 12-17 19:47:45 [backends.py:519] Dynamo bytecode transform time: 8.86 s
INFO 12-17 19:47:48 [backends.py:193] Compiling a graph for general shape takes 2.39 s
INFO 12-17 19:47:57 [monitor.py:34] torch.compile takes 11.26 s in total
INFO 12-17 19:47:58 [worker_v1.py:181] Available memory: 41206727372, total memory: 65464696832
INFO 12-17 19:47:58 [kv_cache_utils.py:716] GPU KV cache size: 295,808 tokens
INFO 12-17 19:47:58 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 9.03x
INFO 12-17 19:48:36 [model_runner_v1.py:2074] Graph capturing finished in 38 secs, took 0.36 GiB
INFO 12-17 19:48:36 [core.py:172] init engine (profile, create kv cache, warmup model) took 60.37 seconds
WARNING 12-17 19:48:37 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 12-17 19:48:38 [ascend_config.py:168] ACL Graph is currently experimental. Please raise an issue on https://github.com/vllm-project/vllm-ascend/issues if you encourage any Error
INFO 12-17 19:48:38 [platform.py:174] PIECEWISE compilation enabled on NPU. use_inductor not supported - using only ACL Graph mode
INFO 12-17 19:48:38 [utils.py:321] Calculated maximum supported batch sizes for ACL graph: 54
INFO 12-17 19:48:38 [utils.py:347] No adjustment needed for ACL graph batch sizes: PanguEmbeddedForCausalLM model (layers: 34) with 54 sizes
Warming up...
Warmup iterations:   0%|          | 0/5 [00:00<?, ?it/s]Warmup iterations:  20%|██        | 1/5 [00:04<00:17,  4.33s/it]Warmup iterations:  40%|████      | 2/5 [00:08<00:12,  4.31s/it]Warmup iterations:  60%|██████    | 3/5 [00:12<00:08,  4.30s/it]Warmup iterations:  80%|████████  | 4/5 [00:17<00:04,  4.29s/it]Warmup iterations: 100%|██████████| 5/5 [00:21<00:00,  4.28s/it]Warmup iterations: 100%|██████████| 5/5 [00:21<00:00,  4.29s/it]
Profiling iterations:   0%|          | 0/10 [00:00<?, ?it/s]Profiling iterations:  10%|█         | 1/10 [00:04<00:38,  4.28s/it]Profiling iterations:  20%|██        | 2/10 [00:08<00:34,  4.28s/it]Profiling iterations:  30%|███       | 3/10 [00:12<00:30,  4.31s/it]Profiling iterations:  40%|████      | 4/10 [00:17<00:25,  4.30s/it]Profiling iterations:  50%|█████     | 5/10 [00:21<00:21,  4.30s/it]Profiling iterations:  60%|██████    | 6/10 [00:25<00:17,  4.31s/it]Profiling iterations:  70%|███████   | 7/10 [00:30<00:12,  4.32s/it]Profiling iterations:  80%|████████  | 8/10 [00:34<00:08,  4.32s/it]Profiling iterations:  90%|█████████ | 9/10 [00:38<00:04,  4.32s/it]Profiling iterations: 100%|██████████| 10/10 [00:43<00:00,  4.32s/it]Profiling iterations: 100%|██████████| 10/10 [00:43<00:00,  4.31s/it]
Avg latency: 4.310782795539126 seconds
10% percentile latency: 4.277729990705848 seconds
25% percentile latency: 4.2853555008769035 seconds
50% percentile latency: 4.313032044097781 seconds
75% percentile latency: 4.333516856888309 seconds
90% percentile latency: 4.340529118245468 seconds
99% percentile latency: 4.346809859299101 seconds
